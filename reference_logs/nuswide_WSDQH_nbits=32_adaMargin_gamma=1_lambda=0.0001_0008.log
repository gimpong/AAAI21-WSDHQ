2022-10-19 01:56:17,494 prepare dataset.
2022-10-19 01:56:22,150 prepare data loader.
2022-10-19 01:56:22,151 Initializing DataLoader.
2022-10-19 01:56:22,153 DataLoader already.
2022-10-19 01:56:22,153 prepare model.
2022-10-19 01:56:22,350 Number of semantic embeddings: 928.
2022-10-19 01:56:30,291 From /data/wangjinpeng/anaconda3/envs/py37torch/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where.
2022-10-19 01:56:43,503 begin training.
2022-10-19 01:56:58,259 step [   1], lr [0.0003000], embedding loss [ 0.9004], quantization loss [ 0.0000], 13.09 sec/batch.
2022-10-19 01:57:00,388 step [   2], lr [0.0003000], embedding loss [ 0.8647], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:57:02,547 step [   3], lr [0.0003000], embedding loss [ 0.8446], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-19 01:57:04,869 step [   4], lr [0.0003000], embedding loss [ 0.8401], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:57:06,947 step [   5], lr [0.0003000], embedding loss [ 0.8280], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:57:09,297 step [   6], lr [0.0003000], embedding loss [ 0.8359], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 01:57:11,512 step [   7], lr [0.0003000], embedding loss [ 0.8119], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:57:13,654 step [   8], lr [0.0003000], embedding loss [ 0.8291], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:57:15,814 step [   9], lr [0.0003000], embedding loss [ 0.8147], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-19 01:57:18,124 step [  10], lr [0.0003000], embedding loss [ 0.8191], quantization loss [ 0.0000],  0.53 sec/batch.
2022-10-19 01:57:20,550 step [  11], lr [0.0003000], embedding loss [ 0.8126], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 01:57:22,790 step [  12], lr [0.0003000], embedding loss [ 0.7977], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:57:24,989 step [  13], lr [0.0003000], embedding loss [ 0.8058], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 01:57:27,186 step [  14], lr [0.0003000], embedding loss [ 0.7931], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 01:57:29,351 step [  15], lr [0.0003000], embedding loss [ 0.7940], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-19 01:57:31,420 step [  16], lr [0.0003000], embedding loss [ 0.7913], quantization loss [ 0.0000],  0.53 sec/batch.
2022-10-19 01:57:33,677 step [  17], lr [0.0003000], embedding loss [ 0.7955], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-19 01:57:35,959 step [  18], lr [0.0003000], embedding loss [ 0.7899], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-19 01:57:38,204 step [  19], lr [0.0003000], embedding loss [ 0.7967], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:57:40,338 step [  20], lr [0.0003000], embedding loss [ 0.7980], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:57:42,673 step [  21], lr [0.0003000], embedding loss [ 0.7962], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-19 01:57:45,014 step [  22], lr [0.0003000], embedding loss [ 0.7844], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-19 01:57:47,065 step [  23], lr [0.0003000], embedding loss [ 0.7990], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 01:57:49,299 step [  24], lr [0.0003000], embedding loss [ 0.7842], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-19 01:57:51,479 step [  25], lr [0.0003000], embedding loss [ 0.7837], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:57:53,747 step [  26], lr [0.0003000], embedding loss [ 0.7793], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 01:57:55,898 step [  27], lr [0.0003000], embedding loss [ 0.7753], quantization loss [ 0.0000],  0.53 sec/batch.
2022-10-19 01:57:58,167 step [  28], lr [0.0003000], embedding loss [ 0.7929], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-19 01:58:01,031 step [  29], lr [0.0003000], embedding loss [ 0.7820], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-19 01:58:03,222 step [  30], lr [0.0003000], embedding loss [ 0.7767], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:58:05,523 step [  31], lr [0.0003000], embedding loss [ 0.7845], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:58:07,700 step [  32], lr [0.0003000], embedding loss [ 0.7826], quantization loss [ 0.0000],  0.50 sec/batch.
2022-10-19 01:58:09,956 step [  33], lr [0.0003000], embedding loss [ 0.7813], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:58:12,176 step [  34], lr [0.0003000], embedding loss [ 0.7850], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:58:14,322 step [  35], lr [0.0003000], embedding loss [ 0.7770], quantization loss [ 0.0000],  0.50 sec/batch.
2022-10-19 01:58:16,616 step [  36], lr [0.0003000], embedding loss [ 0.7796], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:58:18,807 step [  37], lr [0.0003000], embedding loss [ 0.7664], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:58:20,992 step [  38], lr [0.0003000], embedding loss [ 0.7736], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:58:23,164 step [  39], lr [0.0003000], embedding loss [ 0.7704], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:58:25,062 step [  40], lr [0.0003000], embedding loss [ 0.7787], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:58:26,991 step [  41], lr [0.0003000], embedding loss [ 0.7645], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:58:28,890 step [  42], lr [0.0003000], embedding loss [ 0.7683], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 01:58:31,014 step [  43], lr [0.0003000], embedding loss [ 0.7699], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-19 01:58:33,085 step [  44], lr [0.0003000], embedding loss [ 0.7580], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-19 01:58:35,193 step [  45], lr [0.0003000], embedding loss [ 0.7563], quantization loss [ 0.0000],  0.50 sec/batch.
2022-10-19 01:58:37,126 step [  46], lr [0.0003000], embedding loss [ 0.7553], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-19 01:58:38,983 step [  47], lr [0.0003000], embedding loss [ 0.7577], quantization loss [ 0.0000],  0.53 sec/batch.
2022-10-19 01:58:41,051 step [  48], lr [0.0003000], embedding loss [ 0.7607], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:58:42,842 step [  49], lr [0.0003000], embedding loss [ 0.7693], quantization loss [ 0.0000],  0.48 sec/batch.
2022-10-19 01:58:44,699 step [  50], lr [0.0003000], embedding loss [ 0.7612], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 01:58:46,717 step [  51], lr [0.0003000], embedding loss [ 0.7561], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-19 01:58:48,613 step [  52], lr [0.0003000], embedding loss [ 0.7722], quantization loss [ 0.0000],  0.50 sec/batch.
2022-10-19 01:58:50,602 step [  53], lr [0.0003000], embedding loss [ 0.7643], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-19 01:58:52,585 step [  54], lr [0.0003000], embedding loss [ 0.7637], quantization loss [ 0.0000],  0.49 sec/batch.
2022-10-19 01:58:54,436 step [  55], lr [0.0003000], embedding loss [ 0.7588], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:58:56,394 step [  56], lr [0.0003000], embedding loss [ 0.7625], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-19 01:58:58,277 step [  57], lr [0.0003000], embedding loss [ 0.7595], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:59:00,165 step [  58], lr [0.0003000], embedding loss [ 0.7570], quantization loss [ 0.0000],  0.50 sec/batch.
2022-10-19 01:59:02,124 step [  59], lr [0.0003000], embedding loss [ 0.7515], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:59:03,964 step [  60], lr [0.0003000], embedding loss [ 0.7514], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-19 01:59:05,804 step [  61], lr [0.0003000], embedding loss [ 0.7516], quantization loss [ 0.0000],  0.50 sec/batch.
2022-10-19 01:59:07,750 step [  62], lr [0.0003000], embedding loss [ 0.7593], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 01:59:09,609 step [  63], lr [0.0003000], embedding loss [ 0.7687], quantization loss [ 0.0000],  0.50 sec/batch.
2022-10-19 01:59:11,565 step [  64], lr [0.0003000], embedding loss [ 0.7486], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:59:13,455 step [  65], lr [0.0003000], embedding loss [ 0.7487], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:59:15,283 step [  66], lr [0.0003000], embedding loss [ 0.7474], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:59:17,064 step [  67], lr [0.0003000], embedding loss [ 0.7672], quantization loss [ 0.0000],  0.48 sec/batch.
2022-10-19 01:59:18,862 step [  68], lr [0.0003000], embedding loss [ 0.7639], quantization loss [ 0.0000],  0.50 sec/batch.
2022-10-19 01:59:20,628 step [  69], lr [0.0003000], embedding loss [ 0.7709], quantization loss [ 0.0000],  0.50 sec/batch.
2022-10-19 01:59:22,415 step [  70], lr [0.0003000], embedding loss [ 0.7532], quantization loss [ 0.0000],  0.49 sec/batch.
2022-10-19 01:59:24,235 step [  71], lr [0.0003000], embedding loss [ 0.7548], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:59:26,006 step [  72], lr [0.0003000], embedding loss [ 0.7649], quantization loss [ 0.0000],  0.49 sec/batch.
2022-10-19 01:59:27,870 step [  73], lr [0.0003000], embedding loss [ 0.7598], quantization loss [ 0.0000],  0.50 sec/batch.
2022-10-19 01:59:29,726 step [  74], lr [0.0003000], embedding loss [ 0.7528], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:59:32,939 step [  75], lr [0.0003000], embedding loss [ 0.7540], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 01:59:34,694 step [  76], lr [0.0003000], embedding loss [ 0.7441], quantization loss [ 0.0000],  0.53 sec/batch.
2022-10-19 01:59:36,479 step [  77], lr [0.0003000], embedding loss [ 0.7625], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:59:38,290 step [  78], lr [0.0003000], embedding loss [ 0.7483], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-19 01:59:40,125 step [  79], lr [0.0003000], embedding loss [ 0.7497], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:59:42,017 step [  80], lr [0.0003000], embedding loss [ 0.7354], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:59:43,866 step [  81], lr [0.0003000], embedding loss [ 0.7485], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 01:59:43,867 initialize centers iter(1/1).
2022-10-19 01:59:51,777 finish center initialization, duration: 7.91 sec.
2022-10-19 01:59:51,778 update codes and centers iter(1/1).
2022-10-19 01:59:55,984 number of update_code wrong: 0.
2022-10-19 02:00:00,394 non zero codewords: 1024.
2022-10-19 02:00:00,394 finish center update, duration: 8.62 sec.
2022-10-19 02:00:02,141 step [  82], lr [0.0003000], embedding loss [ 0.7496], quantization loss [ 0.3860],  0.49 sec/batch.
2022-10-19 02:00:03,939 step [  83], lr [0.0003000], embedding loss [ 0.7514], quantization loss [ 0.5481],  0.49 sec/batch.
2022-10-19 02:00:05,720 step [  84], lr [0.0003000], embedding loss [ 0.7764], quantization loss [ 1.1404],  0.51 sec/batch.
2022-10-19 02:00:07,586 step [  85], lr [0.0003000], embedding loss [ 0.7637], quantization loss [ 0.6222],  0.52 sec/batch.
2022-10-19 02:00:09,492 step [  86], lr [0.0003000], embedding loss [ 0.7696], quantization loss [ 0.8649],  0.52 sec/batch.
2022-10-19 02:00:11,308 step [  87], lr [0.0003000], embedding loss [ 0.7902], quantization loss [ 1.0527],  0.51 sec/batch.
2022-10-19 02:00:13,086 step [  88], lr [0.0003000], embedding loss [ 0.7711], quantization loss [ 0.6701],  0.51 sec/batch.
2022-10-19 02:00:14,863 step [  89], lr [0.0003000], embedding loss [ 0.7711], quantization loss [ 0.6309],  0.48 sec/batch.
2022-10-19 02:00:16,689 step [  90], lr [0.0003000], embedding loss [ 0.7830], quantization loss [ 0.5834],  0.50 sec/batch.
2022-10-19 02:00:18,530 step [  91], lr [0.0003000], embedding loss [ 0.7745], quantization loss [ 0.5168],  0.51 sec/batch.
2022-10-19 02:00:20,361 step [  92], lr [0.0003000], embedding loss [ 0.7665], quantization loss [ 0.5277],  0.51 sec/batch.
2022-10-19 02:00:22,139 step [  93], lr [0.0003000], embedding loss [ 0.7586], quantization loss [ 0.4456],  0.51 sec/batch.
2022-10-19 02:00:23,959 step [  94], lr [0.0003000], embedding loss [ 0.7760], quantization loss [ 0.5381],  0.51 sec/batch.
2022-10-19 02:00:25,737 step [  95], lr [0.0003000], embedding loss [ 0.7733], quantization loss [ 0.4901],  0.50 sec/batch.
2022-10-19 02:00:27,538 step [  96], lr [0.0003000], embedding loss [ 0.7635], quantization loss [ 0.4473],  0.52 sec/batch.
2022-10-19 02:00:29,329 step [  97], lr [0.0003000], embedding loss [ 0.7614], quantization loss [ 0.4177],  0.57 sec/batch.
2022-10-19 02:00:31,147 step [  98], lr [0.0003000], embedding loss [ 0.7602], quantization loss [ 0.4261],  0.52 sec/batch.
2022-10-19 02:00:32,971 step [  99], lr [0.0003000], embedding loss [ 0.7633], quantization loss [ 0.3894],  0.52 sec/batch.
2022-10-19 02:00:34,736 step [ 100], lr [0.0003000], embedding loss [ 0.7586], quantization loss [ 0.3941],  0.50 sec/batch.
2022-10-19 02:00:36,660 step [ 101], lr [0.0003000], embedding loss [ 0.7703], quantization loss [ 0.3954],  0.52 sec/batch.
2022-10-19 02:00:38,550 step [ 102], lr [0.0003000], embedding loss [ 0.7657], quantization loss [ 0.4155],  0.55 sec/batch.
2022-10-19 02:00:40,361 step [ 103], lr [0.0003000], embedding loss [ 0.7660], quantization loss [ 0.4045],  0.50 sec/batch.
2022-10-19 02:00:42,199 step [ 104], lr [0.0003000], embedding loss [ 0.7640], quantization loss [ 0.3677],  0.49 sec/batch.
2022-10-19 02:00:44,061 step [ 105], lr [0.0003000], embedding loss [ 0.7543], quantization loss [ 0.3754],  0.52 sec/batch.
2022-10-19 02:00:45,947 step [ 106], lr [0.0003000], embedding loss [ 0.7520], quantization loss [ 0.3828],  0.52 sec/batch.
2022-10-19 02:00:47,888 step [ 107], lr [0.0003000], embedding loss [ 0.7575], quantization loss [ 0.3700],  0.51 sec/batch.
2022-10-19 02:00:49,737 step [ 108], lr [0.0003000], embedding loss [ 0.7628], quantization loss [ 0.3554],  0.51 sec/batch.
2022-10-19 02:00:51,624 step [ 109], lr [0.0003000], embedding loss [ 0.7575], quantization loss [ 0.3919],  0.51 sec/batch.
2022-10-19 02:00:53,491 step [ 110], lr [0.0003000], embedding loss [ 0.7617], quantization loss [ 0.3768],  0.52 sec/batch.
2022-10-19 02:00:55,391 step [ 111], lr [0.0003000], embedding loss [ 0.7692], quantization loss [ 0.3199],  0.51 sec/batch.
2022-10-19 02:00:57,291 step [ 112], lr [0.0003000], embedding loss [ 0.7556], quantization loss [ 0.3795],  0.52 sec/batch.
2022-10-19 02:00:59,139 step [ 113], lr [0.0003000], embedding loss [ 0.7547], quantization loss [ 0.3631],  0.51 sec/batch.
2022-10-19 02:01:00,997 step [ 114], lr [0.0003000], embedding loss [ 0.7589], quantization loss [ 0.3904],  0.50 sec/batch.
2022-10-19 02:01:02,819 step [ 115], lr [0.0003000], embedding loss [ 0.7651], quantization loss [ 0.3387],  0.50 sec/batch.
2022-10-19 02:01:04,674 step [ 116], lr [0.0003000], embedding loss [ 0.7550], quantization loss [ 0.3986],  0.51 sec/batch.
2022-10-19 02:01:06,484 step [ 117], lr [0.0003000], embedding loss [ 0.7542], quantization loss [ 0.3196],  0.50 sec/batch.
2022-10-19 02:01:08,271 step [ 118], lr [0.0003000], embedding loss [ 0.7591], quantization loss [ 0.3389],  0.51 sec/batch.
2022-10-19 02:01:10,193 step [ 119], lr [0.0003000], embedding loss [ 0.7590], quantization loss [ 0.3551],  0.51 sec/batch.
2022-10-19 02:01:12,058 step [ 120], lr [0.0003000], embedding loss [ 0.7552], quantization loss [ 0.3295],  0.52 sec/batch.
2022-10-19 02:01:13,911 step [ 121], lr [0.0003000], embedding loss [ 0.7581], quantization loss [ 0.2965],  0.51 sec/batch.
2022-10-19 02:01:15,800 step [ 122], lr [0.0003000], embedding loss [ 0.7576], quantization loss [ 0.3506],  0.51 sec/batch.
2022-10-19 02:01:17,653 step [ 123], lr [0.0003000], embedding loss [ 0.7507], quantization loss [ 0.3692],  0.50 sec/batch.
2022-10-19 02:01:19,501 step [ 124], lr [0.0003000], embedding loss [ 0.7515], quantization loss [ 0.3123],  0.51 sec/batch.
2022-10-19 02:01:21,341 step [ 125], lr [0.0003000], embedding loss [ 0.7599], quantization loss [ 0.3318],  0.51 sec/batch.
2022-10-19 02:01:23,169 step [ 126], lr [0.0003000], embedding loss [ 0.7610], quantization loss [ 0.3460],  0.51 sec/batch.
2022-10-19 02:01:25,010 step [ 127], lr [0.0003000], embedding loss [ 0.7612], quantization loss [ 0.3032],  0.51 sec/batch.
2022-10-19 02:01:26,835 step [ 128], lr [0.0003000], embedding loss [ 0.7603], quantization loss [ 0.3229],  0.50 sec/batch.
2022-10-19 02:01:28,656 step [ 129], lr [0.0003000], embedding loss [ 0.7434], quantization loss [ 0.2721],  0.50 sec/batch.
2022-10-19 02:01:30,505 step [ 130], lr [0.0003000], embedding loss [ 0.7448], quantization loss [ 0.2879],  0.50 sec/batch.
2022-10-19 02:01:32,340 step [ 131], lr [0.0003000], embedding loss [ 0.7465], quantization loss [ 0.3027],  0.49 sec/batch.
2022-10-19 02:01:34,189 step [ 132], lr [0.0003000], embedding loss [ 0.7580], quantization loss [ 0.3461],  0.50 sec/batch.
2022-10-19 02:01:36,016 step [ 133], lr [0.0003000], embedding loss [ 0.7425], quantization loss [ 0.3220],  0.51 sec/batch.
2022-10-19 02:01:37,849 step [ 134], lr [0.0003000], embedding loss [ 0.7531], quantization loss [ 0.2950],  0.51 sec/batch.
2022-10-19 02:01:39,722 step [ 135], lr [0.0003000], embedding loss [ 0.7476], quantization loss [ 0.2845],  0.52 sec/batch.
2022-10-19 02:01:41,575 step [ 136], lr [0.0003000], embedding loss [ 0.7451], quantization loss [ 0.2947],  0.50 sec/batch.
2022-10-19 02:01:43,347 step [ 137], lr [0.0003000], embedding loss [ 0.7559], quantization loss [ 0.2964],  0.51 sec/batch.
2022-10-19 02:01:45,169 step [ 138], lr [0.0003000], embedding loss [ 0.7441], quantization loss [ 0.3375],  0.51 sec/batch.
2022-10-19 02:01:47,053 step [ 139], lr [0.0003000], embedding loss [ 0.7450], quantization loss [ 0.2885],  0.51 sec/batch.
2022-10-19 02:01:48,952 step [ 140], lr [0.0003000], embedding loss [ 0.7444], quantization loss [ 0.2673],  0.51 sec/batch.
2022-10-19 02:01:50,801 step [ 141], lr [0.0003000], embedding loss [ 0.7520], quantization loss [ 0.3147],  0.51 sec/batch.
2022-10-19 02:01:52,709 step [ 142], lr [0.0003000], embedding loss [ 0.7556], quantization loss [ 0.2559],  0.50 sec/batch.
2022-10-19 02:01:54,571 step [ 143], lr [0.0003000], embedding loss [ 0.7636], quantization loss [ 0.2883],  0.50 sec/batch.
2022-10-19 02:01:56,432 step [ 144], lr [0.0003000], embedding loss [ 0.7570], quantization loss [ 0.3172],  0.53 sec/batch.
2022-10-19 02:01:58,199 step [ 145], lr [0.0003000], embedding loss [ 0.7585], quantization loss [ 0.2764],  0.50 sec/batch.
2022-10-19 02:02:00,302 step [ 146], lr [0.0003000], embedding loss [ 0.7466], quantization loss [ 0.3030],  0.51 sec/batch.
2022-10-19 02:02:02,163 step [ 147], lr [0.0003000], embedding loss [ 0.7595], quantization loss [ 0.2812],  0.52 sec/batch.
2022-10-19 02:02:03,990 step [ 148], lr [0.0003000], embedding loss [ 0.7545], quantization loss [ 0.2956],  0.51 sec/batch.
2022-10-19 02:02:05,881 step [ 149], lr [0.0003000], embedding loss [ 0.7474], quantization loss [ 0.2990],  0.51 sec/batch.
2022-10-19 02:02:07,763 step [ 150], lr [0.0003000], embedding loss [ 0.7514], quantization loss [ 0.2623],  0.52 sec/batch.
2022-10-19 02:02:09,647 step [ 151], lr [0.0003000], embedding loss [ 0.7545], quantization loss [ 0.2735],  0.52 sec/batch.
2022-10-19 02:02:11,520 step [ 152], lr [0.0003000], embedding loss [ 0.7500], quantization loss [ 0.3084],  0.51 sec/batch.
2022-10-19 02:02:13,418 step [ 153], lr [0.0003000], embedding loss [ 0.7530], quantization loss [ 0.2914],  0.52 sec/batch.
2022-10-19 02:02:15,183 step [ 154], lr [0.0003000], embedding loss [ 0.7514], quantization loss [ 0.2752],  0.51 sec/batch.
2022-10-19 02:02:17,036 step [ 155], lr [0.0003000], embedding loss [ 0.7579], quantization loss [ 0.3041],  0.50 sec/batch.
2022-10-19 02:02:18,774 step [ 156], lr [0.0003000], embedding loss [ 0.7513], quantization loss [ 0.2792],  0.51 sec/batch.
2022-10-19 02:02:20,878 step [ 157], lr [0.0003000], embedding loss [ 0.7410], quantization loss [ 0.2550],  0.58 sec/batch.
2022-10-19 02:02:22,969 step [ 158], lr [0.0003000], embedding loss [ 0.7281], quantization loss [ 0.2692],  0.59 sec/batch.
2022-10-19 02:02:24,773 step [ 159], lr [0.0003000], embedding loss [ 0.7560], quantization loss [ 0.2644],  0.51 sec/batch.
2022-10-19 02:02:26,579 step [ 160], lr [0.0003000], embedding loss [ 0.7506], quantization loss [ 0.2282],  0.50 sec/batch.
2022-10-19 02:02:28,603 step [ 161], lr [0.0003000], embedding loss [ 0.7480], quantization loss [ 0.2434],  0.58 sec/batch.
2022-10-19 02:02:28,603 update codes and centers iter(1/1).
2022-10-19 02:02:31,809 number of update_code wrong: 0.
2022-10-19 02:02:34,648 non zero codewords: 1024.
2022-10-19 02:02:34,649 finish center update, duration: 6.05 sec.
2022-10-19 02:02:36,506 step [ 162], lr [0.0003000], embedding loss [ 0.7490], quantization loss [ 0.1810],  0.50 sec/batch.
2022-10-19 02:02:38,339 step [ 163], lr [0.0003000], embedding loss [ 0.7470], quantization loss [ 0.1792],  0.51 sec/batch.
2022-10-19 02:02:40,238 step [ 164], lr [0.0003000], embedding loss [ 0.7383], quantization loss [ 0.1660],  0.58 sec/batch.
2022-10-19 02:02:42,112 step [ 165], lr [0.0003000], embedding loss [ 0.7456], quantization loss [ 0.1676],  0.50 sec/batch.
2022-10-19 02:02:44,016 step [ 166], lr [0.0003000], embedding loss [ 0.7512], quantization loss [ 0.1736],  0.50 sec/batch.
2022-10-19 02:02:45,829 step [ 167], lr [0.0003000], embedding loss [ 0.7433], quantization loss [ 0.1955],  0.49 sec/batch.
2022-10-19 02:02:47,707 step [ 168], lr [0.0003000], embedding loss [ 0.7439], quantization loss [ 0.1614],  0.48 sec/batch.
2022-10-19 02:02:49,554 step [ 169], lr [0.0003000], embedding loss [ 0.7419], quantization loss [ 0.1688],  0.52 sec/batch.
2022-10-19 02:02:51,380 step [ 170], lr [0.0003000], embedding loss [ 0.7499], quantization loss [ 0.1636],  0.50 sec/batch.
2022-10-19 02:02:53,310 step [ 171], lr [0.0003000], embedding loss [ 0.7582], quantization loss [ 0.2013],  0.52 sec/batch.
2022-10-19 02:02:55,131 step [ 172], lr [0.0003000], embedding loss [ 0.7511], quantization loss [ 0.1822],  0.51 sec/batch.
2022-10-19 02:02:57,096 step [ 173], lr [0.0003000], embedding loss [ 0.7535], quantization loss [ 0.1488],  0.53 sec/batch.
2022-10-19 02:02:58,990 step [ 174], lr [0.0003000], embedding loss [ 0.7473], quantization loss [ 0.1967],  0.50 sec/batch.
2022-10-19 02:03:00,857 step [ 175], lr [0.0003000], embedding loss [ 0.7507], quantization loss [ 0.1698],  0.51 sec/batch.
2022-10-19 02:03:02,708 step [ 176], lr [0.0003000], embedding loss [ 0.7417], quantization loss [ 0.1662],  0.53 sec/batch.
2022-10-19 02:03:04,597 step [ 177], lr [0.0003000], embedding loss [ 0.7534], quantization loss [ 0.1636],  0.53 sec/batch.
2022-10-19 02:03:06,370 step [ 178], lr [0.0003000], embedding loss [ 0.7486], quantization loss [ 0.1586],  0.50 sec/batch.
2022-10-19 02:03:08,198 step [ 179], lr [0.0003000], embedding loss [ 0.7567], quantization loss [ 0.1716],  0.50 sec/batch.
2022-10-19 02:03:09,994 step [ 180], lr [0.0003000], embedding loss [ 0.7450], quantization loss [ 0.1686],  0.52 sec/batch.
2022-10-19 02:03:11,854 step [ 181], lr [0.0003000], embedding loss [ 0.7515], quantization loss [ 0.1584],  0.51 sec/batch.
2022-10-19 02:03:13,707 step [ 182], lr [0.0003000], embedding loss [ 0.7522], quantization loss [ 0.1515],  0.51 sec/batch.
2022-10-19 02:03:15,532 step [ 183], lr [0.0003000], embedding loss [ 0.7524], quantization loss [ 0.1620],  0.50 sec/batch.
2022-10-19 02:03:17,379 step [ 184], lr [0.0003000], embedding loss [ 0.7473], quantization loss [ 0.1555],  0.51 sec/batch.
2022-10-19 02:03:19,213 step [ 185], lr [0.0003000], embedding loss [ 0.7554], quantization loss [ 0.1725],  0.51 sec/batch.
2022-10-19 02:03:21,141 step [ 186], lr [0.0003000], embedding loss [ 0.7428], quantization loss [ 0.1733],  0.51 sec/batch.
2022-10-19 02:03:22,996 step [ 187], lr [0.0003000], embedding loss [ 0.7504], quantization loss [ 0.1524],  0.51 sec/batch.
2022-10-19 02:03:24,909 step [ 188], lr [0.0003000], embedding loss [ 0.7555], quantization loss [ 0.1496],  0.56 sec/batch.
2022-10-19 02:03:26,752 step [ 189], lr [0.0003000], embedding loss [ 0.7577], quantization loss [ 0.1809],  0.50 sec/batch.
2022-10-19 02:03:28,650 step [ 190], lr [0.0003000], embedding loss [ 0.7517], quantization loss [ 0.1581],  0.51 sec/batch.
2022-10-19 02:03:30,502 step [ 191], lr [0.0003000], embedding loss [ 0.7387], quantization loss [ 0.1594],  0.51 sec/batch.
2022-10-19 02:03:32,281 step [ 192], lr [0.0003000], embedding loss [ 0.7428], quantization loss [ 0.1592],  0.51 sec/batch.
2022-10-19 02:03:34,181 step [ 193], lr [0.0003000], embedding loss [ 0.7406], quantization loss [ 0.1561],  0.52 sec/batch.
2022-10-19 02:03:36,048 step [ 194], lr [0.0003000], embedding loss [ 0.7405], quantization loss [ 0.1534],  0.51 sec/batch.
2022-10-19 02:03:37,923 step [ 195], lr [0.0003000], embedding loss [ 0.7529], quantization loss [ 0.1632],  0.51 sec/batch.
2022-10-19 02:03:39,787 step [ 196], lr [0.0003000], embedding loss [ 0.7549], quantization loss [ 0.1485],  0.51 sec/batch.
2022-10-19 02:03:41,632 step [ 197], lr [0.0003000], embedding loss [ 0.7537], quantization loss [ 0.1546],  0.51 sec/batch.
2022-10-19 02:03:43,528 step [ 198], lr [0.0003000], embedding loss [ 0.7488], quantization loss [ 0.1677],  0.51 sec/batch.
2022-10-19 02:03:45,389 step [ 199], lr [0.0003000], embedding loss [ 0.7418], quantization loss [ 0.1417],  0.50 sec/batch.
2022-10-19 02:03:47,289 step [ 200], lr [0.0003000], embedding loss [ 0.7516], quantization loss [ 0.1548],  0.51 sec/batch.
2022-10-19 02:03:49,247 step [ 201], lr [0.0003000], embedding loss [ 0.7558], quantization loss [ 0.1542],  0.52 sec/batch.
2022-10-19 02:03:51,128 step [ 202], lr [0.0003000], embedding loss [ 0.7440], quantization loss [ 0.1439],  0.51 sec/batch.
2022-10-19 02:03:52,970 step [ 203], lr [0.0003000], embedding loss [ 0.7507], quantization loss [ 0.1474],  0.51 sec/batch.
2022-10-19 02:03:54,741 step [ 204], lr [0.0003000], embedding loss [ 0.7414], quantization loss [ 0.1325],  0.50 sec/batch.
2022-10-19 02:03:56,554 step [ 205], lr [0.0003000], embedding loss [ 0.7496], quantization loss [ 0.1427],  0.50 sec/batch.
2022-10-19 02:03:58,379 step [ 206], lr [0.0003000], embedding loss [ 0.7488], quantization loss [ 0.1642],  0.50 sec/batch.
2022-10-19 02:04:00,236 step [ 207], lr [0.0003000], embedding loss [ 0.7602], quantization loss [ 0.1381],  0.51 sec/batch.
2022-10-19 02:04:02,099 step [ 208], lr [0.0003000], embedding loss [ 0.7540], quantization loss [ 0.1536],  0.51 sec/batch.
2022-10-19 02:04:04,023 step [ 209], lr [0.0003000], embedding loss [ 0.7446], quantization loss [ 0.1219],  0.49 sec/batch.
2022-10-19 02:04:05,882 step [ 210], lr [0.0003000], embedding loss [ 0.7471], quantization loss [ 0.1414],  0.50 sec/batch.
2022-10-19 02:04:07,721 step [ 211], lr [0.0003000], embedding loss [ 0.7462], quantization loss [ 0.1363],  0.50 sec/batch.
2022-10-19 02:04:09,554 step [ 212], lr [0.0003000], embedding loss [ 0.7497], quantization loss [ 0.1322],  0.50 sec/batch.
2022-10-19 02:04:11,329 step [ 213], lr [0.0003000], embedding loss [ 0.7571], quantization loss [ 0.1394],  0.50 sec/batch.
2022-10-19 02:04:13,160 step [ 214], lr [0.0003000], embedding loss [ 0.7363], quantization loss [ 0.1366],  0.51 sec/batch.
2022-10-19 02:04:15,025 step [ 215], lr [0.0003000], embedding loss [ 0.7414], quantization loss [ 0.1330],  0.51 sec/batch.
2022-10-19 02:04:16,955 step [ 216], lr [0.0003000], embedding loss [ 0.7403], quantization loss [ 0.1374],  0.51 sec/batch.
2022-10-19 02:04:18,854 step [ 217], lr [0.0003000], embedding loss [ 0.7595], quantization loss [ 0.1313],  0.50 sec/batch.
2022-10-19 02:04:20,747 step [ 218], lr [0.0003000], embedding loss [ 0.7445], quantization loss [ 0.1365],  0.52 sec/batch.
2022-10-19 02:04:22,609 step [ 219], lr [0.0003000], embedding loss [ 0.7385], quantization loss [ 0.1350],  0.51 sec/batch.
2022-10-19 02:04:24,484 step [ 220], lr [0.0003000], embedding loss [ 0.7564], quantization loss [ 0.1608],  0.51 sec/batch.
2022-10-19 02:04:26,373 step [ 221], lr [0.0003000], embedding loss [ 0.7539], quantization loss [ 0.1401],  0.51 sec/batch.
2022-10-19 02:04:28,256 step [ 222], lr [0.0003000], embedding loss [ 0.7430], quantization loss [ 0.1417],  0.51 sec/batch.
2022-10-19 02:04:30,018 step [ 223], lr [0.0003000], embedding loss [ 0.7345], quantization loss [ 0.1478],  0.50 sec/batch.
2022-10-19 02:04:31,816 step [ 224], lr [0.0003000], embedding loss [ 0.7487], quantization loss [ 0.1200],  0.50 sec/batch.
2022-10-19 02:04:33,697 step [ 225], lr [0.0003000], embedding loss [ 0.7380], quantization loss [ 0.1506],  0.51 sec/batch.
2022-10-19 02:04:35,850 step [ 226], lr [0.0003000], embedding loss [ 0.7496], quantization loss [ 0.1487],  0.56 sec/batch.
2022-10-19 02:04:37,950 step [ 227], lr [0.0003000], embedding loss [ 0.7525], quantization loss [ 0.1401],  0.52 sec/batch.
2022-10-19 02:04:40,013 step [ 228], lr [0.0003000], embedding loss [ 0.7477], quantization loss [ 0.1400],  0.49 sec/batch.
2022-10-19 02:04:41,888 step [ 229], lr [0.0003000], embedding loss [ 0.7508], quantization loss [ 0.1415],  0.51 sec/batch.
2022-10-19 02:04:43,747 step [ 230], lr [0.0003000], embedding loss [ 0.7553], quantization loss [ 0.1323],  0.50 sec/batch.
2022-10-19 02:04:45,781 step [ 231], lr [0.0003000], embedding loss [ 0.7601], quantization loss [ 0.1305],  0.57 sec/batch.
2022-10-19 02:04:47,695 step [ 232], lr [0.0003000], embedding loss [ 0.7437], quantization loss [ 0.1239],  0.52 sec/batch.
2022-10-19 02:04:49,590 step [ 233], lr [0.0003000], embedding loss [ 0.7395], quantization loss [ 0.1415],  0.55 sec/batch.
2022-10-19 02:04:51,724 step [ 234], lr [0.0003000], embedding loss [ 0.7462], quantization loss [ 0.1404],  0.51 sec/batch.
2022-10-19 02:04:53,667 step [ 235], lr [0.0003000], embedding loss [ 0.7388], quantization loss [ 0.1384],  0.53 sec/batch.
2022-10-19 02:04:55,563 step [ 236], lr [0.0003000], embedding loss [ 0.7400], quantization loss [ 0.1210],  0.55 sec/batch.
2022-10-19 02:04:57,523 step [ 237], lr [0.0003000], embedding loss [ 0.7481], quantization loss [ 0.1465],  0.56 sec/batch.
2022-10-19 02:04:59,409 step [ 238], lr [0.0003000], embedding loss [ 0.7304], quantization loss [ 0.1545],  0.51 sec/batch.
2022-10-19 02:05:01,404 step [ 239], lr [0.0003000], embedding loss [ 0.7538], quantization loss [ 0.1409],  0.54 sec/batch.
2022-10-19 02:05:03,352 step [ 240], lr [0.0003000], embedding loss [ 0.7483], quantization loss [ 0.1179],  0.58 sec/batch.
2022-10-19 02:05:05,200 step [ 241], lr [0.0003000], embedding loss [ 0.7432], quantization loss [ 0.1151],  0.51 sec/batch.
2022-10-19 02:05:05,200 update codes and centers iter(1/1).
2022-10-19 02:05:08,409 number of update_code wrong: 0.
2022-10-19 02:05:11,392 non zero codewords: 1024.
2022-10-19 02:05:11,392 finish center update, duration: 6.19 sec.
2022-10-19 02:05:13,384 step [ 242], lr [0.0003000], embedding loss [ 0.7442], quantization loss [ 0.1141],  0.53 sec/batch.
2022-10-19 02:05:15,347 step [ 243], lr [0.0003000], embedding loss [ 0.7393], quantization loss [ 0.0982],  0.53 sec/batch.
2022-10-19 02:05:17,301 step [ 244], lr [0.0003000], embedding loss [ 0.7477], quantization loss [ 0.1004],  0.51 sec/batch.
2022-10-19 02:05:19,249 step [ 245], lr [0.0003000], embedding loss [ 0.7484], quantization loss [ 0.1255],  0.52 sec/batch.
2022-10-19 02:05:21,185 step [ 246], lr [0.0003000], embedding loss [ 0.7610], quantization loss [ 0.1157],  0.50 sec/batch.
2022-10-19 02:05:23,019 step [ 247], lr [0.0003000], embedding loss [ 0.7395], quantization loss [ 0.1176],  0.50 sec/batch.
2022-10-19 02:05:24,859 step [ 248], lr [0.0003000], embedding loss [ 0.7450], quantization loss [ 0.1112],  0.50 sec/batch.
2022-10-19 02:05:26,692 step [ 249], lr [0.0003000], embedding loss [ 0.7391], quantization loss [ 0.1014],  0.51 sec/batch.
2022-10-19 02:05:28,594 step [ 250], lr [0.0003000], embedding loss [ 0.7479], quantization loss [ 0.1039],  0.51 sec/batch.
2022-10-19 02:05:30,442 step [ 251], lr [0.0003000], embedding loss [ 0.7390], quantization loss [ 0.1000],  0.51 sec/batch.
2022-10-19 02:05:32,346 step [ 252], lr [0.0003000], embedding loss [ 0.7506], quantization loss [ 0.1076],  0.51 sec/batch.
2022-10-19 02:05:34,204 step [ 253], lr [0.0003000], embedding loss [ 0.7502], quantization loss [ 0.0935],  0.50 sec/batch.
2022-10-19 02:05:35,988 step [ 254], lr [0.0003000], embedding loss [ 0.7421], quantization loss [ 0.1075],  0.49 sec/batch.
2022-10-19 02:05:37,875 step [ 255], lr [0.0003000], embedding loss [ 0.7448], quantization loss [ 0.1044],  0.51 sec/batch.
2022-10-19 02:05:39,807 step [ 256], lr [0.0003000], embedding loss [ 0.7583], quantization loss [ 0.1031],  0.54 sec/batch.
2022-10-19 02:05:41,758 step [ 257], lr [0.0003000], embedding loss [ 0.7504], quantization loss [ 0.0969],  0.52 sec/batch.
2022-10-19 02:05:43,640 step [ 258], lr [0.0003000], embedding loss [ 0.7519], quantization loss [ 0.1137],  0.51 sec/batch.
2022-10-19 02:05:45,534 step [ 259], lr [0.0003000], embedding loss [ 0.7418], quantization loss [ 0.1107],  0.50 sec/batch.
2022-10-19 02:05:47,439 step [ 260], lr [0.0003000], embedding loss [ 0.7412], quantization loss [ 0.1062],  0.51 sec/batch.
2022-10-19 02:05:49,386 step [ 261], lr [0.0003000], embedding loss [ 0.7376], quantization loss [ 0.1035],  0.51 sec/batch.
2022-10-19 02:05:51,176 step [ 262], lr [0.0003000], embedding loss [ 0.7559], quantization loss [ 0.1154],  0.50 sec/batch.
2022-10-19 02:05:52,961 step [ 263], lr [0.0003000], embedding loss [ 0.7438], quantization loss [ 0.1026],  0.50 sec/batch.
2022-10-19 02:05:54,803 step [ 264], lr [0.0003000], embedding loss [ 0.7441], quantization loss [ 0.0875],  0.51 sec/batch.
2022-10-19 02:05:56,702 step [ 265], lr [0.0003000], embedding loss [ 0.7480], quantization loss [ 0.1110],  0.51 sec/batch.
2022-10-19 02:05:58,551 step [ 266], lr [0.0003000], embedding loss [ 0.7516], quantization loss [ 0.1064],  0.50 sec/batch.
2022-10-19 02:06:00,438 step [ 267], lr [0.0003000], embedding loss [ 0.7406], quantization loss [ 0.1014],  0.50 sec/batch.
2022-10-19 02:06:02,269 step [ 268], lr [0.0003000], embedding loss [ 0.7398], quantization loss [ 0.0968],  0.51 sec/batch.
2022-10-19 02:06:04,159 step [ 269], lr [0.0003000], embedding loss [ 0.7488], quantization loss [ 0.0977],  0.51 sec/batch.
2022-10-19 02:06:06,107 step [ 270], lr [0.0003000], embedding loss [ 0.7391], quantization loss [ 0.0913],  0.53 sec/batch.
2022-10-19 02:06:08,015 step [ 271], lr [0.0003000], embedding loss [ 0.7475], quantization loss [ 0.0977],  0.51 sec/batch.
2022-10-19 02:06:09,919 step [ 272], lr [0.0003000], embedding loss [ 0.7377], quantization loss [ 0.0979],  0.52 sec/batch.
2022-10-19 02:06:11,847 step [ 273], lr [0.0003000], embedding loss [ 0.7509], quantization loss [ 0.0945],  0.50 sec/batch.
2022-10-19 02:06:13,774 step [ 274], lr [0.0003000], embedding loss [ 0.7522], quantization loss [ 0.0991],  0.51 sec/batch.
2022-10-19 02:06:15,608 step [ 275], lr [0.0003000], embedding loss [ 0.7354], quantization loss [ 0.1055],  0.49 sec/batch.
2022-10-19 02:06:17,502 step [ 276], lr [0.0003000], embedding loss [ 0.7493], quantization loss [ 0.1007],  0.51 sec/batch.
2022-10-19 02:06:19,303 step [ 277], lr [0.0003000], embedding loss [ 0.7432], quantization loss [ 0.0917],  0.50 sec/batch.
2022-10-19 02:06:21,218 step [ 278], lr [0.0003000], embedding loss [ 0.7464], quantization loss [ 0.0879],  0.51 sec/batch.
2022-10-19 02:06:23,138 step [ 279], lr [0.0003000], embedding loss [ 0.7422], quantization loss [ 0.1034],  0.51 sec/batch.
2022-10-19 02:06:24,978 step [ 280], lr [0.0003000], embedding loss [ 0.7458], quantization loss [ 0.0947],  0.50 sec/batch.
2022-10-19 02:06:26,852 step [ 281], lr [0.0003000], embedding loss [ 0.7400], quantization loss [ 0.0881],  0.51 sec/batch.
2022-10-19 02:06:28,694 step [ 282], lr [0.0003000], embedding loss [ 0.7371], quantization loss [ 0.0913],  0.51 sec/batch.
2022-10-19 02:06:30,605 step [ 283], lr [0.0003000], embedding loss [ 0.7307], quantization loss [ 0.0811],  0.51 sec/batch.
2022-10-19 02:06:32,513 step [ 284], lr [0.0003000], embedding loss [ 0.7453], quantization loss [ 0.0875],  0.51 sec/batch.
2022-10-19 02:06:34,392 step [ 285], lr [0.0003000], embedding loss [ 0.7478], quantization loss [ 0.0977],  0.51 sec/batch.
2022-10-19 02:06:36,333 step [ 286], lr [0.0003000], embedding loss [ 0.7542], quantization loss [ 0.1021],  0.51 sec/batch.
2022-10-19 02:06:38,176 step [ 287], lr [0.0003000], embedding loss [ 0.7371], quantization loss [ 0.0817],  0.49 sec/batch.
2022-10-19 02:06:40,061 step [ 288], lr [0.0003000], embedding loss [ 0.7399], quantization loss [ 0.0851],  0.50 sec/batch.
2022-10-19 02:06:41,977 step [ 289], lr [0.0003000], embedding loss [ 0.7432], quantization loss [ 0.0892],  0.51 sec/batch.
2022-10-19 02:06:43,903 step [ 290], lr [0.0003000], embedding loss [ 0.7450], quantization loss [ 0.0901],  0.50 sec/batch.
2022-10-19 02:06:45,794 step [ 291], lr [0.0003000], embedding loss [ 0.7608], quantization loss [ 0.0945],  0.51 sec/batch.
2022-10-19 02:06:47,705 step [ 292], lr [0.0003000], embedding loss [ 0.7566], quantization loss [ 0.0891],  0.52 sec/batch.
2022-10-19 02:06:49,672 step [ 293], lr [0.0003000], embedding loss [ 0.7510], quantization loss [ 0.0891],  0.51 sec/batch.
2022-10-19 02:06:51,444 step [ 294], lr [0.0003000], embedding loss [ 0.7480], quantization loss [ 0.0873],  0.48 sec/batch.
2022-10-19 02:06:53,327 step [ 295], lr [0.0003000], embedding loss [ 0.7546], quantization loss [ 0.0877],  0.51 sec/batch.
2022-10-19 02:06:55,171 step [ 296], lr [0.0003000], embedding loss [ 0.7424], quantization loss [ 0.0795],  0.51 sec/batch.
2022-10-19 02:06:57,045 step [ 297], lr [0.0003000], embedding loss [ 0.7446], quantization loss [ 0.0812],  0.50 sec/batch.
2022-10-19 02:06:58,963 step [ 298], lr [0.0003000], embedding loss [ 0.7423], quantization loss [ 0.0877],  0.54 sec/batch.
2022-10-19 02:07:00,884 step [ 299], lr [0.0003000], embedding loss [ 0.7460], quantization loss [ 0.0932],  0.53 sec/batch.
2022-10-19 02:07:02,754 step [ 300], lr [0.0003000], embedding loss [ 0.7445], quantization loss [ 0.0880],  0.50 sec/batch.
2022-10-19 02:07:04,649 step [ 301], lr [0.0001500], embedding loss [ 0.7506], quantization loss [ 0.0858],  0.51 sec/batch.
2022-10-19 02:07:06,504 step [ 302], lr [0.0001500], embedding loss [ 0.7424], quantization loss [ 0.0965],  0.48 sec/batch.
2022-10-19 02:07:08,421 step [ 303], lr [0.0001500], embedding loss [ 0.7498], quantization loss [ 0.0827],  0.52 sec/batch.
2022-10-19 02:07:10,370 step [ 304], lr [0.0001500], embedding loss [ 0.7277], quantization loss [ 0.0803],  0.52 sec/batch.
2022-10-19 02:07:12,311 step [ 305], lr [0.0001500], embedding loss [ 0.7213], quantization loss [ 0.0840],  0.49 sec/batch.
2022-10-19 02:07:14,225 step [ 306], lr [0.0001500], embedding loss [ 0.7361], quantization loss [ 0.0829],  0.51 sec/batch.
2022-10-19 02:07:16,141 step [ 307], lr [0.0001500], embedding loss [ 0.7293], quantization loss [ 0.0893],  0.52 sec/batch.
2022-10-19 02:07:18,062 step [ 308], lr [0.0001500], embedding loss [ 0.7607], quantization loss [ 0.0872],  0.51 sec/batch.
2022-10-19 02:07:19,976 step [ 309], lr [0.0001500], embedding loss [ 0.7476], quantization loss [ 0.0893],  0.51 sec/batch.
2022-10-19 02:07:21,879 step [ 310], lr [0.0001500], embedding loss [ 0.7424], quantization loss [ 0.0780],  0.51 sec/batch.
2022-10-19 02:07:23,793 step [ 311], lr [0.0001500], embedding loss [ 0.7423], quantization loss [ 0.0896],  0.52 sec/batch.
2022-10-19 02:07:25,727 step [ 312], lr [0.0001500], embedding loss [ 0.7528], quantization loss [ 0.0843],  0.52 sec/batch.
2022-10-19 02:07:27,629 step [ 313], lr [0.0001500], embedding loss [ 0.7345], quantization loss [ 0.0772],  0.52 sec/batch.
2022-10-19 02:07:29,510 step [ 314], lr [0.0001500], embedding loss [ 0.7445], quantization loss [ 0.0790],  0.51 sec/batch.
2022-10-19 02:07:31,385 step [ 315], lr [0.0001500], embedding loss [ 0.7313], quantization loss [ 0.0833],  0.51 sec/batch.
2022-10-19 02:07:33,362 step [ 316], lr [0.0001500], embedding loss [ 0.7441], quantization loss [ 0.0816],  0.51 sec/batch.
2022-10-19 02:07:35,342 step [ 317], lr [0.0001500], embedding loss [ 0.7540], quantization loss [ 0.0738],  0.52 sec/batch.
2022-10-19 02:07:37,238 step [ 318], lr [0.0001500], embedding loss [ 0.7383], quantization loss [ 0.0889],  0.51 sec/batch.
2022-10-19 02:07:39,147 step [ 319], lr [0.0001500], embedding loss [ 0.7474], quantization loss [ 0.0869],  0.51 sec/batch.
2022-10-19 02:07:41,065 step [ 320], lr [0.0001500], embedding loss [ 0.7416], quantization loss [ 0.0820],  0.51 sec/batch.
2022-10-19 02:07:43,002 step [ 321], lr [0.0001500], embedding loss [ 0.7347], quantization loss [ 0.0781],  0.51 sec/batch.
2022-10-19 02:07:43,002 update codes and centers iter(1/1).
2022-10-19 02:07:46,170 number of update_code wrong: 0.
2022-10-19 02:07:49,135 non zero codewords: 1024.
2022-10-19 02:07:49,135 finish center update, duration: 6.13 sec.
2022-10-19 02:07:50,974 step [ 322], lr [0.0001500], embedding loss [ 0.7459], quantization loss [ 0.0723],  0.52 sec/batch.
2022-10-19 02:07:52,910 step [ 323], lr [0.0001500], embedding loss [ 0.7416], quantization loss [ 0.0734],  0.51 sec/batch.
2022-10-19 02:07:54,833 step [ 324], lr [0.0001500], embedding loss [ 0.7523], quantization loss [ 0.0698],  0.51 sec/batch.
2022-10-19 02:07:56,796 step [ 325], lr [0.0001500], embedding loss [ 0.7298], quantization loss [ 0.0758],  0.52 sec/batch.
2022-10-19 02:07:58,730 step [ 326], lr [0.0001500], embedding loss [ 0.7489], quantization loss [ 0.0684],  0.51 sec/batch.
2022-10-19 02:08:00,713 step [ 327], lr [0.0001500], embedding loss [ 0.7430], quantization loss [ 0.0714],  0.51 sec/batch.
2022-10-19 02:08:02,559 step [ 328], lr [0.0001500], embedding loss [ 0.7484], quantization loss [ 0.0638],  0.50 sec/batch.
2022-10-19 02:08:04,392 step [ 329], lr [0.0001500], embedding loss [ 0.7465], quantization loss [ 0.0733],  0.50 sec/batch.
2022-10-19 02:08:06,339 step [ 330], lr [0.0001500], embedding loss [ 0.7510], quantization loss [ 0.0675],  0.51 sec/batch.
2022-10-19 02:08:08,198 step [ 331], lr [0.0001500], embedding loss [ 0.7392], quantization loss [ 0.0745],  0.50 sec/batch.
2022-10-19 02:08:10,078 step [ 332], lr [0.0001500], embedding loss [ 0.7424], quantization loss [ 0.0714],  0.50 sec/batch.
2022-10-19 02:08:11,997 step [ 333], lr [0.0001500], embedding loss [ 0.7351], quantization loss [ 0.0814],  0.51 sec/batch.
2022-10-19 02:08:13,906 step [ 334], lr [0.0001500], embedding loss [ 0.7518], quantization loss [ 0.0723],  0.51 sec/batch.
2022-10-19 02:08:15,870 step [ 335], lr [0.0001500], embedding loss [ 0.7542], quantization loss [ 0.0721],  0.51 sec/batch.
2022-10-19 02:08:17,832 step [ 336], lr [0.0001500], embedding loss [ 0.7286], quantization loss [ 0.0685],  0.51 sec/batch.
2022-10-19 02:08:19,786 step [ 337], lr [0.0001500], embedding loss [ 0.7397], quantization loss [ 0.0733],  0.51 sec/batch.
2022-10-19 02:08:21,738 step [ 338], lr [0.0001500], embedding loss [ 0.7365], quantization loss [ 0.0745],  0.50 sec/batch.
2022-10-19 02:08:23,621 step [ 339], lr [0.0001500], embedding loss [ 0.7487], quantization loss [ 0.0706],  0.49 sec/batch.
2022-10-19 02:08:25,523 step [ 340], lr [0.0001500], embedding loss [ 0.7272], quantization loss [ 0.0727],  0.52 sec/batch.
2022-10-19 02:08:27,420 step [ 341], lr [0.0001500], embedding loss [ 0.7278], quantization loss [ 0.0655],  0.50 sec/batch.
2022-10-19 02:08:29,332 step [ 342], lr [0.0001500], embedding loss [ 0.7443], quantization loss [ 0.0720],  0.52 sec/batch.
2022-10-19 02:08:31,170 step [ 343], lr [0.0001500], embedding loss [ 0.7431], quantization loss [ 0.0713],  0.50 sec/batch.
2022-10-19 02:08:33,036 step [ 344], lr [0.0001500], embedding loss [ 0.7518], quantization loss [ 0.0751],  0.50 sec/batch.
2022-10-19 02:08:34,946 step [ 345], lr [0.0001500], embedding loss [ 0.7303], quantization loss [ 0.0761],  0.50 sec/batch.
2022-10-19 02:08:36,879 step [ 346], lr [0.0001500], embedding loss [ 0.7486], quantization loss [ 0.0652],  0.54 sec/batch.
2022-10-19 02:08:38,874 step [ 347], lr [0.0001500], embedding loss [ 0.7464], quantization loss [ 0.0757],  0.54 sec/batch.
2022-10-19 02:08:40,873 step [ 348], lr [0.0001500], embedding loss [ 0.7337], quantization loss [ 0.0776],  0.56 sec/batch.
2022-10-19 02:08:42,747 step [ 349], lr [0.0001500], embedding loss [ 0.7445], quantization loss [ 0.0742],  0.51 sec/batch.
2022-10-19 02:08:44,749 step [ 350], lr [0.0001500], embedding loss [ 0.7395], quantization loss [ 0.0676],  0.52 sec/batch.
2022-10-19 02:08:46,660 step [ 351], lr [0.0001500], embedding loss [ 0.7313], quantization loss [ 0.0604],  0.51 sec/batch.
2022-10-19 02:08:48,593 step [ 352], lr [0.0001500], embedding loss [ 0.7405], quantization loss [ 0.0694],  0.52 sec/batch.
2022-10-19 02:08:50,568 step [ 353], lr [0.0001500], embedding loss [ 0.7382], quantization loss [ 0.0753],  0.52 sec/batch.
2022-10-19 02:08:52,541 step [ 354], lr [0.0001500], embedding loss [ 0.7384], quantization loss [ 0.0740],  0.52 sec/batch.
2022-10-19 02:08:54,553 step [ 355], lr [0.0001500], embedding loss [ 0.7467], quantization loss [ 0.0708],  0.53 sec/batch.
2022-10-19 02:08:56,537 step [ 356], lr [0.0001500], embedding loss [ 0.7520], quantization loss [ 0.0682],  0.52 sec/batch.
2022-10-19 02:08:58,479 step [ 357], lr [0.0001500], embedding loss [ 0.7467], quantization loss [ 0.0710],  0.51 sec/batch.
2022-10-19 02:09:00,389 step [ 358], lr [0.0001500], embedding loss [ 0.7369], quantization loss [ 0.0733],  0.51 sec/batch.
2022-10-19 02:09:02,391 step [ 359], lr [0.0001500], embedding loss [ 0.7455], quantization loss [ 0.0785],  0.51 sec/batch.
2022-10-19 02:09:04,297 step [ 360], lr [0.0001500], embedding loss [ 0.7447], quantization loss [ 0.0783],  0.51 sec/batch.
2022-10-19 02:09:06,229 step [ 361], lr [0.0001500], embedding loss [ 0.7509], quantization loss [ 0.0691],  0.51 sec/batch.
2022-10-19 02:09:08,114 step [ 362], lr [0.0001500], embedding loss [ 0.7380], quantization loss [ 0.0665],  0.51 sec/batch.
2022-10-19 02:09:10,085 step [ 363], lr [0.0001500], embedding loss [ 0.7381], quantization loss [ 0.0635],  0.51 sec/batch.
2022-10-19 02:09:12,037 step [ 364], lr [0.0001500], embedding loss [ 0.7401], quantization loss [ 0.0690],  0.51 sec/batch.
2022-10-19 02:09:13,885 step [ 365], lr [0.0001500], embedding loss [ 0.7371], quantization loss [ 0.0659],  0.51 sec/batch.
2022-10-19 02:09:15,874 step [ 366], lr [0.0001500], embedding loss [ 0.7428], quantization loss [ 0.0665],  0.54 sec/batch.
2022-10-19 02:09:17,803 step [ 367], lr [0.0001500], embedding loss [ 0.7457], quantization loss [ 0.0722],  0.51 sec/batch.
2022-10-19 02:09:19,762 step [ 368], lr [0.0001500], embedding loss [ 0.7298], quantization loss [ 0.0684],  0.51 sec/batch.
2022-10-19 02:09:21,656 step [ 369], lr [0.0001500], embedding loss [ 0.7425], quantization loss [ 0.0653],  0.50 sec/batch.
2022-10-19 02:09:23,622 step [ 370], lr [0.0001500], embedding loss [ 0.7462], quantization loss [ 0.0672],  0.51 sec/batch.
2022-10-19 02:09:25,571 step [ 371], lr [0.0001500], embedding loss [ 0.7499], quantization loss [ 0.0666],  0.50 sec/batch.
2022-10-19 02:09:27,458 step [ 372], lr [0.0001500], embedding loss [ 0.7376], quantization loss [ 0.0662],  0.49 sec/batch.
2022-10-19 02:09:29,411 step [ 373], lr [0.0001500], embedding loss [ 0.7435], quantization loss [ 0.0669],  0.49 sec/batch.
2022-10-19 02:09:31,303 step [ 374], lr [0.0001500], embedding loss [ 0.7457], quantization loss [ 0.0608],  0.50 sec/batch.
2022-10-19 02:09:33,217 step [ 375], lr [0.0001500], embedding loss [ 0.7313], quantization loss [ 0.0642],  0.50 sec/batch.
2022-10-19 02:09:35,132 step [ 376], lr [0.0001500], embedding loss [ 0.7357], quantization loss [ 0.0615],  0.52 sec/batch.
2022-10-19 02:09:37,059 step [ 377], lr [0.0001500], embedding loss [ 0.7367], quantization loss [ 0.0647],  0.52 sec/batch.
2022-10-19 02:09:38,983 step [ 378], lr [0.0001500], embedding loss [ 0.7358], quantization loss [ 0.0691],  0.51 sec/batch.
2022-10-19 02:09:40,875 step [ 379], lr [0.0001500], embedding loss [ 0.7409], quantization loss [ 0.0618],  0.51 sec/batch.
2022-10-19 02:09:42,821 step [ 380], lr [0.0001500], embedding loss [ 0.7435], quantization loss [ 0.0658],  0.52 sec/batch.
2022-10-19 02:09:44,771 step [ 381], lr [0.0001500], embedding loss [ 0.7483], quantization loss [ 0.0599],  0.51 sec/batch.
2022-10-19 02:09:46,689 step [ 382], lr [0.0001500], embedding loss [ 0.7374], quantization loss [ 0.0602],  0.51 sec/batch.
2022-10-19 02:09:48,635 step [ 383], lr [0.0001500], embedding loss [ 0.7401], quantization loss [ 0.0639],  0.52 sec/batch.
2022-10-19 02:09:50,563 step [ 384], lr [0.0001500], embedding loss [ 0.7489], quantization loss [ 0.0659],  0.51 sec/batch.
2022-10-19 02:09:52,498 step [ 385], lr [0.0001500], embedding loss [ 0.7400], quantization loss [ 0.0601],  0.51 sec/batch.
2022-10-19 02:09:54,422 step [ 386], lr [0.0001500], embedding loss [ 0.7405], quantization loss [ 0.0665],  0.50 sec/batch.
2022-10-19 02:09:56,581 step [ 387], lr [0.0001500], embedding loss [ 0.7354], quantization loss [ 0.0681],  0.51 sec/batch.
2022-10-19 02:09:58,456 step [ 388], lr [0.0001500], embedding loss [ 0.7445], quantization loss [ 0.0782],  0.51 sec/batch.
2022-10-19 02:10:00,293 step [ 389], lr [0.0001500], embedding loss [ 0.7401], quantization loss [ 0.0670],  0.52 sec/batch.
2022-10-19 02:10:02,126 step [ 390], lr [0.0001500], embedding loss [ 0.7433], quantization loss [ 0.0672],  0.52 sec/batch.
2022-10-19 02:10:03,954 step [ 391], lr [0.0001500], embedding loss [ 0.7413], quantization loss [ 0.0664],  0.51 sec/batch.
2022-10-19 02:10:05,804 step [ 392], lr [0.0001500], embedding loss [ 0.7370], quantization loss [ 0.0640],  0.54 sec/batch.
2022-10-19 02:10:07,697 step [ 393], lr [0.0001500], embedding loss [ 0.7420], quantization loss [ 0.0625],  0.52 sec/batch.
2022-10-19 02:10:09,638 step [ 394], lr [0.0001500], embedding loss [ 0.7325], quantization loss [ 0.0678],  0.57 sec/batch.
2022-10-19 02:10:11,712 step [ 395], lr [0.0001500], embedding loss [ 0.7472], quantization loss [ 0.0601],  0.58 sec/batch.
2022-10-19 02:10:13,827 step [ 396], lr [0.0001500], embedding loss [ 0.7382], quantization loss [ 0.0670],  0.56 sec/batch.
2022-10-19 02:10:15,794 step [ 397], lr [0.0001500], embedding loss [ 0.7491], quantization loss [ 0.0604],  0.51 sec/batch.
2022-10-19 02:10:17,754 step [ 398], lr [0.0001500], embedding loss [ 0.7354], quantization loss [ 0.0699],  0.56 sec/batch.
2022-10-19 02:10:19,625 step [ 399], lr [0.0001500], embedding loss [ 0.7424], quantization loss [ 0.0621],  0.54 sec/batch.
2022-10-19 02:10:21,624 step [ 400], lr [0.0001500], embedding loss [ 0.7388], quantization loss [ 0.0663],  0.56 sec/batch.
2022-10-19 02:10:23,477 step [ 401], lr [0.0001500], embedding loss [ 0.7241], quantization loss [ 0.0595],  0.52 sec/batch.
2022-10-19 02:10:23,477 update codes and centers iter(1/1).
2022-10-19 02:10:26,776 number of update_code wrong: 0.
2022-10-19 02:10:29,803 non zero codewords: 1024.
2022-10-19 02:10:29,804 finish center update, duration: 6.33 sec.
2022-10-19 02:10:31,813 step [ 402], lr [0.0001500], embedding loss [ 0.7455], quantization loss [ 0.0658],  0.53 sec/batch.
2022-10-19 02:10:33,810 step [ 403], lr [0.0001500], embedding loss [ 0.7389], quantization loss [ 0.0643],  0.52 sec/batch.
2022-10-19 02:10:35,836 step [ 404], lr [0.0001500], embedding loss [ 0.7370], quantization loss [ 0.0674],  0.51 sec/batch.
2022-10-19 02:10:37,820 step [ 405], lr [0.0001500], embedding loss [ 0.7324], quantization loss [ 0.0642],  0.54 sec/batch.
2022-10-19 02:10:39,778 step [ 406], lr [0.0001500], embedding loss [ 0.7467], quantization loss [ 0.0609],  0.51 sec/batch.
2022-10-19 02:10:41,840 step [ 407], lr [0.0001500], embedding loss [ 0.7273], quantization loss [ 0.0606],  0.52 sec/batch.
2022-10-19 02:10:43,859 step [ 408], lr [0.0001500], embedding loss [ 0.7472], quantization loss [ 0.0658],  0.51 sec/batch.
2022-10-19 02:10:45,896 step [ 409], lr [0.0001500], embedding loss [ 0.7344], quantization loss [ 0.0624],  0.52 sec/batch.
2022-10-19 02:10:47,869 step [ 410], lr [0.0001500], embedding loss [ 0.7498], quantization loss [ 0.0639],  0.52 sec/batch.
2022-10-19 02:10:49,855 step [ 411], lr [0.0001500], embedding loss [ 0.7464], quantization loss [ 0.0602],  0.51 sec/batch.
2022-10-19 02:10:51,762 step [ 412], lr [0.0001500], embedding loss [ 0.7339], quantization loss [ 0.0526],  0.51 sec/batch.
2022-10-19 02:10:53,756 step [ 413], lr [0.0001500], embedding loss [ 0.7440], quantization loss [ 0.0573],  0.51 sec/batch.
2022-10-19 02:10:55,691 step [ 414], lr [0.0001500], embedding loss [ 0.7452], quantization loss [ 0.0595],  0.51 sec/batch.
2022-10-19 02:10:57,607 step [ 415], lr [0.0001500], embedding loss [ 0.7479], quantization loss [ 0.0602],  0.56 sec/batch.
2022-10-19 02:10:59,677 step [ 416], lr [0.0001500], embedding loss [ 0.7457], quantization loss [ 0.0629],  0.54 sec/batch.
2022-10-19 02:11:01,619 step [ 417], lr [0.0001500], embedding loss [ 0.7445], quantization loss [ 0.0583],  0.52 sec/batch.
2022-10-19 02:11:03,657 step [ 418], lr [0.0001500], embedding loss [ 0.7405], quantization loss [ 0.0602],  0.52 sec/batch.
2022-10-19 02:11:05,631 step [ 419], lr [0.0001500], embedding loss [ 0.7304], quantization loss [ 0.0600],  0.52 sec/batch.
2022-10-19 02:11:07,588 step [ 420], lr [0.0001500], embedding loss [ 0.7500], quantization loss [ 0.0581],  0.51 sec/batch.
2022-10-19 02:11:09,570 step [ 421], lr [0.0001500], embedding loss [ 0.7349], quantization loss [ 0.0559],  0.51 sec/batch.
2022-10-19 02:11:11,563 step [ 422], lr [0.0001500], embedding loss [ 0.7388], quantization loss [ 0.0604],  0.53 sec/batch.
2022-10-19 02:11:13,550 step [ 423], lr [0.0001500], embedding loss [ 0.7424], quantization loss [ 0.0597],  0.51 sec/batch.
2022-10-19 02:11:15,468 step [ 424], lr [0.0001500], embedding loss [ 0.7438], quantization loss [ 0.0603],  0.51 sec/batch.
2022-10-19 02:11:17,374 step [ 425], lr [0.0001500], embedding loss [ 0.7353], quantization loss [ 0.0581],  0.51 sec/batch.
2022-10-19 02:11:19,265 step [ 426], lr [0.0001500], embedding loss [ 0.7513], quantization loss [ 0.0561],  0.54 sec/batch.
2022-10-19 02:11:21,245 step [ 427], lr [0.0001500], embedding loss [ 0.7276], quantization loss [ 0.0579],  0.51 sec/batch.
2022-10-19 02:11:23,124 step [ 428], lr [0.0001500], embedding loss [ 0.7409], quantization loss [ 0.0596],  0.51 sec/batch.
2022-10-19 02:11:25,040 step [ 429], lr [0.0001500], embedding loss [ 0.7345], quantization loss [ 0.0612],  0.51 sec/batch.
2022-10-19 02:11:26,973 step [ 430], lr [0.0001500], embedding loss [ 0.7467], quantization loss [ 0.0525],  0.51 sec/batch.
2022-10-19 02:11:28,949 step [ 431], lr [0.0001500], embedding loss [ 0.7351], quantization loss [ 0.0597],  0.51 sec/batch.
2022-10-19 02:11:30,896 step [ 432], lr [0.0001500], embedding loss [ 0.7444], quantization loss [ 0.0622],  0.50 sec/batch.
2022-10-19 02:11:32,810 step [ 433], lr [0.0001500], embedding loss [ 0.7281], quantization loss [ 0.0607],  0.51 sec/batch.
2022-10-19 02:11:34,784 step [ 434], lr [0.0001500], embedding loss [ 0.7451], quantization loss [ 0.0592],  0.50 sec/batch.
2022-10-19 02:11:36,735 step [ 435], lr [0.0001500], embedding loss [ 0.7339], quantization loss [ 0.0549],  0.51 sec/batch.
2022-10-19 02:11:38,653 step [ 436], lr [0.0001500], embedding loss [ 0.7383], quantization loss [ 0.0586],  0.51 sec/batch.
2022-10-19 02:11:40,652 step [ 437], lr [0.0001500], embedding loss [ 0.7209], quantization loss [ 0.0557],  0.51 sec/batch.
2022-10-19 02:11:42,552 step [ 438], lr [0.0001500], embedding loss [ 0.7392], quantization loss [ 0.0603],  0.51 sec/batch.
2022-10-19 02:11:44,577 step [ 439], lr [0.0001500], embedding loss [ 0.7398], quantization loss [ 0.0579],  0.51 sec/batch.
2022-10-19 02:11:46,446 step [ 440], lr [0.0001500], embedding loss [ 0.7412], quantization loss [ 0.0551],  0.50 sec/batch.
2022-10-19 02:11:48,439 step [ 441], lr [0.0001500], embedding loss [ 0.7556], quantization loss [ 0.0605],  0.51 sec/batch.
2022-10-19 02:11:50,338 step [ 442], lr [0.0001500], embedding loss [ 0.7464], quantization loss [ 0.0499],  0.51 sec/batch.
2022-10-19 02:11:52,297 step [ 443], lr [0.0001500], embedding loss [ 0.7291], quantization loss [ 0.0521],  0.52 sec/batch.
2022-10-19 02:11:54,290 step [ 444], lr [0.0001500], embedding loss [ 0.7495], quantization loss [ 0.0619],  0.54 sec/batch.
2022-10-19 02:11:56,545 step [ 445], lr [0.0001500], embedding loss [ 0.7376], quantization loss [ 0.0574],  0.58 sec/batch.
2022-10-19 02:11:58,612 step [ 446], lr [0.0001500], embedding loss [ 0.7301], quantization loss [ 0.0596],  0.52 sec/batch.
2022-10-19 02:12:00,657 step [ 447], lr [0.0001500], embedding loss [ 0.7335], quantization loss [ 0.0610],  0.58 sec/batch.
2022-10-19 02:12:02,683 step [ 448], lr [0.0001500], embedding loss [ 0.7319], quantization loss [ 0.0527],  0.58 sec/batch.
2022-10-19 02:12:04,997 step [ 449], lr [0.0001500], embedding loss [ 0.7428], quantization loss [ 0.0551],  0.54 sec/batch.
2022-10-19 02:12:07,021 step [ 450], lr [0.0001500], embedding loss [ 0.7358], quantization loss [ 0.0578],  0.55 sec/batch.
2022-10-19 02:12:08,941 step [ 451], lr [0.0001500], embedding loss [ 0.7384], quantization loss [ 0.0597],  0.54 sec/batch.
2022-10-19 02:12:10,861 step [ 452], lr [0.0001500], embedding loss [ 0.7314], quantization loss [ 0.0614],  0.50 sec/batch.
2022-10-19 02:12:13,023 step [ 453], lr [0.0001500], embedding loss [ 0.7486], quantization loss [ 0.0564],  0.52 sec/batch.
2022-10-19 02:12:14,963 step [ 454], lr [0.0001500], embedding loss [ 0.7378], quantization loss [ 0.0548],  0.54 sec/batch.
2022-10-19 02:12:16,905 step [ 455], lr [0.0001500], embedding loss [ 0.7345], quantization loss [ 0.0547],  0.52 sec/batch.
2022-10-19 02:12:18,813 step [ 456], lr [0.0001500], embedding loss [ 0.7458], quantization loss [ 0.0511],  0.51 sec/batch.
2022-10-19 02:12:20,854 step [ 457], lr [0.0001500], embedding loss [ 0.7414], quantization loss [ 0.0547],  0.54 sec/batch.
2022-10-19 02:12:22,880 step [ 458], lr [0.0001500], embedding loss [ 0.7507], quantization loss [ 0.0589],  0.52 sec/batch.
2022-10-19 02:12:24,857 step [ 459], lr [0.0001500], embedding loss [ 0.7402], quantization loss [ 0.0596],  0.53 sec/batch.
2022-10-19 02:12:26,836 step [ 460], lr [0.0001500], embedding loss [ 0.7259], quantization loss [ 0.0547],  0.57 sec/batch.
2022-10-19 02:12:28,888 step [ 461], lr [0.0001500], embedding loss [ 0.7349], quantization loss [ 0.0531],  0.54 sec/batch.
2022-10-19 02:12:30,990 step [ 462], lr [0.0001500], embedding loss [ 0.7335], quantization loss [ 0.0548],  0.57 sec/batch.
2022-10-19 02:12:33,057 step [ 463], lr [0.0001500], embedding loss [ 0.7367], quantization loss [ 0.0596],  0.56 sec/batch.
2022-10-19 02:12:35,177 step [ 464], lr [0.0001500], embedding loss [ 0.7479], quantization loss [ 0.0637],  0.57 sec/batch.
2022-10-19 02:12:37,405 step [ 465], lr [0.0001500], embedding loss [ 0.7356], quantization loss [ 0.0556],  0.57 sec/batch.
2022-10-19 02:12:39,426 step [ 466], lr [0.0001500], embedding loss [ 0.7382], quantization loss [ 0.0580],  0.57 sec/batch.
2022-10-19 02:12:41,490 step [ 467], lr [0.0001500], embedding loss [ 0.7385], quantization loss [ 0.0560],  0.53 sec/batch.
2022-10-19 02:12:43,454 step [ 468], lr [0.0001500], embedding loss [ 0.7301], quantization loss [ 0.0555],  0.54 sec/batch.
2022-10-19 02:12:45,509 step [ 469], lr [0.0001500], embedding loss [ 0.7368], quantization loss [ 0.0544],  0.54 sec/batch.
2022-10-19 02:12:47,469 step [ 470], lr [0.0001500], embedding loss [ 0.7406], quantization loss [ 0.0543],  0.52 sec/batch.
2022-10-19 02:12:49,467 step [ 471], lr [0.0001500], embedding loss [ 0.7305], quantization loss [ 0.0530],  0.54 sec/batch.
2022-10-19 02:12:51,441 step [ 472], lr [0.0001500], embedding loss [ 0.7441], quantization loss [ 0.0617],  0.54 sec/batch.
2022-10-19 02:12:53,327 step [ 473], lr [0.0001500], embedding loss [ 0.7413], quantization loss [ 0.0555],  0.52 sec/batch.
2022-10-19 02:12:55,342 step [ 474], lr [0.0001500], embedding loss [ 0.7356], quantization loss [ 0.0542],  0.53 sec/batch.
2022-10-19 02:12:57,298 step [ 475], lr [0.0001500], embedding loss [ 0.7432], quantization loss [ 0.0575],  0.57 sec/batch.
2022-10-19 02:12:59,412 step [ 476], lr [0.0001500], embedding loss [ 0.7437], quantization loss [ 0.0602],  0.58 sec/batch.
2022-10-19 02:13:01,445 step [ 477], lr [0.0001500], embedding loss [ 0.7264], quantization loss [ 0.0520],  0.51 sec/batch.
2022-10-19 02:13:03,371 step [ 478], lr [0.0001500], embedding loss [ 0.7374], quantization loss [ 0.0576],  0.51 sec/batch.
2022-10-19 02:13:05,331 step [ 479], lr [0.0001500], embedding loss [ 0.7412], quantization loss [ 0.0569],  0.51 sec/batch.
2022-10-19 02:13:07,303 step [ 480], lr [0.0001500], embedding loss [ 0.7270], quantization loss [ 0.0524],  0.52 sec/batch.
2022-10-19 02:13:09,212 step [ 481], lr [0.0001500], embedding loss [ 0.7428], quantization loss [ 0.0512],  0.52 sec/batch.
2022-10-19 02:13:09,212 update codes and centers iter(1/1).
2022-10-19 02:13:12,568 number of update_code wrong: 0.
2022-10-19 02:13:15,575 non zero codewords: 1024.
2022-10-19 02:13:15,576 finish center update, duration: 6.36 sec.
2022-10-19 02:13:17,619 step [ 482], lr [0.0001500], embedding loss [ 0.7410], quantization loss [ 0.0581],  0.54 sec/batch.
2022-10-19 02:13:19,625 step [ 483], lr [0.0001500], embedding loss [ 0.7352], quantization loss [ 0.0599],  0.58 sec/batch.
2022-10-19 02:13:21,703 step [ 484], lr [0.0001500], embedding loss [ 0.7299], quantization loss [ 0.0548],  0.56 sec/batch.
2022-10-19 02:13:23,724 step [ 485], lr [0.0001500], embedding loss [ 0.7373], quantization loss [ 0.0615],  0.53 sec/batch.
2022-10-19 02:13:25,806 step [ 486], lr [0.0001500], embedding loss [ 0.7288], quantization loss [ 0.0503],  0.54 sec/batch.
2022-10-19 02:13:27,893 step [ 487], lr [0.0001500], embedding loss [ 0.7308], quantization loss [ 0.0563],  0.54 sec/batch.
2022-10-19 02:13:30,115 step [ 488], lr [0.0001500], embedding loss [ 0.7304], quantization loss [ 0.0553],  0.53 sec/batch.
2022-10-19 02:13:32,125 step [ 489], lr [0.0001500], embedding loss [ 0.7365], quantization loss [ 0.0582],  0.52 sec/batch.
2022-10-19 02:13:34,228 step [ 490], lr [0.0001500], embedding loss [ 0.7365], quantization loss [ 0.0504],  0.57 sec/batch.
2022-10-19 02:13:36,166 step [ 491], lr [0.0001500], embedding loss [ 0.7267], quantization loss [ 0.0589],  0.55 sec/batch.
2022-10-19 02:13:38,342 step [ 492], lr [0.0001500], embedding loss [ 0.7455], quantization loss [ 0.0564],  0.52 sec/batch.
2022-10-19 02:13:40,392 step [ 493], lr [0.0001500], embedding loss [ 0.7329], quantization loss [ 0.0575],  0.53 sec/batch.
2022-10-19 02:13:42,393 step [ 494], lr [0.0001500], embedding loss [ 0.7462], quantization loss [ 0.0575],  0.53 sec/batch.
2022-10-19 02:13:44,460 step [ 495], lr [0.0001500], embedding loss [ 0.7442], quantization loss [ 0.0521],  0.56 sec/batch.
2022-10-19 02:13:46,737 step [ 496], lr [0.0001500], embedding loss [ 0.7471], quantization loss [ 0.0545],  0.58 sec/batch.
2022-10-19 02:13:49,106 step [ 497], lr [0.0001500], embedding loss [ 0.7338], quantization loss [ 0.0543],  0.58 sec/batch.
2022-10-19 02:13:51,155 step [ 498], lr [0.0001500], embedding loss [ 0.7371], quantization loss [ 0.0559],  0.54 sec/batch.
2022-10-19 02:13:53,277 step [ 499], lr [0.0001500], embedding loss [ 0.7326], quantization loss [ 0.0560],  0.55 sec/batch.
2022-10-19 02:13:55,250 step [ 500], lr [0.0001500], embedding loss [ 0.7326], quantization loss [ 0.0521],  0.52 sec/batch.
2022-10-19 02:13:57,202 step [ 501], lr [0.0001500], embedding loss [ 0.7440], quantization loss [ 0.0546],  0.51 sec/batch.
2022-10-19 02:13:59,181 step [ 502], lr [0.0001500], embedding loss [ 0.7311], quantization loss [ 0.0568],  0.54 sec/batch.
2022-10-19 02:14:01,246 step [ 503], lr [0.0001500], embedding loss [ 0.7294], quantization loss [ 0.0569],  0.51 sec/batch.
2022-10-19 02:14:03,150 step [ 504], lr [0.0001500], embedding loss [ 0.7413], quantization loss [ 0.0507],  0.51 sec/batch.
2022-10-19 02:14:05,119 step [ 505], lr [0.0001500], embedding loss [ 0.7507], quantization loss [ 0.0497],  0.51 sec/batch.
2022-10-19 02:14:07,083 step [ 506], lr [0.0001500], embedding loss [ 0.7433], quantization loss [ 0.0559],  0.50 sec/batch.
2022-10-19 02:14:09,022 step [ 507], lr [0.0001500], embedding loss [ 0.7366], quantization loss [ 0.0549],  0.51 sec/batch.
2022-10-19 02:14:11,010 step [ 508], lr [0.0001500], embedding loss [ 0.7370], quantization loss [ 0.0522],  0.51 sec/batch.
2022-10-19 02:14:12,967 step [ 509], lr [0.0001500], embedding loss [ 0.7423], quantization loss [ 0.0540],  0.50 sec/batch.
2022-10-19 02:14:14,859 step [ 510], lr [0.0001500], embedding loss [ 0.7510], quantization loss [ 0.0524],  0.51 sec/batch.
2022-10-19 02:14:16,827 step [ 511], lr [0.0001500], embedding loss [ 0.7304], quantization loss [ 0.0476],  0.51 sec/batch.
2022-10-19 02:14:18,846 step [ 512], lr [0.0001500], embedding loss [ 0.7353], quantization loss [ 0.0536],  0.53 sec/batch.
2022-10-19 02:14:20,853 step [ 513], lr [0.0001500], embedding loss [ 0.7406], quantization loss [ 0.0507],  0.50 sec/batch.
2022-10-19 02:14:22,837 step [ 514], lr [0.0001500], embedding loss [ 0.7393], quantization loss [ 0.0566],  0.52 sec/batch.
2022-10-19 02:14:24,846 step [ 515], lr [0.0001500], embedding loss [ 0.7420], quantization loss [ 0.0569],  0.52 sec/batch.
2022-10-19 02:14:26,813 step [ 516], lr [0.0001500], embedding loss [ 0.7236], quantization loss [ 0.0568],  0.51 sec/batch.
2022-10-19 02:14:28,765 step [ 517], lr [0.0001500], embedding loss [ 0.7370], quantization loss [ 0.0636],  0.52 sec/batch.
2022-10-19 02:14:30,638 step [ 518], lr [0.0001500], embedding loss [ 0.7317], quantization loss [ 0.0563],  0.50 sec/batch.
2022-10-19 02:14:32,560 step [ 519], lr [0.0001500], embedding loss [ 0.7441], quantization loss [ 0.0561],  0.51 sec/batch.
2022-10-19 02:14:34,561 step [ 520], lr [0.0001500], embedding loss [ 0.7396], quantization loss [ 0.0538],  0.51 sec/batch.
2022-10-19 02:14:36,528 step [ 521], lr [0.0001500], embedding loss [ 0.7416], quantization loss [ 0.0587],  0.51 sec/batch.
2022-10-19 02:14:38,477 step [ 522], lr [0.0001500], embedding loss [ 0.7310], quantization loss [ 0.0505],  0.51 sec/batch.
2022-10-19 02:14:40,446 step [ 523], lr [0.0001500], embedding loss [ 0.7410], quantization loss [ 0.0566],  0.51 sec/batch.
2022-10-19 02:14:42,407 step [ 524], lr [0.0001500], embedding loss [ 0.7329], quantization loss [ 0.0531],  0.51 sec/batch.
2022-10-19 02:14:44,474 step [ 525], lr [0.0001500], embedding loss [ 0.7270], quantization loss [ 0.0492],  0.53 sec/batch.
2022-10-19 02:14:46,490 step [ 526], lr [0.0001500], embedding loss [ 0.7440], quantization loss [ 0.0513],  0.51 sec/batch.
2022-10-19 02:14:48,553 step [ 527], lr [0.0001500], embedding loss [ 0.7363], quantization loss [ 0.0516],  0.52 sec/batch.
2022-10-19 02:14:50,557 step [ 528], lr [0.0001500], embedding loss [ 0.7346], quantization loss [ 0.0523],  0.51 sec/batch.
2022-10-19 02:14:52,605 step [ 529], lr [0.0001500], embedding loss [ 0.7244], quantization loss [ 0.0467],  0.52 sec/batch.
2022-10-19 02:14:54,620 step [ 530], lr [0.0001500], embedding loss [ 0.7314], quantization loss [ 0.0498],  0.51 sec/batch.
2022-10-19 02:14:56,665 step [ 531], lr [0.0001500], embedding loss [ 0.7257], quantization loss [ 0.0550],  0.52 sec/batch.
2022-10-19 02:14:58,657 step [ 532], lr [0.0001500], embedding loss [ 0.7424], quantization loss [ 0.0448],  0.51 sec/batch.
2022-10-19 02:15:00,686 step [ 533], lr [0.0001500], embedding loss [ 0.7436], quantization loss [ 0.0532],  0.52 sec/batch.
2022-10-19 02:15:02,636 step [ 534], lr [0.0001500], embedding loss [ 0.7412], quantization loss [ 0.0562],  0.51 sec/batch.
2022-10-19 02:15:04,625 step [ 535], lr [0.0001500], embedding loss [ 0.7426], quantization loss [ 0.0556],  0.51 sec/batch.
2022-10-19 02:15:06,591 step [ 536], lr [0.0001500], embedding loss [ 0.7418], quantization loss [ 0.0518],  0.51 sec/batch.
2022-10-19 02:15:08,509 step [ 537], lr [0.0001500], embedding loss [ 0.7253], quantization loss [ 0.0512],  0.50 sec/batch.
2022-10-19 02:15:10,447 step [ 538], lr [0.0001500], embedding loss [ 0.7334], quantization loss [ 0.0525],  0.51 sec/batch.
2022-10-19 02:15:12,446 step [ 539], lr [0.0001500], embedding loss [ 0.7324], quantization loss [ 0.0565],  0.51 sec/batch.
2022-10-19 02:15:14,441 step [ 540], lr [0.0001500], embedding loss [ 0.7383], quantization loss [ 0.0503],  0.51 sec/batch.
2022-10-19 02:15:16,465 step [ 541], lr [0.0001500], embedding loss [ 0.7407], quantization loss [ 0.0554],  0.52 sec/batch.
2022-10-19 02:15:18,377 step [ 542], lr [0.0001500], embedding loss [ 0.7441], quantization loss [ 0.0515],  0.50 sec/batch.
2022-10-19 02:15:20,366 step [ 543], lr [0.0001500], embedding loss [ 0.7317], quantization loss [ 0.0490],  0.50 sec/batch.
2022-10-19 02:15:22,312 step [ 544], lr [0.0001500], embedding loss [ 0.7399], quantization loss [ 0.0571],  0.50 sec/batch.
2022-10-19 02:15:24,219 step [ 545], lr [0.0001500], embedding loss [ 0.7412], quantization loss [ 0.0531],  0.51 sec/batch.
2022-10-19 02:15:26,156 step [ 546], lr [0.0001500], embedding loss [ 0.7249], quantization loss [ 0.0560],  0.50 sec/batch.
2022-10-19 02:15:28,108 step [ 547], lr [0.0001500], embedding loss [ 0.7377], quantization loss [ 0.0534],  0.51 sec/batch.
2022-10-19 02:15:30,069 step [ 548], lr [0.0001500], embedding loss [ 0.7404], quantization loss [ 0.0491],  0.53 sec/batch.
2022-10-19 02:15:31,980 step [ 549], lr [0.0001500], embedding loss [ 0.7291], quantization loss [ 0.0516],  0.50 sec/batch.
2022-10-19 02:15:33,969 step [ 550], lr [0.0001500], embedding loss [ 0.7392], quantization loss [ 0.0582],  0.51 sec/batch.
2022-10-19 02:15:36,040 step [ 551], lr [0.0001500], embedding loss [ 0.7348], quantization loss [ 0.0571],  0.54 sec/batch.
2022-10-19 02:15:38,062 step [ 552], lr [0.0001500], embedding loss [ 0.7333], quantization loss [ 0.0521],  0.52 sec/batch.
2022-10-19 02:15:40,060 step [ 553], lr [0.0001500], embedding loss [ 0.7378], quantization loss [ 0.0495],  0.51 sec/batch.
2022-10-19 02:15:42,104 step [ 554], lr [0.0001500], embedding loss [ 0.7571], quantization loss [ 0.0599],  0.52 sec/batch.
2022-10-19 02:15:44,142 step [ 555], lr [0.0001500], embedding loss [ 0.7285], quantization loss [ 0.0494],  0.50 sec/batch.
2022-10-19 02:15:46,134 step [ 556], lr [0.0001500], embedding loss [ 0.7368], quantization loss [ 0.0462],  0.52 sec/batch.
2022-10-19 02:15:48,114 step [ 557], lr [0.0001500], embedding loss [ 0.7393], quantization loss [ 0.0509],  0.52 sec/batch.
2022-10-19 02:15:50,135 step [ 558], lr [0.0001500], embedding loss [ 0.7355], quantization loss [ 0.0488],  0.52 sec/batch.
2022-10-19 02:15:52,180 step [ 559], lr [0.0001500], embedding loss [ 0.7319], quantization loss [ 0.0470],  0.52 sec/batch.
2022-10-19 02:15:54,149 step [ 560], lr [0.0001500], embedding loss [ 0.7375], quantization loss [ 0.0477],  0.50 sec/batch.
2022-10-19 02:15:56,099 step [ 561], lr [0.0001500], embedding loss [ 0.7320], quantization loss [ 0.0452],  0.50 sec/batch.
2022-10-19 02:15:56,100 update codes and centers iter(1/1).
2022-10-19 02:15:59,312 number of update_code wrong: 0.
2022-10-19 02:16:02,352 non zero codewords: 1024.
2022-10-19 02:16:02,352 finish center update, duration: 6.25 sec.
2022-10-19 02:16:04,277 step [ 562], lr [0.0001500], embedding loss [ 0.7378], quantization loss [ 0.0510],  0.52 sec/batch.
2022-10-19 02:16:06,330 step [ 563], lr [0.0001500], embedding loss [ 0.7287], quantization loss [ 0.0547],  0.52 sec/batch.
2022-10-19 02:16:08,378 step [ 564], lr [0.0001500], embedding loss [ 0.7313], quantization loss [ 0.0529],  0.52 sec/batch.
2022-10-19 02:16:10,393 step [ 565], lr [0.0001500], embedding loss [ 0.7273], quantization loss [ 0.0493],  0.50 sec/batch.
2022-10-19 02:16:12,373 step [ 566], lr [0.0001500], embedding loss [ 0.7358], quantization loss [ 0.0526],  0.50 sec/batch.
2022-10-19 02:16:14,354 step [ 567], lr [0.0001500], embedding loss [ 0.7352], quantization loss [ 0.0494],  0.51 sec/batch.
2022-10-19 02:16:16,371 step [ 568], lr [0.0001500], embedding loss [ 0.7272], quantization loss [ 0.0526],  0.51 sec/batch.
2022-10-19 02:16:18,409 step [ 569], lr [0.0001500], embedding loss [ 0.7303], quantization loss [ 0.0548],  0.51 sec/batch.
2022-10-19 02:16:20,443 step [ 570], lr [0.0001500], embedding loss [ 0.7480], quantization loss [ 0.0553],  0.53 sec/batch.
2022-10-19 02:16:22,427 step [ 571], lr [0.0001500], embedding loss [ 0.7390], quantization loss [ 0.0575],  0.51 sec/batch.
2022-10-19 02:16:24,460 step [ 572], lr [0.0001500], embedding loss [ 0.7186], quantization loss [ 0.0490],  0.51 sec/batch.
2022-10-19 02:16:26,494 step [ 573], lr [0.0001500], embedding loss [ 0.7328], quantization loss [ 0.0556],  0.49 sec/batch.
2022-10-19 02:16:28,459 step [ 574], lr [0.0001500], embedding loss [ 0.7387], quantization loss [ 0.0623],  0.52 sec/batch.
2022-10-19 02:16:30,481 step [ 575], lr [0.0001500], embedding loss [ 0.7412], quantization loss [ 0.0501],  0.51 sec/batch.
2022-10-19 02:16:32,475 step [ 576], lr [0.0001500], embedding loss [ 0.7402], quantization loss [ 0.0539],  0.51 sec/batch.
2022-10-19 02:16:34,449 step [ 577], lr [0.0001500], embedding loss [ 0.7505], quantization loss [ 0.0558],  0.50 sec/batch.
2022-10-19 02:16:36,517 step [ 578], lr [0.0001500], embedding loss [ 0.7393], quantization loss [ 0.0496],  0.51 sec/batch.
2022-10-19 02:16:38,492 step [ 579], lr [0.0001500], embedding loss [ 0.7372], quantization loss [ 0.0530],  0.51 sec/batch.
2022-10-19 02:16:40,495 step [ 580], lr [0.0001500], embedding loss [ 0.7283], quantization loss [ 0.0489],  0.52 sec/batch.
2022-10-19 02:16:42,481 step [ 581], lr [0.0001500], embedding loss [ 0.7477], quantization loss [ 0.0569],  0.50 sec/batch.
2022-10-19 02:16:44,514 step [ 582], lr [0.0001500], embedding loss [ 0.7262], quantization loss [ 0.0556],  0.51 sec/batch.
2022-10-19 02:16:46,479 step [ 583], lr [0.0001500], embedding loss [ 0.7255], quantization loss [ 0.0520],  0.50 sec/batch.
2022-10-19 02:16:48,517 step [ 584], lr [0.0001500], embedding loss [ 0.7307], quantization loss [ 0.0545],  0.52 sec/batch.
2022-10-19 02:16:50,510 step [ 585], lr [0.0001500], embedding loss [ 0.7430], quantization loss [ 0.0523],  0.50 sec/batch.
2022-10-19 02:16:52,475 step [ 586], lr [0.0001500], embedding loss [ 0.7303], quantization loss [ 0.0483],  0.49 sec/batch.
2022-10-19 02:16:54,392 step [ 587], lr [0.0001500], embedding loss [ 0.7539], quantization loss [ 0.0516],  0.48 sec/batch.
2022-10-19 02:16:56,356 step [ 588], lr [0.0001500], embedding loss [ 0.7274], quantization loss [ 0.0509],  0.51 sec/batch.
2022-10-19 02:16:58,341 step [ 589], lr [0.0001500], embedding loss [ 0.7351], quantization loss [ 0.0517],  0.50 sec/batch.
2022-10-19 02:17:00,276 step [ 590], lr [0.0001500], embedding loss [ 0.7349], quantization loss [ 0.0490],  0.51 sec/batch.
2022-10-19 02:17:02,269 step [ 591], lr [0.0001500], embedding loss [ 0.7311], quantization loss [ 0.0522],  0.50 sec/batch.
2022-10-19 02:17:04,242 step [ 592], lr [0.0001500], embedding loss [ 0.7290], quantization loss [ 0.0505],  0.50 sec/batch.
2022-10-19 02:17:06,242 step [ 593], lr [0.0001500], embedding loss [ 0.7306], quantization loss [ 0.0556],  0.51 sec/batch.
2022-10-19 02:17:08,260 step [ 594], lr [0.0001500], embedding loss [ 0.7455], quantization loss [ 0.0477],  0.51 sec/batch.
2022-10-19 02:17:10,275 step [ 595], lr [0.0001500], embedding loss [ 0.7346], quantization loss [ 0.0561],  0.50 sec/batch.
2022-10-19 02:17:12,257 step [ 596], lr [0.0001500], embedding loss [ 0.7359], quantization loss [ 0.0486],  0.51 sec/batch.
2022-10-19 02:17:14,203 step [ 597], lr [0.0001500], embedding loss [ 0.7389], quantization loss [ 0.0538],  0.49 sec/batch.
2022-10-19 02:17:16,176 step [ 598], lr [0.0001500], embedding loss [ 0.7243], quantization loss [ 0.0539],  0.50 sec/batch.
2022-10-19 02:17:18,163 step [ 599], lr [0.0001500], embedding loss [ 0.7374], quantization loss [ 0.0545],  0.51 sec/batch.
2022-10-19 02:17:20,154 step [ 600], lr [0.0001500], embedding loss [ 0.7294], quantization loss [ 0.0533],  0.51 sec/batch.
2022-10-19 02:17:22,140 step [ 601], lr [0.0000750], embedding loss [ 0.7342], quantization loss [ 0.0478],  0.51 sec/batch.
2022-10-19 02:17:24,115 step [ 602], lr [0.0000750], embedding loss [ 0.7394], quantization loss [ 0.0465],  0.52 sec/batch.
2022-10-19 02:17:26,035 step [ 603], lr [0.0000750], embedding loss [ 0.7477], quantization loss [ 0.0468],  0.49 sec/batch.
2022-10-19 02:17:28,005 step [ 604], lr [0.0000750], embedding loss [ 0.7438], quantization loss [ 0.0467],  0.49 sec/batch.
2022-10-19 02:17:29,939 step [ 605], lr [0.0000750], embedding loss [ 0.7304], quantization loss [ 0.0585],  0.49 sec/batch.
2022-10-19 02:17:31,891 step [ 606], lr [0.0000750], embedding loss [ 0.7213], quantization loss [ 0.0500],  0.51 sec/batch.
2022-10-19 02:17:33,810 step [ 607], lr [0.0000750], embedding loss [ 0.7396], quantization loss [ 0.0455],  0.49 sec/batch.
2022-10-19 02:17:35,731 step [ 608], lr [0.0000750], embedding loss [ 0.7281], quantization loss [ 0.0525],  0.50 sec/batch.
2022-10-19 02:17:37,770 step [ 609], lr [0.0000750], embedding loss [ 0.7337], quantization loss [ 0.0487],  0.51 sec/batch.
2022-10-19 02:17:39,785 step [ 610], lr [0.0000750], embedding loss [ 0.7309], quantization loss [ 0.0511],  0.51 sec/batch.
2022-10-19 02:17:41,773 step [ 611], lr [0.0000750], embedding loss [ 0.7312], quantization loss [ 0.0479],  0.51 sec/batch.
2022-10-19 02:17:43,793 step [ 612], lr [0.0000750], embedding loss [ 0.7383], quantization loss [ 0.0480],  0.50 sec/batch.
2022-10-19 02:17:45,781 step [ 613], lr [0.0000750], embedding loss [ 0.7268], quantization loss [ 0.0480],  0.50 sec/batch.
2022-10-19 02:17:47,774 step [ 614], lr [0.0000750], embedding loss [ 0.7312], quantization loss [ 0.0459],  0.51 sec/batch.
2022-10-19 02:17:49,810 step [ 615], lr [0.0000750], embedding loss [ 0.7357], quantization loss [ 0.0476],  0.50 sec/batch.
2022-10-19 02:17:51,711 step [ 616], lr [0.0000750], embedding loss [ 0.7409], quantization loss [ 0.0507],  0.49 sec/batch.
2022-10-19 02:17:53,594 step [ 617], lr [0.0000750], embedding loss [ 0.7275], quantization loss [ 0.0486],  0.50 sec/batch.
2022-10-19 02:17:55,489 step [ 618], lr [0.0000750], embedding loss [ 0.7367], quantization loss [ 0.0458],  0.50 sec/batch.
2022-10-19 02:17:57,471 step [ 619], lr [0.0000750], embedding loss [ 0.7277], quantization loss [ 0.0450],  0.51 sec/batch.
2022-10-19 02:17:59,402 step [ 620], lr [0.0000750], embedding loss [ 0.7412], quantization loss [ 0.0486],  0.51 sec/batch.
2022-10-19 02:18:01,337 step [ 621], lr [0.0000750], embedding loss [ 0.7311], quantization loss [ 0.0419],  0.50 sec/batch.
2022-10-19 02:18:03,343 step [ 622], lr [0.0000750], embedding loss [ 0.7191], quantization loss [ 0.0454],  0.52 sec/batch.
2022-10-19 02:18:05,330 step [ 623], lr [0.0000750], embedding loss [ 0.7192], quantization loss [ 0.0434],  0.50 sec/batch.
2022-10-19 02:18:07,283 step [ 624], lr [0.0000750], embedding loss [ 0.7419], quantization loss [ 0.0484],  0.51 sec/batch.
2022-10-19 02:18:09,260 step [ 625], lr [0.0000750], embedding loss [ 0.7258], quantization loss [ 0.0506],  0.51 sec/batch.
2022-10-19 02:18:11,237 step [ 626], lr [0.0000750], embedding loss [ 0.7361], quantization loss [ 0.0477],  0.51 sec/batch.
2022-10-19 02:18:13,170 step [ 627], lr [0.0000750], embedding loss [ 0.7469], quantization loss [ 0.0511],  0.51 sec/batch.
2022-10-19 02:18:15,141 step [ 628], lr [0.0000750], embedding loss [ 0.7216], quantization loss [ 0.0460],  0.50 sec/batch.
2022-10-19 02:18:17,146 step [ 629], lr [0.0000750], embedding loss [ 0.7254], quantization loss [ 0.0517],  0.51 sec/batch.
2022-10-19 02:18:19,073 step [ 630], lr [0.0000750], embedding loss [ 0.7230], quantization loss [ 0.0477],  0.50 sec/batch.
2022-10-19 02:18:21,111 step [ 631], lr [0.0000750], embedding loss [ 0.7387], quantization loss [ 0.0457],  0.51 sec/batch.
2022-10-19 02:18:23,029 step [ 632], lr [0.0000750], embedding loss [ 0.7266], quantization loss [ 0.0488],  0.49 sec/batch.
2022-10-19 02:18:25,033 step [ 633], lr [0.0000750], embedding loss [ 0.7469], quantization loss [ 0.0514],  0.49 sec/batch.
2022-10-19 02:18:26,972 step [ 634], lr [0.0000750], embedding loss [ 0.7494], quantization loss [ 0.0504],  0.50 sec/batch.
2022-10-19 02:18:28,985 step [ 635], lr [0.0000750], embedding loss [ 0.7348], quantization loss [ 0.0441],  0.51 sec/batch.
2022-10-19 02:18:30,963 step [ 636], lr [0.0000750], embedding loss [ 0.7378], quantization loss [ 0.0454],  0.50 sec/batch.
2022-10-19 02:18:33,010 step [ 637], lr [0.0000750], embedding loss [ 0.7365], quantization loss [ 0.0438],  0.51 sec/batch.
2022-10-19 02:18:34,929 step [ 638], lr [0.0000750], embedding loss [ 0.7388], quantization loss [ 0.0419],  0.50 sec/batch.
2022-10-19 02:18:36,879 step [ 639], lr [0.0000750], embedding loss [ 0.7326], quantization loss [ 0.0476],  0.51 sec/batch.
2022-10-19 02:18:38,829 step [ 640], lr [0.0000750], embedding loss [ 0.7301], quantization loss [ 0.0448],  0.50 sec/batch.
2022-10-19 02:18:40,829 step [ 641], lr [0.0000750], embedding loss [ 0.7373], quantization loss [ 0.0434],  0.51 sec/batch.
2022-10-19 02:18:40,829 update codes and centers iter(1/1).
2022-10-19 02:18:43,953 number of update_code wrong: 0.
2022-10-19 02:18:46,812 non zero codewords: 1024.
2022-10-19 02:18:46,812 finish center update, duration: 5.98 sec.
2022-10-19 02:18:48,738 step [ 642], lr [0.0000750], embedding loss [ 0.7381], quantization loss [ 0.0466],  0.52 sec/batch.
2022-10-19 02:18:50,744 step [ 643], lr [0.0000750], embedding loss [ 0.7324], quantization loss [ 0.0434],  0.51 sec/batch.
2022-10-19 02:18:52,803 step [ 644], lr [0.0000750], embedding loss [ 0.7280], quantization loss [ 0.0444],  0.52 sec/batch.
2022-10-19 02:18:54,821 step [ 645], lr [0.0000750], embedding loss [ 0.7312], quantization loss [ 0.0463],  0.51 sec/batch.
2022-10-19 02:18:56,879 step [ 646], lr [0.0000750], embedding loss [ 0.7194], quantization loss [ 0.0479],  0.52 sec/batch.
2022-10-19 02:18:58,880 step [ 647], lr [0.0000750], embedding loss [ 0.7350], quantization loss [ 0.0474],  0.51 sec/batch.
2022-10-19 02:19:00,918 step [ 648], lr [0.0000750], embedding loss [ 0.7260], quantization loss [ 0.0466],  0.50 sec/batch.
2022-10-19 02:19:02,927 step [ 649], lr [0.0000750], embedding loss [ 0.7361], quantization loss [ 0.0406],  0.51 sec/batch.
2022-10-19 02:19:04,961 step [ 650], lr [0.0000750], embedding loss [ 0.7141], quantization loss [ 0.0439],  0.51 sec/batch.
2022-10-19 02:19:06,960 step [ 651], lr [0.0000750], embedding loss [ 0.7322], quantization loss [ 0.0432],  0.51 sec/batch.
2022-10-19 02:19:08,979 step [ 652], lr [0.0000750], embedding loss [ 0.7419], quantization loss [ 0.0447],  0.52 sec/batch.
2022-10-19 02:19:10,993 step [ 653], lr [0.0000750], embedding loss [ 0.7260], quantization loss [ 0.0455],  0.52 sec/batch.
2022-10-19 02:19:13,007 step [ 654], lr [0.0000750], embedding loss [ 0.7205], quantization loss [ 0.0477],  0.51 sec/batch.
2022-10-19 02:19:14,985 step [ 655], lr [0.0000750], embedding loss [ 0.7434], quantization loss [ 0.0425],  0.50 sec/batch.
2022-10-19 02:19:16,952 step [ 656], lr [0.0000750], embedding loss [ 0.7343], quantization loss [ 0.0423],  0.49 sec/batch.
2022-10-19 02:19:18,918 step [ 657], lr [0.0000750], embedding loss [ 0.7291], quantization loss [ 0.0501],  0.49 sec/batch.
2022-10-19 02:19:20,903 step [ 658], lr [0.0000750], embedding loss [ 0.7366], quantization loss [ 0.0444],  0.51 sec/batch.
2022-10-19 02:19:22,926 step [ 659], lr [0.0000750], embedding loss [ 0.7433], quantization loss [ 0.0460],  0.51 sec/batch.
2022-10-19 02:19:24,892 step [ 660], lr [0.0000750], embedding loss [ 0.7291], quantization loss [ 0.0434],  0.49 sec/batch.
2022-10-19 02:19:26,844 step [ 661], lr [0.0000750], embedding loss [ 0.7300], quantization loss [ 0.0454],  0.50 sec/batch.
2022-10-19 02:19:28,782 step [ 662], lr [0.0000750], embedding loss [ 0.7386], quantization loss [ 0.0455],  0.48 sec/batch.
2022-10-19 02:19:30,781 step [ 663], lr [0.0000750], embedding loss [ 0.7222], quantization loss [ 0.0446],  0.49 sec/batch.
2022-10-19 02:19:32,757 step [ 664], lr [0.0000750], embedding loss [ 0.7376], quantization loss [ 0.0430],  0.51 sec/batch.
2022-10-19 02:19:34,731 step [ 665], lr [0.0000750], embedding loss [ 0.7366], quantization loss [ 0.0455],  0.51 sec/batch.
2022-10-19 02:19:36,783 step [ 666], lr [0.0000750], embedding loss [ 0.7292], quantization loss [ 0.0492],  0.51 sec/batch.
2022-10-19 02:19:38,813 step [ 667], lr [0.0000750], embedding loss [ 0.7316], quantization loss [ 0.0477],  0.51 sec/batch.
2022-10-19 02:19:40,806 step [ 668], lr [0.0000750], embedding loss [ 0.7324], quantization loss [ 0.0461],  0.50 sec/batch.
2022-10-19 02:19:42,742 step [ 669], lr [0.0000750], embedding loss [ 0.7389], quantization loss [ 0.0457],  0.49 sec/batch.
2022-10-19 02:19:44,724 step [ 670], lr [0.0000750], embedding loss [ 0.7250], quantization loss [ 0.0462],  0.49 sec/batch.
2022-10-19 02:19:46,697 step [ 671], lr [0.0000750], embedding loss [ 0.7328], quantization loss [ 0.0420],  0.51 sec/batch.
2022-10-19 02:19:48,699 step [ 672], lr [0.0000750], embedding loss [ 0.7410], quantization loss [ 0.0502],  0.51 sec/batch.
2022-10-19 02:19:50,755 step [ 673], lr [0.0000750], embedding loss [ 0.7319], quantization loss [ 0.0458],  0.51 sec/batch.
2022-10-19 02:19:52,801 step [ 674], lr [0.0000750], embedding loss [ 0.7248], quantization loss [ 0.0481],  0.51 sec/batch.
2022-10-19 02:19:54,774 step [ 675], lr [0.0000750], embedding loss [ 0.7316], quantization loss [ 0.0468],  0.50 sec/batch.
2022-10-19 02:19:56,762 step [ 676], lr [0.0000750], embedding loss [ 0.7331], quantization loss [ 0.0434],  0.50 sec/batch.
2022-10-19 02:19:58,761 step [ 677], lr [0.0000750], embedding loss [ 0.7295], quantization loss [ 0.0527],  0.51 sec/batch.
2022-10-19 02:20:00,754 step [ 678], lr [0.0000750], embedding loss [ 0.7437], quantization loss [ 0.0444],  0.51 sec/batch.
2022-10-19 02:20:02,750 step [ 679], lr [0.0000750], embedding loss [ 0.7400], quantization loss [ 0.0467],  0.51 sec/batch.
2022-10-19 02:20:04,768 step [ 680], lr [0.0000750], embedding loss [ 0.7446], quantization loss [ 0.0454],  0.51 sec/batch.
2022-10-19 02:20:06,776 step [ 681], lr [0.0000750], embedding loss [ 0.7292], quantization loss [ 0.0428],  0.51 sec/batch.
2022-10-19 02:20:08,802 step [ 682], lr [0.0000750], embedding loss [ 0.7382], quantization loss [ 0.0479],  0.51 sec/batch.
2022-10-19 02:20:10,840 step [ 683], lr [0.0000750], embedding loss [ 0.7450], quantization loss [ 0.0455],  0.51 sec/batch.
2022-10-19 02:20:12,925 step [ 684], lr [0.0000750], embedding loss [ 0.7412], quantization loss [ 0.0417],  0.52 sec/batch.
2022-10-19 02:20:14,944 step [ 685], lr [0.0000750], embedding loss [ 0.7212], quantization loss [ 0.0397],  0.50 sec/batch.
2022-10-19 02:20:16,932 step [ 686], lr [0.0000750], embedding loss [ 0.7307], quantization loss [ 0.0448],  0.51 sec/batch.
2022-10-19 02:20:18,935 step [ 687], lr [0.0000750], embedding loss [ 0.7370], quantization loss [ 0.0459],  0.51 sec/batch.
2022-10-19 02:20:20,950 step [ 688], lr [0.0000750], embedding loss [ 0.7340], quantization loss [ 0.0381],  0.51 sec/batch.
2022-10-19 02:20:22,964 step [ 689], lr [0.0000750], embedding loss [ 0.7322], quantization loss [ 0.0424],  0.51 sec/batch.
2022-10-19 02:20:24,970 step [ 690], lr [0.0000750], embedding loss [ 0.7330], quantization loss [ 0.0435],  0.52 sec/batch.
2022-10-19 02:20:27,021 step [ 691], lr [0.0000750], embedding loss [ 0.7351], quantization loss [ 0.0413],  0.50 sec/batch.
2022-10-19 02:20:28,997 step [ 692], lr [0.0000750], embedding loss [ 0.7285], quantization loss [ 0.0429],  0.49 sec/batch.
2022-10-19 02:20:30,970 step [ 693], lr [0.0000750], embedding loss [ 0.7335], quantization loss [ 0.0449],  0.49 sec/batch.
2022-10-19 02:20:32,947 step [ 694], lr [0.0000750], embedding loss [ 0.7362], quantization loss [ 0.0433],  0.50 sec/batch.
2022-10-19 02:20:34,968 step [ 695], lr [0.0000750], embedding loss [ 0.7224], quantization loss [ 0.0428],  0.51 sec/batch.
2022-10-19 02:20:37,022 step [ 696], lr [0.0000750], embedding loss [ 0.7337], quantization loss [ 0.0461],  0.51 sec/batch.
2022-10-19 02:20:39,035 step [ 697], lr [0.0000750], embedding loss [ 0.7364], quantization loss [ 0.0425],  0.52 sec/batch.
2022-10-19 02:20:41,076 step [ 698], lr [0.0000750], embedding loss [ 0.7254], quantization loss [ 0.0395],  0.50 sec/batch.
2022-10-19 02:20:43,027 step [ 699], lr [0.0000750], embedding loss [ 0.7155], quantization loss [ 0.0447],  0.50 sec/batch.
2022-10-19 02:20:45,003 step [ 700], lr [0.0000750], embedding loss [ 0.7326], quantization loss [ 0.0440],  0.51 sec/batch.
2022-10-19 02:20:46,998 step [ 701], lr [0.0000750], embedding loss [ 0.7215], quantization loss [ 0.0458],  0.49 sec/batch.
2022-10-19 02:20:48,961 step [ 702], lr [0.0000750], embedding loss [ 0.7493], quantization loss [ 0.0422],  0.50 sec/batch.
2022-10-19 02:20:50,902 step [ 703], lr [0.0000750], embedding loss [ 0.7272], quantization loss [ 0.0410],  0.50 sec/batch.
2022-10-19 02:20:52,977 step [ 704], lr [0.0000750], embedding loss [ 0.7411], quantization loss [ 0.0416],  0.51 sec/batch.
2022-10-19 02:20:54,948 step [ 705], lr [0.0000750], embedding loss [ 0.7240], quantization loss [ 0.0405],  0.50 sec/batch.
2022-10-19 02:20:56,952 step [ 706], lr [0.0000750], embedding loss [ 0.7358], quantization loss [ 0.0404],  0.51 sec/batch.
2022-10-19 02:20:58,970 step [ 707], lr [0.0000750], embedding loss [ 0.7337], quantization loss [ 0.0414],  0.50 sec/batch.
2022-10-19 02:21:00,945 step [ 708], lr [0.0000750], embedding loss [ 0.7324], quantization loss [ 0.0406],  0.51 sec/batch.
2022-10-19 02:21:02,934 step [ 709], lr [0.0000750], embedding loss [ 0.7282], quantization loss [ 0.0403],  0.51 sec/batch.
2022-10-19 02:21:04,990 step [ 710], lr [0.0000750], embedding loss [ 0.7371], quantization loss [ 0.0438],  0.51 sec/batch.
2022-10-19 02:21:06,983 step [ 711], lr [0.0000750], embedding loss [ 0.7322], quantization loss [ 0.0408],  0.51 sec/batch.
2022-10-19 02:21:09,043 step [ 712], lr [0.0000750], embedding loss [ 0.7258], quantization loss [ 0.0412],  0.52 sec/batch.
2022-10-19 02:21:11,070 step [ 713], lr [0.0000750], embedding loss [ 0.7250], quantization loss [ 0.0450],  0.51 sec/batch.
2022-10-19 02:21:13,160 step [ 714], lr [0.0000750], embedding loss [ 0.7449], quantization loss [ 0.0438],  0.52 sec/batch.
2022-10-19 02:21:15,265 step [ 715], lr [0.0000750], embedding loss [ 0.7365], quantization loss [ 0.0396],  0.51 sec/batch.
2022-10-19 02:21:17,359 step [ 716], lr [0.0000750], embedding loss [ 0.7201], quantization loss [ 0.0424],  0.52 sec/batch.
2022-10-19 02:21:19,360 step [ 717], lr [0.0000750], embedding loss [ 0.7370], quantization loss [ 0.0414],  0.49 sec/batch.
2022-10-19 02:21:21,372 step [ 718], lr [0.0000750], embedding loss [ 0.7270], quantization loss [ 0.0454],  0.50 sec/batch.
2022-10-19 02:21:23,380 step [ 719], lr [0.0000750], embedding loss [ 0.7184], quantization loss [ 0.0422],  0.51 sec/batch.
2022-10-19 02:21:25,382 step [ 720], lr [0.0000750], embedding loss [ 0.7423], quantization loss [ 0.0382],  0.51 sec/batch.
2022-10-19 02:21:27,515 step [ 721], lr [0.0000750], embedding loss [ 0.7276], quantization loss [ 0.0377],  0.52 sec/batch.
2022-10-19 02:21:27,516 update codes and centers iter(1/1).
2022-10-19 02:21:30,748 number of update_code wrong: 0.
2022-10-19 02:21:33,923 non zero codewords: 1024.
2022-10-19 02:21:33,923 finish center update, duration: 6.41 sec.
2022-10-19 02:21:35,893 step [ 722], lr [0.0000750], embedding loss [ 0.7250], quantization loss [ 0.0430],  0.53 sec/batch.
2022-10-19 02:21:37,979 step [ 723], lr [0.0000750], embedding loss [ 0.7269], quantization loss [ 0.0407],  0.52 sec/batch.
2022-10-19 02:21:40,060 step [ 724], lr [0.0000750], embedding loss [ 0.7363], quantization loss [ 0.0444],  0.51 sec/batch.
2022-10-19 02:21:42,121 step [ 725], lr [0.0000750], embedding loss [ 0.7395], quantization loss [ 0.0421],  0.51 sec/batch.
2022-10-19 02:21:44,147 step [ 726], lr [0.0000750], embedding loss [ 0.7424], quantization loss [ 0.0450],  0.50 sec/batch.
2022-10-19 02:21:46,175 step [ 727], lr [0.0000750], embedding loss [ 0.7277], quantization loss [ 0.0444],  0.51 sec/batch.
2022-10-19 02:21:48,184 step [ 728], lr [0.0000750], embedding loss [ 0.7483], quantization loss [ 0.0398],  0.51 sec/batch.
2022-10-19 02:21:50,216 step [ 729], lr [0.0000750], embedding loss [ 0.7396], quantization loss [ 0.0416],  0.51 sec/batch.
2022-10-19 02:21:52,289 step [ 730], lr [0.0000750], embedding loss [ 0.7285], quantization loss [ 0.0455],  0.50 sec/batch.
2022-10-19 02:21:54,374 step [ 731], lr [0.0000750], embedding loss [ 0.7327], quantization loss [ 0.0416],  0.51 sec/batch.
2022-10-19 02:21:56,435 step [ 732], lr [0.0000750], embedding loss [ 0.7424], quantization loss [ 0.0450],  0.51 sec/batch.
2022-10-19 02:21:58,463 step [ 733], lr [0.0000750], embedding loss [ 0.7412], quantization loss [ 0.0426],  0.51 sec/batch.
2022-10-19 02:22:00,459 step [ 734], lr [0.0000750], embedding loss [ 0.7286], quantization loss [ 0.0434],  0.51 sec/batch.
2022-10-19 02:22:02,539 step [ 735], lr [0.0000750], embedding loss [ 0.7171], quantization loss [ 0.0391],  0.51 sec/batch.
2022-10-19 02:22:04,569 step [ 736], lr [0.0000750], embedding loss [ 0.7263], quantization loss [ 0.0413],  0.51 sec/batch.
2022-10-19 02:22:06,582 step [ 737], lr [0.0000750], embedding loss [ 0.7328], quantization loss [ 0.0391],  0.51 sec/batch.
2022-10-19 02:22:08,608 step [ 738], lr [0.0000750], embedding loss [ 0.7339], quantization loss [ 0.0363],  0.52 sec/batch.
2022-10-19 02:22:10,659 step [ 739], lr [0.0000750], embedding loss [ 0.7305], quantization loss [ 0.0420],  0.51 sec/batch.
2022-10-19 02:22:12,720 step [ 740], lr [0.0000750], embedding loss [ 0.7377], quantization loss [ 0.0399],  0.51 sec/batch.
2022-10-19 02:22:14,766 step [ 741], lr [0.0000750], embedding loss [ 0.7438], quantization loss [ 0.0394],  0.50 sec/batch.
2022-10-19 02:22:16,767 step [ 742], lr [0.0000750], embedding loss [ 0.7333], quantization loss [ 0.0407],  0.51 sec/batch.
2022-10-19 02:22:18,785 step [ 743], lr [0.0000750], embedding loss [ 0.7293], quantization loss [ 0.0383],  0.50 sec/batch.
2022-10-19 02:22:20,816 step [ 744], lr [0.0000750], embedding loss [ 0.7306], quantization loss [ 0.0459],  0.52 sec/batch.
2022-10-19 02:22:22,830 step [ 745], lr [0.0000750], embedding loss [ 0.7206], quantization loss [ 0.0377],  0.51 sec/batch.
2022-10-19 02:22:24,805 step [ 746], lr [0.0000750], embedding loss [ 0.7292], quantization loss [ 0.0468],  0.51 sec/batch.
2022-10-19 02:22:26,855 step [ 747], lr [0.0000750], embedding loss [ 0.7321], quantization loss [ 0.0396],  0.51 sec/batch.
2022-10-19 02:22:28,931 step [ 748], lr [0.0000750], embedding loss [ 0.7269], quantization loss [ 0.0433],  0.52 sec/batch.
2022-10-19 02:22:31,010 step [ 749], lr [0.0000750], embedding loss [ 0.7315], quantization loss [ 0.0418],  0.51 sec/batch.
2022-10-19 02:22:33,009 step [ 750], lr [0.0000750], embedding loss [ 0.7294], quantization loss [ 0.0427],  0.50 sec/batch.
2022-10-19 02:22:34,974 step [ 751], lr [0.0000750], embedding loss [ 0.7362], quantization loss [ 0.0398],  0.50 sec/batch.
2022-10-19 02:22:36,994 step [ 752], lr [0.0000750], embedding loss [ 0.7138], quantization loss [ 0.0442],  0.52 sec/batch.
2022-10-19 02:22:39,027 step [ 753], lr [0.0000750], embedding loss [ 0.7277], quantization loss [ 0.0448],  0.51 sec/batch.
2022-10-19 02:22:41,038 step [ 754], lr [0.0000750], embedding loss [ 0.7339], quantization loss [ 0.0446],  0.51 sec/batch.
2022-10-19 02:22:43,006 step [ 755], lr [0.0000750], embedding loss [ 0.7137], quantization loss [ 0.0416],  0.51 sec/batch.
2022-10-19 02:22:45,038 step [ 756], lr [0.0000750], embedding loss [ 0.7370], quantization loss [ 0.0467],  0.50 sec/batch.
2022-10-19 02:22:47,061 step [ 757], lr [0.0000750], embedding loss [ 0.7307], quantization loss [ 0.0438],  0.51 sec/batch.
2022-10-19 02:22:49,139 step [ 758], lr [0.0000750], embedding loss [ 0.7304], quantization loss [ 0.0447],  0.52 sec/batch.
2022-10-19 02:22:51,138 step [ 759], lr [0.0000750], embedding loss [ 0.7237], quantization loss [ 0.0453],  0.51 sec/batch.
2022-10-19 02:22:53,219 step [ 760], lr [0.0000750], embedding loss [ 0.7241], quantization loss [ 0.0406],  0.51 sec/batch.
2022-10-19 02:22:55,178 step [ 761], lr [0.0000750], embedding loss [ 0.7344], quantization loss [ 0.0388],  0.50 sec/batch.
2022-10-19 02:22:57,187 step [ 762], lr [0.0000750], embedding loss [ 0.7192], quantization loss [ 0.0426],  0.51 sec/batch.
2022-10-19 02:22:59,183 step [ 763], lr [0.0000750], embedding loss [ 0.7341], quantization loss [ 0.0405],  0.49 sec/batch.
2022-10-19 02:23:01,194 step [ 764], lr [0.0000750], embedding loss [ 0.7232], quantization loss [ 0.0412],  0.51 sec/batch.
2022-10-19 02:23:03,169 step [ 765], lr [0.0000750], embedding loss [ 0.7277], quantization loss [ 0.0391],  0.50 sec/batch.
2022-10-19 02:23:05,229 step [ 766], lr [0.0000750], embedding loss [ 0.7294], quantization loss [ 0.0352],  0.49 sec/batch.
2022-10-19 02:23:07,211 step [ 767], lr [0.0000750], embedding loss [ 0.7409], quantization loss [ 0.0383],  0.51 sec/batch.
2022-10-19 02:23:09,267 step [ 768], lr [0.0000750], embedding loss [ 0.7195], quantization loss [ 0.0414],  0.49 sec/batch.
2022-10-19 02:23:11,279 step [ 769], lr [0.0000750], embedding loss [ 0.7312], quantization loss [ 0.0389],  0.51 sec/batch.
2022-10-19 02:23:13,296 step [ 770], lr [0.0000750], embedding loss [ 0.7221], quantization loss [ 0.0385],  0.51 sec/batch.
2022-10-19 02:23:15,212 step [ 771], lr [0.0000750], embedding loss [ 0.7356], quantization loss [ 0.0402],  0.50 sec/batch.
2022-10-19 02:23:17,238 step [ 772], lr [0.0000750], embedding loss [ 0.7362], quantization loss [ 0.0413],  0.51 sec/batch.
2022-10-19 02:23:19,255 step [ 773], lr [0.0000750], embedding loss [ 0.7334], quantization loss [ 0.0410],  0.51 sec/batch.
2022-10-19 02:23:21,299 step [ 774], lr [0.0000750], embedding loss [ 0.7378], quantization loss [ 0.0396],  0.51 sec/batch.
2022-10-19 02:23:23,327 step [ 775], lr [0.0000750], embedding loss [ 0.7251], quantization loss [ 0.0433],  0.51 sec/batch.
2022-10-19 02:23:25,288 step [ 776], lr [0.0000750], embedding loss [ 0.7262], quantization loss [ 0.0415],  0.49 sec/batch.
2022-10-19 02:23:27,291 step [ 777], lr [0.0000750], embedding loss [ 0.7271], quantization loss [ 0.0477],  0.51 sec/batch.
2022-10-19 02:23:29,276 step [ 778], lr [0.0000750], embedding loss [ 0.7438], quantization loss [ 0.0408],  0.49 sec/batch.
2022-10-19 02:23:31,226 step [ 779], lr [0.0000750], embedding loss [ 0.7252], quantization loss [ 0.0365],  0.48 sec/batch.
2022-10-19 02:23:33,217 step [ 780], lr [0.0000750], embedding loss [ 0.7292], quantization loss [ 0.0387],  0.51 sec/batch.
2022-10-19 02:23:35,255 step [ 781], lr [0.0000750], embedding loss [ 0.7459], quantization loss [ 0.0484],  0.51 sec/batch.
2022-10-19 02:23:37,334 step [ 782], lr [0.0000750], embedding loss [ 0.7242], quantization loss [ 0.0407],  0.52 sec/batch.
2022-10-19 02:23:39,367 step [ 783], lr [0.0000750], embedding loss [ 0.7236], quantization loss [ 0.0410],  0.51 sec/batch.
2022-10-19 02:23:41,361 step [ 784], lr [0.0000750], embedding loss [ 0.7300], quantization loss [ 0.0373],  0.49 sec/batch.
2022-10-19 02:23:43,391 step [ 785], lr [0.0000750], embedding loss [ 0.7343], quantization loss [ 0.0383],  0.51 sec/batch.
2022-10-19 02:23:45,425 step [ 786], lr [0.0000750], embedding loss [ 0.7249], quantization loss [ 0.0429],  0.51 sec/batch.
2022-10-19 02:23:47,425 step [ 787], lr [0.0000750], embedding loss [ 0.7437], quantization loss [ 0.0394],  0.51 sec/batch.
2022-10-19 02:23:49,511 step [ 788], lr [0.0000750], embedding loss [ 0.7229], quantization loss [ 0.0419],  0.52 sec/batch.
2022-10-19 02:23:51,526 step [ 789], lr [0.0000750], embedding loss [ 0.7250], quantization loss [ 0.0422],  0.51 sec/batch.
2022-10-19 02:23:53,530 step [ 790], lr [0.0000750], embedding loss [ 0.7297], quantization loss [ 0.0395],  0.51 sec/batch.
2022-10-19 02:23:55,499 step [ 791], lr [0.0000750], embedding loss [ 0.7386], quantization loss [ 0.0422],  0.49 sec/batch.
2022-10-19 02:23:57,508 step [ 792], lr [0.0000750], embedding loss [ 0.7320], quantization loss [ 0.0397],  0.49 sec/batch.
2022-10-19 02:23:59,494 step [ 793], lr [0.0000750], embedding loss [ 0.7309], quantization loss [ 0.0407],  0.49 sec/batch.
2022-10-19 02:24:01,489 step [ 794], lr [0.0000750], embedding loss [ 0.7331], quantization loss [ 0.0387],  0.50 sec/batch.
2022-10-19 02:24:03,514 step [ 795], lr [0.0000750], embedding loss [ 0.7295], quantization loss [ 0.0441],  0.51 sec/batch.
2022-10-19 02:24:05,596 step [ 796], lr [0.0000750], embedding loss [ 0.7275], quantization loss [ 0.0457],  0.52 sec/batch.
2022-10-19 02:24:07,654 step [ 797], lr [0.0000750], embedding loss [ 0.7239], quantization loss [ 0.0425],  0.51 sec/batch.
2022-10-19 02:24:09,710 step [ 798], lr [0.0000750], embedding loss [ 0.7256], quantization loss [ 0.0390],  0.51 sec/batch.
2022-10-19 02:24:11,749 step [ 799], lr [0.0000750], embedding loss [ 0.7256], quantization loss [ 0.0415],  0.50 sec/batch.
2022-10-19 02:24:13,771 step [ 800], lr [0.0000750], embedding loss [ 0.7286], quantization loss [ 0.0411],  0.52 sec/batch.
2022-10-19 02:24:15,800 step [ 801], lr [0.0000750], embedding loss [ 0.7240], quantization loss [ 0.0369],  0.52 sec/batch.
2022-10-19 02:24:15,800 update codes and centers iter(1/1).
2022-10-19 02:24:18,989 number of update_code wrong: 0.
2022-10-19 02:24:22,161 non zero codewords: 1024.
2022-10-19 02:24:22,161 finish center update, duration: 6.36 sec.
2022-10-19 02:24:24,174 step [ 802], lr [0.0000750], embedding loss [ 0.7303], quantization loss [ 0.0428],  0.52 sec/batch.
2022-10-19 02:24:26,213 step [ 803], lr [0.0000750], embedding loss [ 0.7320], quantization loss [ 0.0395],  0.52 sec/batch.
2022-10-19 02:24:28,309 step [ 804], lr [0.0000750], embedding loss [ 0.7180], quantization loss [ 0.0414],  0.52 sec/batch.
2022-10-19 02:24:30,330 step [ 805], lr [0.0000750], embedding loss [ 0.7231], quantization loss [ 0.0409],  0.50 sec/batch.
2022-10-19 02:24:32,336 step [ 806], lr [0.0000750], embedding loss [ 0.7333], quantization loss [ 0.0448],  0.49 sec/batch.
2022-10-19 02:24:34,330 step [ 807], lr [0.0000750], embedding loss [ 0.7285], quantization loss [ 0.0419],  0.49 sec/batch.
2022-10-19 02:24:36,314 step [ 808], lr [0.0000750], embedding loss [ 0.7350], quantization loss [ 0.0442],  0.50 sec/batch.
2022-10-19 02:24:38,357 step [ 809], lr [0.0000750], embedding loss [ 0.7251], quantization loss [ 0.0412],  0.50 sec/batch.
2022-10-19 02:24:40,478 step [ 810], lr [0.0000750], embedding loss [ 0.7232], quantization loss [ 0.0366],  0.52 sec/batch.
2022-10-19 02:24:42,536 step [ 811], lr [0.0000750], embedding loss [ 0.7295], quantization loss [ 0.0390],  0.51 sec/batch.
2022-10-19 02:24:44,592 step [ 812], lr [0.0000750], embedding loss [ 0.7094], quantization loss [ 0.0381],  0.50 sec/batch.
2022-10-19 02:24:46,605 step [ 813], lr [0.0000750], embedding loss [ 0.7305], quantization loss [ 0.0396],  0.49 sec/batch.
2022-10-19 02:24:48,632 step [ 814], lr [0.0000750], embedding loss [ 0.7289], quantization loss [ 0.0414],  0.52 sec/batch.
2022-10-19 02:24:50,712 step [ 815], lr [0.0000750], embedding loss [ 0.7169], quantization loss [ 0.0468],  0.51 sec/batch.
2022-10-19 02:24:52,814 step [ 816], lr [0.0000750], embedding loss [ 0.7326], quantization loss [ 0.0432],  0.52 sec/batch.
2022-10-19 02:24:54,828 step [ 817], lr [0.0000750], embedding loss [ 0.7337], quantization loss [ 0.0419],  0.50 sec/batch.
2022-10-19 02:24:56,838 step [ 818], lr [0.0000750], embedding loss [ 0.7298], quantization loss [ 0.0407],  0.50 sec/batch.
2022-10-19 02:24:58,865 step [ 819], lr [0.0000750], embedding loss [ 0.7272], quantization loss [ 0.0420],  0.51 sec/batch.
2022-10-19 02:25:00,997 step [ 820], lr [0.0000750], embedding loss [ 0.7226], quantization loss [ 0.0404],  0.52 sec/batch.
2022-10-19 02:25:03,021 step [ 821], lr [0.0000750], embedding loss [ 0.7416], quantization loss [ 0.0410],  0.51 sec/batch.
2022-10-19 02:25:05,051 step [ 822], lr [0.0000750], embedding loss [ 0.7246], quantization loss [ 0.0386],  0.51 sec/batch.
2022-10-19 02:25:07,087 step [ 823], lr [0.0000750], embedding loss [ 0.7368], quantization loss [ 0.0433],  0.51 sec/batch.
2022-10-19 02:25:09,111 step [ 824], lr [0.0000750], embedding loss [ 0.7255], quantization loss [ 0.0367],  0.50 sec/batch.
2022-10-19 02:25:11,145 step [ 825], lr [0.0000750], embedding loss [ 0.7315], quantization loss [ 0.0499],  0.51 sec/batch.
2022-10-19 02:25:13,186 step [ 826], lr [0.0000750], embedding loss [ 0.7333], quantization loss [ 0.0477],  0.51 sec/batch.
2022-10-19 02:25:15,230 step [ 827], lr [0.0000750], embedding loss [ 0.7212], quantization loss [ 0.0395],  0.51 sec/batch.
2022-10-19 02:25:17,272 step [ 828], lr [0.0000750], embedding loss [ 0.7221], quantization loss [ 0.0400],  0.52 sec/batch.
2022-10-19 02:25:19,292 step [ 829], lr [0.0000750], embedding loss [ 0.7389], quantization loss [ 0.0400],  0.50 sec/batch.
2022-10-19 02:25:21,341 step [ 830], lr [0.0000750], embedding loss [ 0.7313], quantization loss [ 0.0463],  0.51 sec/batch.
2022-10-19 02:25:23,402 step [ 831], lr [0.0000750], embedding loss [ 0.7337], quantization loss [ 0.0414],  0.51 sec/batch.
2022-10-19 02:25:25,445 step [ 832], lr [0.0000750], embedding loss [ 0.7325], quantization loss [ 0.0395],  0.52 sec/batch.
2022-10-19 02:25:27,511 step [ 833], lr [0.0000750], embedding loss [ 0.7314], quantization loss [ 0.0411],  0.52 sec/batch.
2022-10-19 02:25:29,627 step [ 834], lr [0.0000750], embedding loss [ 0.7309], quantization loss [ 0.0452],  0.52 sec/batch.
2022-10-19 02:25:31,743 step [ 835], lr [0.0000750], embedding loss [ 0.7392], quantization loss [ 0.0430],  0.51 sec/batch.
2022-10-19 02:25:33,854 step [ 836], lr [0.0000750], embedding loss [ 0.7269], quantization loss [ 0.0404],  0.52 sec/batch.
2022-10-19 02:25:35,987 step [ 837], lr [0.0000750], embedding loss [ 0.7239], quantization loss [ 0.0420],  0.52 sec/batch.
2022-10-19 02:25:38,103 step [ 838], lr [0.0000750], embedding loss [ 0.7317], quantization loss [ 0.0456],  0.52 sec/batch.
2022-10-19 02:25:40,235 step [ 839], lr [0.0000750], embedding loss [ 0.7439], quantization loss [ 0.0494],  0.52 sec/batch.
2022-10-19 02:25:42,358 step [ 840], lr [0.0000750], embedding loss [ 0.7364], quantization loss [ 0.0427],  0.52 sec/batch.
2022-10-19 02:25:44,481 step [ 841], lr [0.0000750], embedding loss [ 0.7299], quantization loss [ 0.0442],  0.51 sec/batch.
2022-10-19 02:25:46,548 step [ 842], lr [0.0000750], embedding loss [ 0.7391], quantization loss [ 0.0419],  0.51 sec/batch.
2022-10-19 02:25:48,689 step [ 843], lr [0.0000750], embedding loss [ 0.7239], quantization loss [ 0.0393],  0.52 sec/batch.
2022-10-19 02:25:50,793 step [ 844], lr [0.0000750], embedding loss [ 0.7280], quantization loss [ 0.0417],  0.52 sec/batch.
2022-10-19 02:25:52,935 step [ 845], lr [0.0000750], embedding loss [ 0.7396], quantization loss [ 0.0364],  0.52 sec/batch.
2022-10-19 02:25:55,046 step [ 846], lr [0.0000750], embedding loss [ 0.7240], quantization loss [ 0.0336],  0.52 sec/batch.
2022-10-19 02:25:57,172 step [ 847], lr [0.0000750], embedding loss [ 0.7275], quantization loss [ 0.0374],  0.52 sec/batch.
2022-10-19 02:25:59,293 step [ 848], lr [0.0000750], embedding loss [ 0.7365], quantization loss [ 0.0387],  0.51 sec/batch.
2022-10-19 02:26:01,416 step [ 849], lr [0.0000750], embedding loss [ 0.7253], quantization loss [ 0.0363],  0.52 sec/batch.
2022-10-19 02:26:03,508 step [ 850], lr [0.0000750], embedding loss [ 0.7340], quantization loss [ 0.0383],  0.51 sec/batch.
2022-10-19 02:26:05,630 step [ 851], lr [0.0000750], embedding loss [ 0.7356], quantization loss [ 0.0381],  0.52 sec/batch.
2022-10-19 02:26:07,746 step [ 852], lr [0.0000750], embedding loss [ 0.7227], quantization loss [ 0.0381],  0.51 sec/batch.
2022-10-19 02:26:09,815 step [ 853], lr [0.0000750], embedding loss [ 0.7267], quantization loss [ 0.0393],  0.51 sec/batch.
2022-10-19 02:26:11,934 step [ 854], lr [0.0000750], embedding loss [ 0.7174], quantization loss [ 0.0399],  0.52 sec/batch.
2022-10-19 02:26:14,028 step [ 855], lr [0.0000750], embedding loss [ 0.7274], quantization loss [ 0.0390],  0.51 sec/batch.
2022-10-19 02:26:16,150 step [ 856], lr [0.0000750], embedding loss [ 0.7200], quantization loss [ 0.0416],  0.52 sec/batch.
2022-10-19 02:26:18,260 step [ 857], lr [0.0000750], embedding loss [ 0.7292], quantization loss [ 0.0378],  0.51 sec/batch.
2022-10-19 02:26:20,384 step [ 858], lr [0.0000750], embedding loss [ 0.7366], quantization loss [ 0.0404],  0.52 sec/batch.
2022-10-19 02:26:22,490 step [ 859], lr [0.0000750], embedding loss [ 0.7332], quantization loss [ 0.0405],  0.51 sec/batch.
2022-10-19 02:26:24,618 step [ 860], lr [0.0000750], embedding loss [ 0.7217], quantization loss [ 0.0381],  0.52 sec/batch.
2022-10-19 02:26:26,763 step [ 861], lr [0.0000750], embedding loss [ 0.7196], quantization loss [ 0.0387],  0.51 sec/batch.
2022-10-19 02:26:28,888 step [ 862], lr [0.0000750], embedding loss [ 0.7175], quantization loss [ 0.0393],  0.52 sec/batch.
2022-10-19 02:26:30,981 step [ 863], lr [0.0000750], embedding loss [ 0.7227], quantization loss [ 0.0376],  0.51 sec/batch.
2022-10-19 02:26:33,101 step [ 864], lr [0.0000750], embedding loss [ 0.7286], quantization loss [ 0.0409],  0.52 sec/batch.
2022-10-19 02:26:35,123 step [ 865], lr [0.0000750], embedding loss [ 0.7181], quantization loss [ 0.0411],  0.51 sec/batch.
2022-10-19 02:26:37,250 step [ 866], lr [0.0000750], embedding loss [ 0.7302], quantization loss [ 0.0446],  0.52 sec/batch.
2022-10-19 02:26:39,338 step [ 867], lr [0.0000750], embedding loss [ 0.7350], quantization loss [ 0.0417],  0.52 sec/batch.
2022-10-19 02:26:41,451 step [ 868], lr [0.0000750], embedding loss [ 0.7310], quantization loss [ 0.0427],  0.52 sec/batch.
2022-10-19 02:26:43,468 step [ 869], lr [0.0000750], embedding loss [ 0.7272], quantization loss [ 0.0401],  0.51 sec/batch.
2022-10-19 02:26:45,616 step [ 870], lr [0.0000750], embedding loss [ 0.7144], quantization loss [ 0.0418],  0.52 sec/batch.
2022-10-19 02:26:47,713 step [ 871], lr [0.0000750], embedding loss [ 0.7302], quantization loss [ 0.0402],  0.52 sec/batch.
2022-10-19 02:26:49,781 step [ 872], lr [0.0000750], embedding loss [ 0.7254], quantization loss [ 0.0413],  0.52 sec/batch.
2022-10-19 02:26:51,833 step [ 873], lr [0.0000750], embedding loss [ 0.7309], quantization loss [ 0.0459],  0.51 sec/batch.
2022-10-19 02:26:53,914 step [ 874], lr [0.0000750], embedding loss [ 0.7399], quantization loss [ 0.0391],  0.52 sec/batch.
2022-10-19 02:26:56,058 step [ 875], lr [0.0000750], embedding loss [ 0.7315], quantization loss [ 0.0383],  0.52 sec/batch.
2022-10-19 02:26:58,141 step [ 876], lr [0.0000750], embedding loss [ 0.7450], quantization loss [ 0.0355],  0.52 sec/batch.
2022-10-19 02:27:00,272 step [ 877], lr [0.0000750], embedding loss [ 0.7305], quantization loss [ 0.0372],  0.53 sec/batch.
2022-10-19 02:27:02,394 step [ 878], lr [0.0000750], embedding loss [ 0.7269], quantization loss [ 0.0383],  0.51 sec/batch.
2022-10-19 02:27:04,477 step [ 879], lr [0.0000750], embedding loss [ 0.7300], quantization loss [ 0.0385],  0.51 sec/batch.
2022-10-19 02:27:06,561 step [ 880], lr [0.0000750], embedding loss [ 0.7229], quantization loss [ 0.0341],  0.51 sec/batch.
2022-10-19 02:27:08,690 step [ 881], lr [0.0000750], embedding loss [ 0.7187], quantization loss [ 0.0406],  0.52 sec/batch.
2022-10-19 02:27:08,690 update codes and centers iter(1/1).
2022-10-19 02:27:11,879 number of update_code wrong: 0.
2022-10-19 02:27:15,320 non zero codewords: 1024.
2022-10-19 02:27:15,321 finish center update, duration: 6.63 sec.
2022-10-19 02:27:17,272 step [ 882], lr [0.0000750], embedding loss [ 0.7299], quantization loss [ 0.0425],  0.51 sec/batch.
2022-10-19 02:27:19,297 step [ 883], lr [0.0000750], embedding loss [ 0.7153], quantization loss [ 0.0430],  0.51 sec/batch.
2022-10-19 02:27:21,544 step [ 884], lr [0.0000750], embedding loss [ 0.7407], quantization loss [ 0.0403],  0.56 sec/batch.
2022-10-19 02:27:23,608 step [ 885], lr [0.0000750], embedding loss [ 0.7163], quantization loss [ 0.0440],  0.51 sec/batch.
2022-10-19 02:27:25,666 step [ 886], lr [0.0000750], embedding loss [ 0.7293], quantization loss [ 0.0416],  0.52 sec/batch.
2022-10-19 02:27:27,846 step [ 887], lr [0.0000750], embedding loss [ 0.7239], quantization loss [ 0.0404],  0.52 sec/batch.
2022-10-19 02:27:29,952 step [ 888], lr [0.0000750], embedding loss [ 0.7288], quantization loss [ 0.0370],  0.52 sec/batch.
2022-10-19 02:27:32,107 step [ 889], lr [0.0000750], embedding loss [ 0.7377], quantization loss [ 0.0435],  0.51 sec/batch.
2022-10-19 02:27:34,280 step [ 890], lr [0.0000750], embedding loss [ 0.7253], quantization loss [ 0.0380],  0.52 sec/batch.
2022-10-19 02:27:36,421 step [ 891], lr [0.0000750], embedding loss [ 0.7215], quantization loss [ 0.0407],  0.52 sec/batch.
2022-10-19 02:27:38,501 step [ 892], lr [0.0000750], embedding loss [ 0.7246], quantization loss [ 0.0389],  0.51 sec/batch.
2022-10-19 02:27:40,594 step [ 893], lr [0.0000750], embedding loss [ 0.7272], quantization loss [ 0.0399],  0.51 sec/batch.
2022-10-19 02:27:42,686 step [ 894], lr [0.0000750], embedding loss [ 0.7305], quantization loss [ 0.0399],  0.52 sec/batch.
2022-10-19 02:27:44,811 step [ 895], lr [0.0000750], embedding loss [ 0.7220], quantization loss [ 0.0411],  0.52 sec/batch.
2022-10-19 02:27:46,915 step [ 896], lr [0.0000750], embedding loss [ 0.7241], quantization loss [ 0.0372],  0.51 sec/batch.
2022-10-19 02:27:49,017 step [ 897], lr [0.0000750], embedding loss [ 0.7265], quantization loss [ 0.0364],  0.52 sec/batch.
2022-10-19 02:27:51,107 step [ 898], lr [0.0000750], embedding loss [ 0.7269], quantization loss [ 0.0417],  0.52 sec/batch.
2022-10-19 02:27:53,208 step [ 899], lr [0.0000750], embedding loss [ 0.7371], quantization loss [ 0.0408],  0.52 sec/batch.
2022-10-19 02:27:55,282 step [ 900], lr [0.0000750], embedding loss [ 0.7245], quantization loss [ 0.0413],  0.51 sec/batch.
2022-10-19 02:27:57,386 step [ 901], lr [0.0000375], embedding loss [ 0.7273], quantization loss [ 0.0383],  0.52 sec/batch.
2022-10-19 02:27:59,478 step [ 902], lr [0.0000375], embedding loss [ 0.7385], quantization loss [ 0.0350],  0.51 sec/batch.
2022-10-19 02:28:01,601 step [ 903], lr [0.0000375], embedding loss [ 0.7242], quantization loss [ 0.0391],  0.52 sec/batch.
2022-10-19 02:28:03,747 step [ 904], lr [0.0000375], embedding loss [ 0.7284], quantization loss [ 0.0415],  0.52 sec/batch.
2022-10-19 02:28:05,892 step [ 905], lr [0.0000375], embedding loss [ 0.7252], quantization loss [ 0.0351],  0.52 sec/batch.
2022-10-19 02:28:08,037 step [ 906], lr [0.0000375], embedding loss [ 0.7267], quantization loss [ 0.0409],  0.53 sec/batch.
2022-10-19 02:28:10,177 step [ 907], lr [0.0000375], embedding loss [ 0.7258], quantization loss [ 0.0386],  0.51 sec/batch.
2022-10-19 02:28:12,313 step [ 908], lr [0.0000375], embedding loss [ 0.7248], quantization loss [ 0.0387],  0.52 sec/batch.
2022-10-19 02:28:14,402 step [ 909], lr [0.0000375], embedding loss [ 0.7220], quantization loss [ 0.0391],  0.51 sec/batch.
2022-10-19 02:28:16,470 step [ 910], lr [0.0000375], embedding loss [ 0.7275], quantization loss [ 0.0362],  0.51 sec/batch.
2022-10-19 02:28:18,566 step [ 911], lr [0.0000375], embedding loss [ 0.7292], quantization loss [ 0.0444],  0.51 sec/batch.
2022-10-19 02:28:20,712 step [ 912], lr [0.0000375], embedding loss [ 0.7307], quantization loss [ 0.0409],  0.52 sec/batch.
2022-10-19 02:28:22,812 step [ 913], lr [0.0000375], embedding loss [ 0.7183], quantization loss [ 0.0370],  0.52 sec/batch.
2022-10-19 02:28:24,905 step [ 914], lr [0.0000375], embedding loss [ 0.7252], quantization loss [ 0.0449],  0.52 sec/batch.
2022-10-19 02:28:26,921 step [ 915], lr [0.0000375], embedding loss [ 0.7347], quantization loss [ 0.0435],  0.54 sec/batch.
2022-10-19 02:28:29,002 step [ 916], lr [0.0000375], embedding loss [ 0.7218], quantization loss [ 0.0401],  0.51 sec/batch.
2022-10-19 02:28:31,027 step [ 917], lr [0.0000375], embedding loss [ 0.7204], quantization loss [ 0.0365],  0.51 sec/batch.
2022-10-19 02:28:33,167 step [ 918], lr [0.0000375], embedding loss [ 0.7387], quantization loss [ 0.0357],  0.52 sec/batch.
2022-10-19 02:28:35,248 step [ 919], lr [0.0000375], embedding loss [ 0.7294], quantization loss [ 0.0431],  0.51 sec/batch.
2022-10-19 02:28:37,411 step [ 920], lr [0.0000375], embedding loss [ 0.7367], quantization loss [ 0.0401],  0.52 sec/batch.
2022-10-19 02:28:39,528 step [ 921], lr [0.0000375], embedding loss [ 0.7178], quantization loss [ 0.0411],  0.52 sec/batch.
2022-10-19 02:28:41,589 step [ 922], lr [0.0000375], embedding loss [ 0.7142], quantization loss [ 0.0392],  0.51 sec/batch.
2022-10-19 02:28:43,619 step [ 923], lr [0.0000375], embedding loss [ 0.7332], quantization loss [ 0.0349],  0.49 sec/batch.
2022-10-19 02:28:45,663 step [ 924], lr [0.0000375], embedding loss [ 0.7301], quantization loss [ 0.0410],  0.50 sec/batch.
2022-10-19 02:28:47,743 step [ 925], lr [0.0000375], embedding loss [ 0.7252], quantization loss [ 0.0377],  0.51 sec/batch.
2022-10-19 02:28:49,829 step [ 926], lr [0.0000375], embedding loss [ 0.7347], quantization loss [ 0.0390],  0.52 sec/batch.
2022-10-19 02:28:51,914 step [ 927], lr [0.0000375], embedding loss [ 0.7302], quantization loss [ 0.0335],  0.53 sec/batch.
2022-10-19 02:28:53,952 step [ 928], lr [0.0000375], embedding loss [ 0.7260], quantization loss [ 0.0404],  0.51 sec/batch.
2022-10-19 02:28:56,051 step [ 929], lr [0.0000375], embedding loss [ 0.7272], quantization loss [ 0.0358],  0.52 sec/batch.
2022-10-19 02:28:58,149 step [ 930], lr [0.0000375], embedding loss [ 0.7231], quantization loss [ 0.0382],  0.52 sec/batch.
2022-10-19 02:29:00,234 step [ 931], lr [0.0000375], embedding loss [ 0.7263], quantization loss [ 0.0385],  0.52 sec/batch.
2022-10-19 02:29:02,334 step [ 932], lr [0.0000375], embedding loss [ 0.7275], quantization loss [ 0.0420],  0.52 sec/batch.
2022-10-19 02:29:04,430 step [ 933], lr [0.0000375], embedding loss [ 0.7178], quantization loss [ 0.0410],  0.52 sec/batch.
2022-10-19 02:29:06,471 step [ 934], lr [0.0000375], embedding loss [ 0.7211], quantization loss [ 0.0362],  0.51 sec/batch.
2022-10-19 02:29:08,549 step [ 935], lr [0.0000375], embedding loss [ 0.7356], quantization loss [ 0.0408],  0.51 sec/batch.
2022-10-19 02:29:10,617 step [ 936], lr [0.0000375], embedding loss [ 0.7295], quantization loss [ 0.0436],  0.51 sec/batch.
2022-10-19 02:29:12,700 step [ 937], lr [0.0000375], embedding loss [ 0.7384], quantization loss [ 0.0377],  0.51 sec/batch.
2022-10-19 02:29:14,803 step [ 938], lr [0.0000375], embedding loss [ 0.7146], quantization loss [ 0.0341],  0.52 sec/batch.
2022-10-19 02:29:16,944 step [ 939], lr [0.0000375], embedding loss [ 0.7175], quantization loss [ 0.0385],  0.51 sec/batch.
2022-10-19 02:29:18,994 step [ 940], lr [0.0000375], embedding loss [ 0.7297], quantization loss [ 0.0358],  0.50 sec/batch.
2022-10-19 02:29:21,055 step [ 941], lr [0.0000375], embedding loss [ 0.7397], quantization loss [ 0.0398],  0.52 sec/batch.
2022-10-19 02:29:23,064 step [ 942], lr [0.0000375], embedding loss [ 0.7254], quantization loss [ 0.0377],  0.51 sec/batch.
2022-10-19 02:29:25,011 step [ 943], lr [0.0000375], embedding loss [ 0.7393], quantization loss [ 0.0368],  0.48 sec/batch.
2022-10-19 02:29:27,022 step [ 944], lr [0.0000375], embedding loss [ 0.7093], quantization loss [ 0.0387],  0.49 sec/batch.
2022-10-19 02:29:29,082 step [ 945], lr [0.0000375], embedding loss [ 0.7287], quantization loss [ 0.0366],  0.52 sec/batch.
2022-10-19 02:29:31,129 step [ 946], lr [0.0000375], embedding loss [ 0.7316], quantization loss [ 0.0348],  0.50 sec/batch.
2022-10-19 02:29:33,268 step [ 947], lr [0.0000375], embedding loss [ 0.7230], quantization loss [ 0.0397],  0.52 sec/batch.
2022-10-19 02:29:35,314 step [ 948], lr [0.0000375], embedding loss [ 0.7354], quantization loss [ 0.0442],  0.50 sec/batch.
2022-10-19 02:29:37,389 step [ 949], lr [0.0000375], embedding loss [ 0.7327], quantization loss [ 0.0362],  0.51 sec/batch.
2022-10-19 02:29:39,421 step [ 950], lr [0.0000375], embedding loss [ 0.7321], quantization loss [ 0.0374],  0.50 sec/batch.
2022-10-19 02:29:41,572 step [ 951], lr [0.0000375], embedding loss [ 0.7270], quantization loss [ 0.0413],  0.52 sec/batch.
2022-10-19 02:29:43,686 step [ 952], lr [0.0000375], embedding loss [ 0.7295], quantization loss [ 0.0413],  0.51 sec/batch.
2022-10-19 02:29:45,755 step [ 953], lr [0.0000375], embedding loss [ 0.7260], quantization loss [ 0.0393],  0.51 sec/batch.
2022-10-19 02:29:47,859 step [ 954], lr [0.0000375], embedding loss [ 0.7365], quantization loss [ 0.0369],  0.50 sec/batch.
2022-10-19 02:29:49,976 step [ 955], lr [0.0000375], embedding loss [ 0.7292], quantization loss [ 0.0376],  0.51 sec/batch.
2022-10-19 02:29:52,002 step [ 956], lr [0.0000375], embedding loss [ 0.7251], quantization loss [ 0.0373],  0.49 sec/batch.
2022-10-19 02:29:54,098 step [ 957], lr [0.0000375], embedding loss [ 0.7242], quantization loss [ 0.0360],  0.51 sec/batch.
2022-10-19 02:29:56,121 step [ 958], lr [0.0000375], embedding loss [ 0.7181], quantization loss [ 0.0381],  0.51 sec/batch.
2022-10-19 02:29:58,191 step [ 959], lr [0.0000375], embedding loss [ 0.7147], quantization loss [ 0.0399],  0.51 sec/batch.
2022-10-19 02:30:00,307 step [ 960], lr [0.0000375], embedding loss [ 0.7260], quantization loss [ 0.0395],  0.52 sec/batch.
2022-10-19 02:30:02,402 step [ 961], lr [0.0000375], embedding loss [ 0.7205], quantization loss [ 0.0384],  0.51 sec/batch.
2022-10-19 02:30:02,402 update codes and centers iter(1/1).
2022-10-19 02:30:05,590 number of update_code wrong: 0.
2022-10-19 02:30:09,174 non zero codewords: 1024.
2022-10-19 02:30:09,174 finish center update, duration: 6.77 sec.
2022-10-19 02:30:11,231 step [ 962], lr [0.0000375], embedding loss [ 0.7124], quantization loss [ 0.0381],  0.53 sec/batch.
2022-10-19 02:30:13,322 step [ 963], lr [0.0000375], embedding loss [ 0.7237], quantization loss [ 0.0364],  0.51 sec/batch.
2022-10-19 02:30:15,461 step [ 964], lr [0.0000375], embedding loss [ 0.7247], quantization loss [ 0.0373],  0.52 sec/batch.
2022-10-19 02:30:17,600 step [ 965], lr [0.0000375], embedding loss [ 0.7321], quantization loss [ 0.0380],  0.52 sec/batch.
2022-10-19 02:30:19,714 step [ 966], lr [0.0000375], embedding loss [ 0.7258], quantization loss [ 0.0386],  0.52 sec/batch.
2022-10-19 02:30:21,879 step [ 967], lr [0.0000375], embedding loss [ 0.7199], quantization loss [ 0.0380],  0.52 sec/batch.
2022-10-19 02:30:24,057 step [ 968], lr [0.0000375], embedding loss [ 0.7272], quantization loss [ 0.0364],  0.52 sec/batch.
2022-10-19 02:30:26,144 step [ 969], lr [0.0000375], embedding loss [ 0.7148], quantization loss [ 0.0400],  0.49 sec/batch.
2022-10-19 02:30:28,162 step [ 970], lr [0.0000375], embedding loss [ 0.7350], quantization loss [ 0.0370],  0.50 sec/batch.
2022-10-19 02:30:30,277 step [ 971], lr [0.0000375], embedding loss [ 0.7325], quantization loss [ 0.0391],  0.51 sec/batch.
2022-10-19 02:30:32,450 step [ 972], lr [0.0000375], embedding loss [ 0.7258], quantization loss [ 0.0358],  0.50 sec/batch.
2022-10-19 02:30:34,538 step [ 973], lr [0.0000375], embedding loss [ 0.7276], quantization loss [ 0.0375],  0.51 sec/batch.
2022-10-19 02:30:36,694 step [ 974], lr [0.0000375], embedding loss [ 0.7183], quantization loss [ 0.0384],  0.52 sec/batch.
2022-10-19 02:30:38,761 step [ 975], lr [0.0000375], embedding loss [ 0.7242], quantization loss [ 0.0405],  0.51 sec/batch.
2022-10-19 02:30:40,856 step [ 976], lr [0.0000375], embedding loss [ 0.7215], quantization loss [ 0.0377],  0.52 sec/batch.
2022-10-19 02:30:42,939 step [ 977], lr [0.0000375], embedding loss [ 0.7354], quantization loss [ 0.0393],  0.52 sec/batch.
2022-10-19 02:30:45,134 step [ 978], lr [0.0000375], embedding loss [ 0.7262], quantization loss [ 0.0342],  0.52 sec/batch.
2022-10-19 02:30:47,241 step [ 979], lr [0.0000375], embedding loss [ 0.7314], quantization loss [ 0.0342],  0.51 sec/batch.
2022-10-19 02:30:49,393 step [ 980], lr [0.0000375], embedding loss [ 0.7270], quantization loss [ 0.0364],  0.51 sec/batch.
2022-10-19 02:30:51,386 step [ 981], lr [0.0000375], embedding loss [ 0.7299], quantization loss [ 0.0359],  0.51 sec/batch.
2022-10-19 02:30:53,498 step [ 982], lr [0.0000375], embedding loss [ 0.7265], quantization loss [ 0.0361],  0.52 sec/batch.
2022-10-19 02:30:55,553 step [ 983], lr [0.0000375], embedding loss [ 0.7271], quantization loss [ 0.0386],  0.49 sec/batch.
2022-10-19 02:30:57,607 step [ 984], lr [0.0000375], embedding loss [ 0.7209], quantization loss [ 0.0440],  0.50 sec/batch.
2022-10-19 02:30:59,689 step [ 985], lr [0.0000375], embedding loss [ 0.7273], quantization loss [ 0.0380],  0.50 sec/batch.
2022-10-19 02:31:01,781 step [ 986], lr [0.0000375], embedding loss [ 0.7203], quantization loss [ 0.0361],  0.50 sec/batch.
2022-10-19 02:31:03,898 step [ 987], lr [0.0000375], embedding loss [ 0.7352], quantization loss [ 0.0381],  0.51 sec/batch.
2022-10-19 02:31:05,990 step [ 988], lr [0.0000375], embedding loss [ 0.7195], quantization loss [ 0.0393],  0.51 sec/batch.
2022-10-19 02:31:08,090 step [ 989], lr [0.0000375], embedding loss [ 0.7091], quantization loss [ 0.0392],  0.51 sec/batch.
2022-10-19 02:31:10,175 step [ 990], lr [0.0000375], embedding loss [ 0.7317], quantization loss [ 0.0372],  0.51 sec/batch.
2022-10-19 02:31:12,317 step [ 991], lr [0.0000375], embedding loss [ 0.7183], quantization loss [ 0.0379],  0.52 sec/batch.
2022-10-19 02:31:14,443 step [ 992], lr [0.0000375], embedding loss [ 0.7337], quantization loss [ 0.0370],  0.51 sec/batch.
2022-10-19 02:31:16,544 step [ 993], lr [0.0000375], embedding loss [ 0.7276], quantization loss [ 0.0364],  0.51 sec/batch.
2022-10-19 02:31:18,564 step [ 994], lr [0.0000375], embedding loss [ 0.7247], quantization loss [ 0.0332],  0.51 sec/batch.
2022-10-19 02:31:20,736 step [ 995], lr [0.0000375], embedding loss [ 0.7161], quantization loss [ 0.0335],  0.52 sec/batch.
2022-10-19 02:31:22,840 step [ 996], lr [0.0000375], embedding loss [ 0.7240], quantization loss [ 0.0371],  0.50 sec/batch.
2022-10-19 02:31:24,914 step [ 997], lr [0.0000375], embedding loss [ 0.7270], quantization loss [ 0.0367],  0.49 sec/batch.
2022-10-19 02:31:26,962 step [ 998], lr [0.0000375], embedding loss [ 0.7242], quantization loss [ 0.0367],  0.50 sec/batch.
2022-10-19 02:31:29,135 step [ 999], lr [0.0000375], embedding loss [ 0.7305], quantization loss [ 0.0383],  0.52 sec/batch.
2022-10-19 02:31:31,210 step [1000], lr [0.0000375], embedding loss [ 0.7385], quantization loss [ 0.0396],  0.52 sec/batch.
2022-10-19 02:31:33,394 step [1001], lr [0.0000375], embedding loss [ 0.7254], quantization loss [ 0.0380],  0.52 sec/batch.
2022-10-19 02:31:35,486 step [1002], lr [0.0000375], embedding loss [ 0.7256], quantization loss [ 0.0347],  0.51 sec/batch.
2022-10-19 02:31:37,645 step [1003], lr [0.0000375], embedding loss [ 0.7327], quantization loss [ 0.0355],  0.52 sec/batch.
2022-10-19 02:31:39,702 step [1004], lr [0.0000375], embedding loss [ 0.7195], quantization loss [ 0.0341],  0.51 sec/batch.
2022-10-19 02:31:41,862 step [1005], lr [0.0000375], embedding loss [ 0.7409], quantization loss [ 0.0377],  0.52 sec/batch.
2022-10-19 02:31:44,011 step [1006], lr [0.0000375], embedding loss [ 0.7333], quantization loss [ 0.0335],  0.51 sec/batch.
2022-10-19 02:31:46,084 step [1007], lr [0.0000375], embedding loss [ 0.7216], quantization loss [ 0.0339],  0.51 sec/batch.
2022-10-19 02:31:48,155 step [1008], lr [0.0000375], embedding loss [ 0.7225], quantization loss [ 0.0380],  0.51 sec/batch.
2022-10-19 02:31:50,286 step [1009], lr [0.0000375], embedding loss [ 0.7258], quantization loss [ 0.0351],  0.51 sec/batch.
2022-10-19 02:31:52,455 step [1010], lr [0.0000375], embedding loss [ 0.7223], quantization loss [ 0.0381],  0.52 sec/batch.
2022-10-19 02:31:54,539 step [1011], lr [0.0000375], embedding loss [ 0.7171], quantization loss [ 0.0353],  0.51 sec/batch.
2022-10-19 02:31:56,630 step [1012], lr [0.0000375], embedding loss [ 0.7278], quantization loss [ 0.0366],  0.51 sec/batch.
2022-10-19 02:31:58,769 step [1013], lr [0.0000375], embedding loss [ 0.7198], quantization loss [ 0.0360],  0.51 sec/batch.
2022-10-19 02:32:00,877 step [1014], lr [0.0000375], embedding loss [ 0.7375], quantization loss [ 0.0372],  0.51 sec/batch.
2022-10-19 02:32:02,963 step [1015], lr [0.0000375], embedding loss [ 0.7266], quantization loss [ 0.0350],  0.53 sec/batch.
2022-10-19 02:32:05,000 step [1016], lr [0.0000375], embedding loss [ 0.7289], quantization loss [ 0.0367],  0.51 sec/batch.
2022-10-19 02:32:07,098 step [1017], lr [0.0000375], embedding loss [ 0.7236], quantization loss [ 0.0350],  0.51 sec/batch.
2022-10-19 02:32:09,247 step [1018], lr [0.0000375], embedding loss [ 0.7234], quantization loss [ 0.0368],  0.51 sec/batch.
2022-10-19 02:32:11,352 step [1019], lr [0.0000375], embedding loss [ 0.7170], quantization loss [ 0.0381],  0.52 sec/batch.
2022-10-19 02:32:13,496 step [1020], lr [0.0000375], embedding loss [ 0.7332], quantization loss [ 0.0331],  0.52 sec/batch.
2022-10-19 02:32:15,529 step [1021], lr [0.0000375], embedding loss [ 0.7118], quantization loss [ 0.0361],  0.50 sec/batch.
2022-10-19 02:32:17,638 step [1022], lr [0.0000375], embedding loss [ 0.7288], quantization loss [ 0.0344],  0.52 sec/batch.
2022-10-19 02:32:19,726 step [1023], lr [0.0000375], embedding loss [ 0.7324], quantization loss [ 0.0377],  0.51 sec/batch.
2022-10-19 02:32:21,743 step [1024], lr [0.0000375], embedding loss [ 0.7193], quantization loss [ 0.0378],  0.49 sec/batch.
2022-10-19 02:32:23,858 step [1025], lr [0.0000375], embedding loss [ 0.7196], quantization loss [ 0.0313],  0.51 sec/batch.
2022-10-19 02:32:25,985 step [1026], lr [0.0000375], embedding loss [ 0.7226], quantization loss [ 0.0402],  0.52 sec/batch.
2022-10-19 02:32:28,136 step [1027], lr [0.0000375], embedding loss [ 0.7241], quantization loss [ 0.0367],  0.52 sec/batch.
2022-10-19 02:32:30,269 step [1028], lr [0.0000375], embedding loss [ 0.7229], quantization loss [ 0.0360],  0.51 sec/batch.
2022-10-19 02:32:32,393 step [1029], lr [0.0000375], embedding loss [ 0.7287], quantization loss [ 0.0346],  0.52 sec/batch.
2022-10-19 02:32:34,500 step [1030], lr [0.0000375], embedding loss [ 0.7319], quantization loss [ 0.0392],  0.52 sec/batch.
2022-10-19 02:32:36,619 step [1031], lr [0.0000375], embedding loss [ 0.7240], quantization loss [ 0.0347],  0.51 sec/batch.
2022-10-19 02:32:38,732 step [1032], lr [0.0000375], embedding loss [ 0.7304], quantization loss [ 0.0359],  0.51 sec/batch.
2022-10-19 02:32:40,851 step [1033], lr [0.0000375], embedding loss [ 0.7239], quantization loss [ 0.0371],  0.52 sec/batch.
2022-10-19 02:32:42,973 step [1034], lr [0.0000375], embedding loss [ 0.7304], quantization loss [ 0.0332],  0.52 sec/batch.
2022-10-19 02:32:45,100 step [1035], lr [0.0000375], embedding loss [ 0.7252], quantization loss [ 0.0377],  0.52 sec/batch.
2022-10-19 02:32:47,196 step [1036], lr [0.0000375], embedding loss [ 0.7192], quantization loss [ 0.0392],  0.51 sec/batch.
2022-10-19 02:32:49,314 step [1037], lr [0.0000375], embedding loss [ 0.7222], quantization loss [ 0.0382],  0.51 sec/batch.
2022-10-19 02:32:51,411 step [1038], lr [0.0000375], embedding loss [ 0.7151], quantization loss [ 0.0381],  0.51 sec/batch.
2022-10-19 02:32:53,554 step [1039], lr [0.0000375], embedding loss [ 0.7273], quantization loss [ 0.0394],  0.51 sec/batch.
2022-10-19 02:32:55,652 step [1040], lr [0.0000375], embedding loss [ 0.7224], quantization loss [ 0.0355],  0.51 sec/batch.
2022-10-19 02:32:57,785 step [1041], lr [0.0000375], embedding loss [ 0.7173], quantization loss [ 0.0387],  0.52 sec/batch.
2022-10-19 02:32:57,785 update codes and centers iter(1/1).
2022-10-19 02:33:00,976 number of update_code wrong: 0.
2022-10-19 02:33:04,397 non zero codewords: 1024.
2022-10-19 02:33:04,398 finish center update, duration: 6.61 sec.
2022-10-19 02:33:06,491 step [1042], lr [0.0000375], embedding loss [ 0.7103], quantization loss [ 0.0323],  0.52 sec/batch.
2022-10-19 02:33:08,641 step [1043], lr [0.0000375], embedding loss [ 0.7403], quantization loss [ 0.0362],  0.52 sec/batch.
2022-10-19 02:33:10,886 step [1044], lr [0.0000375], embedding loss [ 0.7254], quantization loss [ 0.0366],  0.52 sec/batch.
2022-10-19 02:33:13,055 step [1045], lr [0.0000375], embedding loss [ 0.7039], quantization loss [ 0.0343],  0.52 sec/batch.
2022-10-19 02:33:15,193 step [1046], lr [0.0000375], embedding loss [ 0.7390], quantization loss [ 0.0375],  0.51 sec/batch.
2022-10-19 02:33:17,250 step [1047], lr [0.0000375], embedding loss [ 0.7367], quantization loss [ 0.0345],  0.51 sec/batch.
2022-10-19 02:33:19,338 step [1048], lr [0.0000375], embedding loss [ 0.7234], quantization loss [ 0.0394],  0.51 sec/batch.
2022-10-19 02:33:21,455 step [1049], lr [0.0000375], embedding loss [ 0.7261], quantization loss [ 0.0360],  0.51 sec/batch.
2022-10-19 02:33:23,471 step [1050], lr [0.0000375], embedding loss [ 0.7399], quantization loss [ 0.0363],  0.49 sec/batch.
2022-10-19 02:33:25,611 step [1051], lr [0.0000375], embedding loss [ 0.7168], quantization loss [ 0.0375],  0.50 sec/batch.
2022-10-19 02:33:27,747 step [1052], lr [0.0000375], embedding loss [ 0.7260], quantization loss [ 0.0409],  0.50 sec/batch.
2022-10-19 02:33:29,836 step [1053], lr [0.0000375], embedding loss [ 0.7288], quantization loss [ 0.0359],  0.51 sec/batch.
2022-10-19 02:33:31,993 step [1054], lr [0.0000375], embedding loss [ 0.7215], quantization loss [ 0.0377],  0.51 sec/batch.
2022-10-19 02:33:34,162 step [1055], lr [0.0000375], embedding loss [ 0.7188], quantization loss [ 0.0373],  0.51 sec/batch.
2022-10-19 02:33:36,306 step [1056], lr [0.0000375], embedding loss [ 0.7196], quantization loss [ 0.0348],  0.52 sec/batch.
2022-10-19 02:33:38,458 step [1057], lr [0.0000375], embedding loss [ 0.7462], quantization loss [ 0.0391],  0.51 sec/batch.
2022-10-19 02:33:40,661 step [1058], lr [0.0000375], embedding loss [ 0.7120], quantization loss [ 0.0334],  0.52 sec/batch.
2022-10-19 02:33:42,850 step [1059], lr [0.0000375], embedding loss [ 0.7197], quantization loss [ 0.0341],  0.51 sec/batch.
2022-10-19 02:33:45,018 step [1060], lr [0.0000375], embedding loss [ 0.7250], quantization loss [ 0.0354],  0.51 sec/batch.
2022-10-19 02:33:47,096 step [1061], lr [0.0000375], embedding loss [ 0.7173], quantization loss [ 0.0368],  0.50 sec/batch.
2022-10-19 02:33:49,176 step [1062], lr [0.0000375], embedding loss [ 0.7287], quantization loss [ 0.0341],  0.50 sec/batch.
2022-10-19 02:33:51,179 step [1063], lr [0.0000375], embedding loss [ 0.7263], quantization loss [ 0.0370],  0.51 sec/batch.
2022-10-19 02:33:53,360 step [1064], lr [0.0000375], embedding loss [ 0.7259], quantization loss [ 0.0354],  0.51 sec/batch.
2022-10-19 02:33:55,468 step [1065], lr [0.0000375], embedding loss [ 0.7272], quantization loss [ 0.0407],  0.51 sec/batch.
2022-10-19 02:33:57,615 step [1066], lr [0.0000375], embedding loss [ 0.7224], quantization loss [ 0.0384],  0.51 sec/batch.
2022-10-19 02:33:59,711 step [1067], lr [0.0000375], embedding loss [ 0.7284], quantization loss [ 0.0356],  0.52 sec/batch.
2022-10-19 02:34:01,863 step [1068], lr [0.0000375], embedding loss [ 0.7197], quantization loss [ 0.0373],  0.51 sec/batch.
2022-10-19 02:34:03,999 step [1069], lr [0.0000375], embedding loss [ 0.7280], quantization loss [ 0.0387],  0.52 sec/batch.
2022-10-19 02:34:06,084 step [1070], lr [0.0000375], embedding loss [ 0.7277], quantization loss [ 0.0395],  0.49 sec/batch.
2022-10-19 02:34:08,178 step [1071], lr [0.0000375], embedding loss [ 0.7236], quantization loss [ 0.0346],  0.52 sec/batch.
2022-10-19 02:34:10,328 step [1072], lr [0.0000375], embedding loss [ 0.7207], quantization loss [ 0.0368],  0.51 sec/batch.
2022-10-19 02:34:12,467 step [1073], lr [0.0000375], embedding loss [ 0.7152], quantization loss [ 0.0347],  0.51 sec/batch.
2022-10-19 02:34:14,559 step [1074], lr [0.0000375], embedding loss [ 0.7299], quantization loss [ 0.0378],  0.50 sec/batch.
2022-10-19 02:34:16,682 step [1075], lr [0.0000375], embedding loss [ 0.7164], quantization loss [ 0.0331],  0.51 sec/batch.
2022-10-19 02:34:18,797 step [1076], lr [0.0000375], embedding loss [ 0.7214], quantization loss [ 0.0392],  0.51 sec/batch.
2022-10-19 02:34:20,907 step [1077], lr [0.0000375], embedding loss [ 0.7142], quantization loss [ 0.0395],  0.51 sec/batch.
2022-10-19 02:34:22,937 step [1078], lr [0.0000375], embedding loss [ 0.7363], quantization loss [ 0.0365],  0.50 sec/batch.
2022-10-19 02:34:25,027 step [1079], lr [0.0000375], embedding loss [ 0.7354], quantization loss [ 0.0353],  0.51 sec/batch.
2022-10-19 02:34:27,155 step [1080], lr [0.0000375], embedding loss [ 0.7181], quantization loss [ 0.0381],  0.51 sec/batch.
2022-10-19 02:34:29,281 step [1081], lr [0.0000375], embedding loss [ 0.7168], quantization loss [ 0.0408],  0.52 sec/batch.
2022-10-19 02:34:31,406 step [1082], lr [0.0000375], embedding loss [ 0.7298], quantization loss [ 0.0327],  0.51 sec/batch.
2022-10-19 02:34:33,479 step [1083], lr [0.0000375], embedding loss [ 0.7279], quantization loss [ 0.0370],  0.52 sec/batch.
2022-10-19 02:34:35,606 step [1084], lr [0.0000375], embedding loss [ 0.7292], quantization loss [ 0.0318],  0.51 sec/batch.
2022-10-19 02:34:37,718 step [1085], lr [0.0000375], embedding loss [ 0.7255], quantization loss [ 0.0345],  0.51 sec/batch.
2022-10-19 02:34:39,800 step [1086], lr [0.0000375], embedding loss [ 0.7255], quantization loss [ 0.0354],  0.50 sec/batch.
2022-10-19 02:34:41,904 step [1087], lr [0.0000375], embedding loss [ 0.7212], quantization loss [ 0.0345],  0.51 sec/batch.
2022-10-19 02:34:44,005 step [1088], lr [0.0000375], embedding loss [ 0.7172], quantization loss [ 0.0350],  0.51 sec/batch.
2022-10-19 02:34:46,108 step [1089], lr [0.0000375], embedding loss [ 0.7285], quantization loss [ 0.0374],  0.51 sec/batch.
2022-10-19 02:34:48,207 step [1090], lr [0.0000375], embedding loss [ 0.7140], quantization loss [ 0.0355],  0.51 sec/batch.
2022-10-19 02:34:50,287 step [1091], lr [0.0000375], embedding loss [ 0.7265], quantization loss [ 0.0396],  0.51 sec/batch.
2022-10-19 02:34:52,393 step [1092], lr [0.0000375], embedding loss [ 0.7129], quantization loss [ 0.0348],  0.51 sec/batch.
2022-10-19 02:34:54,557 step [1093], lr [0.0000375], embedding loss [ 0.7239], quantization loss [ 0.0320],  0.51 sec/batch.
2022-10-19 02:34:56,666 step [1094], lr [0.0000375], embedding loss [ 0.7233], quantization loss [ 0.0349],  0.51 sec/batch.
2022-10-19 02:34:58,816 step [1095], lr [0.0000375], embedding loss [ 0.7311], quantization loss [ 0.0370],  0.51 sec/batch.
2022-10-19 02:35:00,911 step [1096], lr [0.0000375], embedding loss [ 0.7175], quantization loss [ 0.0350],  0.51 sec/batch.
2022-10-19 02:35:02,982 step [1097], lr [0.0000375], embedding loss [ 0.7282], quantization loss [ 0.0342],  0.51 sec/batch.
2022-10-19 02:35:05,079 step [1098], lr [0.0000375], embedding loss [ 0.7249], quantization loss [ 0.0366],  0.51 sec/batch.
2022-10-19 02:35:07,136 step [1099], lr [0.0000375], embedding loss [ 0.7274], quantization loss [ 0.0390],  0.51 sec/batch.
2022-10-19 02:35:09,299 step [1100], lr [0.0000375], embedding loss [ 0.7279], quantization loss [ 0.0333],  0.52 sec/batch.
2022-10-19 02:35:11,395 step [1101], lr [0.0000375], embedding loss [ 0.7229], quantization loss [ 0.0357],  0.51 sec/batch.
2022-10-19 02:35:13,482 step [1102], lr [0.0000375], embedding loss [ 0.7257], quantization loss [ 0.0350],  0.51 sec/batch.
2022-10-19 02:35:15,530 step [1103], lr [0.0000375], embedding loss [ 0.7198], quantization loss [ 0.0316],  0.50 sec/batch.
2022-10-19 02:35:17,719 step [1104], lr [0.0000375], embedding loss [ 0.7324], quantization loss [ 0.0355],  0.52 sec/batch.
2022-10-19 02:35:19,810 step [1105], lr [0.0000375], embedding loss [ 0.7234], quantization loss [ 0.0385],  0.48 sec/batch.
2022-10-19 02:35:21,929 step [1106], lr [0.0000375], embedding loss [ 0.7235], quantization loss [ 0.0377],  0.52 sec/batch.
2022-10-19 02:35:24,116 step [1107], lr [0.0000375], embedding loss [ 0.7299], quantization loss [ 0.0360],  0.51 sec/batch.
2022-10-19 02:35:26,215 step [1108], lr [0.0000375], embedding loss [ 0.7215], quantization loss [ 0.0347],  0.51 sec/batch.
2022-10-19 02:35:28,337 step [1109], lr [0.0000375], embedding loss [ 0.7202], quantization loss [ 0.0308],  0.51 sec/batch.
2022-10-19 02:35:30,503 step [1110], lr [0.0000375], embedding loss [ 0.7226], quantization loss [ 0.0353],  0.52 sec/batch.
2022-10-19 02:35:32,548 step [1111], lr [0.0000375], embedding loss [ 0.7160], quantization loss [ 0.0362],  0.51 sec/batch.
2022-10-19 02:35:34,597 step [1112], lr [0.0000375], embedding loss [ 0.7128], quantization loss [ 0.0335],  0.48 sec/batch.
2022-10-19 02:35:36,689 step [1113], lr [0.0000375], embedding loss [ 0.7288], quantization loss [ 0.0374],  0.51 sec/batch.
2022-10-19 02:35:38,787 step [1114], lr [0.0000375], embedding loss [ 0.7281], quantization loss [ 0.0345],  0.51 sec/batch.
2022-10-19 02:35:40,936 step [1115], lr [0.0000375], embedding loss [ 0.7274], quantization loss [ 0.0357],  0.50 sec/batch.
2022-10-19 02:35:42,954 step [1116], lr [0.0000375], embedding loss [ 0.7266], quantization loss [ 0.0359],  0.50 sec/batch.
2022-10-19 02:35:45,121 step [1117], lr [0.0000375], embedding loss [ 0.7323], quantization loss [ 0.0371],  0.51 sec/batch.
2022-10-19 02:35:47,191 step [1118], lr [0.0000375], embedding loss [ 0.7179], quantization loss [ 0.0398],  0.51 sec/batch.
2022-10-19 02:35:49,333 step [1119], lr [0.0000375], embedding loss [ 0.7194], quantization loss [ 0.0295],  0.51 sec/batch.
2022-10-19 02:35:51,392 step [1120], lr [0.0000375], embedding loss [ 0.7271], quantization loss [ 0.0336],  0.50 sec/batch.
2022-10-19 02:35:53,527 step [1121], lr [0.0000375], embedding loss [ 0.7381], quantization loss [ 0.0372],  0.51 sec/batch.
2022-10-19 02:35:53,528 update codes and centers iter(1/1).
2022-10-19 02:35:56,663 number of update_code wrong: 0.
2022-10-19 02:36:00,024 non zero codewords: 1024.
2022-10-19 02:36:00,024 finish center update, duration: 6.50 sec.
2022-10-19 02:36:02,027 step [1122], lr [0.0000375], embedding loss [ 0.7203], quantization loss [ 0.0328],  0.52 sec/batch.
2022-10-19 02:36:04,171 step [1123], lr [0.0000375], embedding loss [ 0.7126], quantization loss [ 0.0361],  0.52 sec/batch.
2022-10-19 02:36:06,528 step [1124], lr [0.0000375], embedding loss [ 0.7235], quantization loss [ 0.0319],  0.52 sec/batch.
2022-10-19 02:36:08,704 step [1125], lr [0.0000375], embedding loss [ 0.7165], quantization loss [ 0.0354],  0.53 sec/batch.
2022-10-19 02:36:10,904 step [1126], lr [0.0000375], embedding loss [ 0.7246], quantization loss [ 0.0363],  0.52 sec/batch.
2022-10-19 02:36:13,076 step [1127], lr [0.0000375], embedding loss [ 0.7281], quantization loss [ 0.0368],  0.52 sec/batch.
2022-10-19 02:36:15,296 step [1128], lr [0.0000375], embedding loss [ 0.7210], quantization loss [ 0.0380],  0.52 sec/batch.
2022-10-19 02:36:17,600 step [1129], lr [0.0000375], embedding loss [ 0.7247], quantization loss [ 0.0327],  0.52 sec/batch.
2022-10-19 02:36:19,828 step [1130], lr [0.0000375], embedding loss [ 0.7478], quantization loss [ 0.0369],  0.52 sec/batch.
2022-10-19 02:36:22,034 step [1131], lr [0.0000375], embedding loss [ 0.7168], quantization loss [ 0.0351],  0.51 sec/batch.
2022-10-19 02:36:24,206 step [1132], lr [0.0000375], embedding loss [ 0.7243], quantization loss [ 0.0338],  0.52 sec/batch.
2022-10-19 02:36:26,448 step [1133], lr [0.0000375], embedding loss [ 0.7205], quantization loss [ 0.0377],  0.52 sec/batch.
2022-10-19 02:36:28,604 step [1134], lr [0.0000375], embedding loss [ 0.7249], quantization loss [ 0.0329],  0.52 sec/batch.
2022-10-19 02:36:30,765 step [1135], lr [0.0000375], embedding loss [ 0.7297], quantization loss [ 0.0365],  0.52 sec/batch.
2022-10-19 02:36:32,963 step [1136], lr [0.0000375], embedding loss [ 0.7202], quantization loss [ 0.0332],  0.51 sec/batch.
2022-10-19 02:36:35,184 step [1137], lr [0.0000375], embedding loss [ 0.7163], quantization loss [ 0.0387],  0.51 sec/batch.
2022-10-19 02:36:37,350 step [1138], lr [0.0000375], embedding loss [ 0.7181], quantization loss [ 0.0391],  0.51 sec/batch.
2022-10-19 02:36:39,511 step [1139], lr [0.0000375], embedding loss [ 0.7379], quantization loss [ 0.0344],  0.51 sec/batch.
2022-10-19 02:36:41,706 step [1140], lr [0.0000375], embedding loss [ 0.7267], quantization loss [ 0.0369],  0.51 sec/batch.
2022-10-19 02:36:43,841 step [1141], lr [0.0000375], embedding loss [ 0.7267], quantization loss [ 0.0361],  0.51 sec/batch.
2022-10-19 02:36:45,963 step [1142], lr [0.0000375], embedding loss [ 0.7202], quantization loss [ 0.0351],  0.51 sec/batch.
2022-10-19 02:36:48,106 step [1143], lr [0.0000375], embedding loss [ 0.7278], quantization loss [ 0.0388],  0.51 sec/batch.
2022-10-19 02:36:50,227 step [1144], lr [0.0000375], embedding loss [ 0.7239], quantization loss [ 0.0357],  0.50 sec/batch.
2022-10-19 02:36:52,315 step [1145], lr [0.0000375], embedding loss [ 0.7201], quantization loss [ 0.0360],  0.49 sec/batch.
2022-10-19 02:36:54,430 step [1146], lr [0.0000375], embedding loss [ 0.7163], quantization loss [ 0.0361],  0.51 sec/batch.
2022-10-19 02:36:56,572 step [1147], lr [0.0000375], embedding loss [ 0.7285], quantization loss [ 0.0398],  0.52 sec/batch.
2022-10-19 02:36:58,672 step [1148], lr [0.0000375], embedding loss [ 0.7201], quantization loss [ 0.0391],  0.51 sec/batch.
2022-10-19 02:37:00,890 step [1149], lr [0.0000375], embedding loss [ 0.7344], quantization loss [ 0.0385],  0.54 sec/batch.
2022-10-19 02:37:02,986 step [1150], lr [0.0000375], embedding loss [ 0.7295], quantization loss [ 0.0344],  0.51 sec/batch.
2022-10-19 02:37:05,191 step [1151], lr [0.0000375], embedding loss [ 0.7137], quantization loss [ 0.0375],  0.50 sec/batch.
2022-10-19 02:37:07,384 step [1152], lr [0.0000375], embedding loss [ 0.7290], quantization loss [ 0.0363],  0.51 sec/batch.
2022-10-19 02:37:09,582 step [1153], lr [0.0000375], embedding loss [ 0.7216], quantization loss [ 0.0377],  0.50 sec/batch.
2022-10-19 02:37:11,751 step [1154], lr [0.0000375], embedding loss [ 0.7135], quantization loss [ 0.0360],  0.51 sec/batch.
2022-10-19 02:37:13,940 step [1155], lr [0.0000375], embedding loss [ 0.7425], quantization loss [ 0.0344],  0.51 sec/batch.
2022-10-19 02:37:16,141 step [1156], lr [0.0000375], embedding loss [ 0.7129], quantization loss [ 0.0354],  0.51 sec/batch.
2022-10-19 02:37:18,354 step [1157], lr [0.0000375], embedding loss [ 0.7058], quantization loss [ 0.0372],  0.51 sec/batch.
2022-10-19 02:37:20,491 step [1158], lr [0.0000375], embedding loss [ 0.7162], quantization loss [ 0.0383],  0.51 sec/batch.
2022-10-19 02:37:22,627 step [1159], lr [0.0000375], embedding loss [ 0.7241], quantization loss [ 0.0332],  0.51 sec/batch.
2022-10-19 02:37:24,798 step [1160], lr [0.0000375], embedding loss [ 0.7175], quantization loss [ 0.0388],  0.51 sec/batch.
2022-10-19 02:37:26,898 step [1161], lr [0.0000375], embedding loss [ 0.7301], quantization loss [ 0.0377],  0.51 sec/batch.
2022-10-19 02:37:29,060 step [1162], lr [0.0000375], embedding loss [ 0.7330], quantization loss [ 0.0395],  0.51 sec/batch.
2022-10-19 02:37:31,163 step [1163], lr [0.0000375], embedding loss [ 0.7334], quantization loss [ 0.0371],  0.51 sec/batch.
2022-10-19 02:37:33,337 step [1164], lr [0.0000375], embedding loss [ 0.7281], quantization loss [ 0.0323],  0.52 sec/batch.
2022-10-19 02:37:35,436 step [1165], lr [0.0000375], embedding loss [ 0.7137], quantization loss [ 0.0350],  0.50 sec/batch.
2022-10-19 02:37:37,662 step [1166], lr [0.0000375], embedding loss [ 0.7162], quantization loss [ 0.0336],  0.52 sec/batch.
2022-10-19 02:37:39,849 step [1167], lr [0.0000375], embedding loss [ 0.7240], quantization loss [ 0.0377],  0.51 sec/batch.
2022-10-19 02:37:42,022 step [1168], lr [0.0000375], embedding loss [ 0.7280], quantization loss [ 0.0355],  0.51 sec/batch.
2022-10-19 02:37:44,172 step [1169], lr [0.0000375], embedding loss [ 0.7282], quantization loss [ 0.0347],  0.52 sec/batch.
2022-10-19 02:37:46,331 step [1170], lr [0.0000375], embedding loss [ 0.7210], quantization loss [ 0.0349],  0.51 sec/batch.
2022-10-19 02:37:48,512 step [1171], lr [0.0000375], embedding loss [ 0.7171], quantization loss [ 0.0368],  0.51 sec/batch.
2022-10-19 02:37:50,650 step [1172], lr [0.0000375], embedding loss [ 0.7180], quantization loss [ 0.0349],  0.51 sec/batch.
2022-10-19 02:37:52,855 step [1173], lr [0.0000375], embedding loss [ 0.7152], quantization loss [ 0.0348],  0.52 sec/batch.
2022-10-19 02:37:55,032 step [1174], lr [0.0000375], embedding loss [ 0.7142], quantization loss [ 0.0324],  0.52 sec/batch.
2022-10-19 02:37:57,196 step [1175], lr [0.0000375], embedding loss [ 0.7362], quantization loss [ 0.0371],  0.51 sec/batch.
2022-10-19 02:37:59,272 step [1176], lr [0.0000375], embedding loss [ 0.7210], quantization loss [ 0.0359],  0.50 sec/batch.
2022-10-19 02:38:01,474 step [1177], lr [0.0000375], embedding loss [ 0.7203], quantization loss [ 0.0352],  0.52 sec/batch.
2022-10-19 02:38:03,575 step [1178], lr [0.0000375], embedding loss [ 0.7118], quantization loss [ 0.0368],  0.51 sec/batch.
2022-10-19 02:38:05,707 step [1179], lr [0.0000375], embedding loss [ 0.7278], quantization loss [ 0.0388],  0.51 sec/batch.
2022-10-19 02:38:07,875 step [1180], lr [0.0000375], embedding loss [ 0.7116], quantization loss [ 0.0333],  0.53 sec/batch.
2022-10-19 02:38:10,082 step [1181], lr [0.0000375], embedding loss [ 0.7294], quantization loss [ 0.0372],  0.51 sec/batch.
2022-10-19 02:38:12,207 step [1182], lr [0.0000375], embedding loss [ 0.7167], quantization loss [ 0.0360],  0.51 sec/batch.
2022-10-19 02:38:14,330 step [1183], lr [0.0000375], embedding loss [ 0.7218], quantization loss [ 0.0378],  0.51 sec/batch.
2022-10-19 02:38:16,450 step [1184], lr [0.0000375], embedding loss [ 0.7158], quantization loss [ 0.0332],  0.51 sec/batch.
2022-10-19 02:38:18,580 step [1185], lr [0.0000375], embedding loss [ 0.7315], quantization loss [ 0.0334],  0.51 sec/batch.
2022-10-19 02:38:20,694 step [1186], lr [0.0000375], embedding loss [ 0.7298], quantization loss [ 0.0354],  0.51 sec/batch.
2022-10-19 02:38:22,792 step [1187], lr [0.0000375], embedding loss [ 0.7244], quantization loss [ 0.0346],  0.51 sec/batch.
2022-10-19 02:38:24,974 step [1188], lr [0.0000375], embedding loss [ 0.7379], quantization loss [ 0.0329],  0.52 sec/batch.
2022-10-19 02:38:27,040 step [1189], lr [0.0000375], embedding loss [ 0.7183], quantization loss [ 0.0344],  0.51 sec/batch.
2022-10-19 02:38:29,225 step [1190], lr [0.0000375], embedding loss [ 0.7121], quantization loss [ 0.0333],  0.51 sec/batch.
2022-10-19 02:38:31,402 step [1191], lr [0.0000375], embedding loss [ 0.7234], quantization loss [ 0.0366],  0.51 sec/batch.
2022-10-19 02:38:33,527 step [1192], lr [0.0000375], embedding loss [ 0.7191], quantization loss [ 0.0329],  0.50 sec/batch.
2022-10-19 02:38:35,653 step [1193], lr [0.0000375], embedding loss [ 0.7221], quantization loss [ 0.0322],  0.51 sec/batch.
2022-10-19 02:38:37,780 step [1194], lr [0.0000375], embedding loss [ 0.7210], quantization loss [ 0.0359],  0.52 sec/batch.
2022-10-19 02:38:39,864 step [1195], lr [0.0000375], embedding loss [ 0.7205], quantization loss [ 0.0348],  0.50 sec/batch.
2022-10-19 02:38:41,973 step [1196], lr [0.0000375], embedding loss [ 0.7166], quantization loss [ 0.0343],  0.51 sec/batch.
2022-10-19 02:38:44,110 step [1197], lr [0.0000375], embedding loss [ 0.7190], quantization loss [ 0.0361],  0.51 sec/batch.
2022-10-19 02:38:46,205 step [1198], lr [0.0000375], embedding loss [ 0.7144], quantization loss [ 0.0360],  0.50 sec/batch.
2022-10-19 02:38:48,302 step [1199], lr [0.0000375], embedding loss [ 0.7286], quantization loss [ 0.0338],  0.51 sec/batch.
2022-10-19 02:38:50,418 step [1200], lr [0.0000375], embedding loss [ 0.7233], quantization loss [ 0.0350],  0.51 sec/batch.
2022-10-19 02:38:52,620 step [1201], lr [0.0000188], embedding loss [ 0.7259], quantization loss [ 0.0360],  0.51 sec/batch.
2022-10-19 02:38:52,620 update codes and centers iter(1/1).
2022-10-19 02:38:55,761 number of update_code wrong: 0.
2022-10-19 02:38:59,280 non zero codewords: 1024.
2022-10-19 02:38:59,281 finish center update, duration: 6.66 sec.
2022-10-19 02:39:01,341 step [1202], lr [0.0000188], embedding loss [ 0.7317], quantization loss [ 0.0365],  0.53 sec/batch.
2022-10-19 02:39:03,492 step [1203], lr [0.0000188], embedding loss [ 0.7151], quantization loss [ 0.0367],  0.52 sec/batch.
2022-10-19 02:39:05,802 step [1204], lr [0.0000188], embedding loss [ 0.7257], quantization loss [ 0.0349],  0.53 sec/batch.
2022-10-19 02:39:07,977 step [1205], lr [0.0000188], embedding loss [ 0.7230], quantization loss [ 0.0358],  0.52 sec/batch.
2022-10-19 02:39:10,121 step [1206], lr [0.0000188], embedding loss [ 0.7142], quantization loss [ 0.0368],  0.51 sec/batch.
2022-10-19 02:39:12,295 step [1207], lr [0.0000188], embedding loss [ 0.7340], quantization loss [ 0.0341],  0.52 sec/batch.
2022-10-19 02:39:14,435 step [1208], lr [0.0000188], embedding loss [ 0.7244], quantization loss [ 0.0376],  0.52 sec/batch.
2022-10-19 02:39:16,641 step [1209], lr [0.0000188], embedding loss [ 0.7130], quantization loss [ 0.0347],  0.52 sec/batch.
2022-10-19 02:39:18,885 step [1210], lr [0.0000188], embedding loss [ 0.7352], quantization loss [ 0.0370],  0.51 sec/batch.
2022-10-19 02:39:21,061 step [1211], lr [0.0000188], embedding loss [ 0.7239], quantization loss [ 0.0344],  0.52 sec/batch.
2022-10-19 02:39:23,230 step [1212], lr [0.0000188], embedding loss [ 0.7157], quantization loss [ 0.0387],  0.52 sec/batch.
2022-10-19 02:39:25,375 step [1213], lr [0.0000188], embedding loss [ 0.7157], quantization loss [ 0.0339],  0.51 sec/batch.
2022-10-19 02:39:27,618 step [1214], lr [0.0000188], embedding loss [ 0.7185], quantization loss [ 0.0405],  0.54 sec/batch.
2022-10-19 02:39:29,798 step [1215], lr [0.0000188], embedding loss [ 0.7307], quantization loss [ 0.0355],  0.50 sec/batch.
2022-10-19 02:39:31,829 step [1216], lr [0.0000188], embedding loss [ 0.7238], quantization loss [ 0.0347],  0.51 sec/batch.
2022-10-19 02:39:33,960 step [1217], lr [0.0000188], embedding loss [ 0.7310], quantization loss [ 0.0357],  0.52 sec/batch.
2022-10-19 02:39:36,119 step [1218], lr [0.0000188], embedding loss [ 0.7098], quantization loss [ 0.0351],  0.52 sec/batch.
2022-10-19 02:39:38,323 step [1219], lr [0.0000188], embedding loss [ 0.7136], quantization loss [ 0.0333],  0.51 sec/batch.
2022-10-19 02:39:40,553 step [1220], lr [0.0000188], embedding loss [ 0.7180], quantization loss [ 0.0381],  0.52 sec/batch.
2022-10-19 02:39:42,731 step [1221], lr [0.0000188], embedding loss [ 0.7102], quantization loss [ 0.0335],  0.52 sec/batch.
2022-10-19 02:39:44,906 step [1222], lr [0.0000188], embedding loss [ 0.7341], quantization loss [ 0.0372],  0.52 sec/batch.
2022-10-19 02:39:47,036 step [1223], lr [0.0000188], embedding loss [ 0.7208], quantization loss [ 0.0320],  0.51 sec/batch.
2022-10-19 02:39:49,296 step [1224], lr [0.0000188], embedding loss [ 0.7163], quantization loss [ 0.0344],  0.52 sec/batch.
2022-10-19 02:39:51,505 step [1225], lr [0.0000188], embedding loss [ 0.7250], quantization loss [ 0.0368],  0.52 sec/batch.
2022-10-19 02:39:53,735 step [1226], lr [0.0000188], embedding loss [ 0.7200], quantization loss [ 0.0309],  0.50 sec/batch.
2022-10-19 02:39:55,863 step [1227], lr [0.0000188], embedding loss [ 0.7107], quantization loss [ 0.0354],  0.51 sec/batch.
2022-10-19 02:39:58,016 step [1228], lr [0.0000188], embedding loss [ 0.7298], quantization loss [ 0.0349],  0.52 sec/batch.
2022-10-19 02:40:00,216 step [1229], lr [0.0000188], embedding loss [ 0.7210], quantization loss [ 0.0322],  0.51 sec/batch.
2022-10-19 02:40:02,333 step [1230], lr [0.0000188], embedding loss [ 0.7285], quantization loss [ 0.0368],  0.50 sec/batch.
2022-10-19 02:40:04,458 step [1231], lr [0.0000188], embedding loss [ 0.7277], quantization loss [ 0.0365],  0.51 sec/batch.
2022-10-19 02:40:06,537 step [1232], lr [0.0000188], embedding loss [ 0.7181], quantization loss [ 0.0357],  0.50 sec/batch.
2022-10-19 02:40:08,666 step [1233], lr [0.0000188], embedding loss [ 0.7211], quantization loss [ 0.0326],  0.51 sec/batch.
2022-10-19 02:40:10,773 step [1234], lr [0.0000188], embedding loss [ 0.7221], quantization loss [ 0.0350],  0.50 sec/batch.
2022-10-19 02:40:12,883 step [1235], lr [0.0000188], embedding loss [ 0.7227], quantization loss [ 0.0353],  0.50 sec/batch.
2022-10-19 02:40:15,031 step [1236], lr [0.0000188], embedding loss [ 0.7326], quantization loss [ 0.0345],  0.50 sec/batch.
2022-10-19 02:40:17,170 step [1237], lr [0.0000188], embedding loss [ 0.7191], quantization loss [ 0.0366],  0.50 sec/batch.
2022-10-19 02:40:19,295 step [1238], lr [0.0000188], embedding loss [ 0.7102], quantization loss [ 0.0392],  0.51 sec/batch.
2022-10-19 02:40:21,399 step [1239], lr [0.0000188], embedding loss [ 0.7278], quantization loss [ 0.0314],  0.50 sec/batch.
2022-10-19 02:40:23,418 step [1240], lr [0.0000188], embedding loss [ 0.7132], quantization loss [ 0.0350],  0.50 sec/batch.
2022-10-19 02:40:25,556 step [1241], lr [0.0000188], embedding loss [ 0.7159], quantization loss [ 0.0351],  0.51 sec/batch.
2022-10-19 02:40:27,678 step [1242], lr [0.0000188], embedding loss [ 0.7242], quantization loss [ 0.0316],  0.51 sec/batch.
2022-10-19 02:40:29,874 step [1243], lr [0.0000188], embedding loss [ 0.7144], quantization loss [ 0.0339],  0.52 sec/batch.
2022-10-19 02:40:32,033 step [1244], lr [0.0000188], embedding loss [ 0.7108], quantization loss [ 0.0346],  0.51 sec/batch.
2022-10-19 02:40:34,182 step [1245], lr [0.0000188], embedding loss [ 0.7136], quantization loss [ 0.0377],  0.50 sec/batch.
2022-10-19 02:40:36,315 step [1246], lr [0.0000188], embedding loss [ 0.7032], quantization loss [ 0.0323],  0.52 sec/batch.
2022-10-19 02:40:38,457 step [1247], lr [0.0000188], embedding loss [ 0.7127], quantization loss [ 0.0302],  0.51 sec/batch.
2022-10-19 02:40:40,591 step [1248], lr [0.0000188], embedding loss [ 0.7121], quantization loss [ 0.0311],  0.51 sec/batch.
2022-10-19 02:40:42,758 step [1249], lr [0.0000188], embedding loss [ 0.7300], quantization loss [ 0.0328],  0.50 sec/batch.
2022-10-19 02:40:44,900 step [1250], lr [0.0000188], embedding loss [ 0.7200], quantization loss [ 0.0326],  0.51 sec/batch.
2022-10-19 02:40:47,080 step [1251], lr [0.0000188], embedding loss [ 0.7158], quantization loss [ 0.0340],  0.51 sec/batch.
2022-10-19 02:40:49,244 step [1252], lr [0.0000188], embedding loss [ 0.7317], quantization loss [ 0.0331],  0.51 sec/batch.
2022-10-19 02:40:51,376 step [1253], lr [0.0000188], embedding loss [ 0.7192], quantization loss [ 0.0374],  0.51 sec/batch.
2022-10-19 02:40:53,522 step [1254], lr [0.0000188], embedding loss [ 0.7152], quantization loss [ 0.0348],  0.52 sec/batch.
2022-10-19 02:40:55,607 step [1255], lr [0.0000188], embedding loss [ 0.7276], quantization loss [ 0.0331],  0.52 sec/batch.
2022-10-19 02:40:57,726 step [1256], lr [0.0000188], embedding loss [ 0.7113], quantization loss [ 0.0301],  0.51 sec/batch.
2022-10-19 02:40:59,878 step [1257], lr [0.0000188], embedding loss [ 0.7369], quantization loss [ 0.0377],  0.52 sec/batch.
2022-10-19 02:41:02,044 step [1258], lr [0.0000188], embedding loss [ 0.7139], quantization loss [ 0.0344],  0.50 sec/batch.
2022-10-19 02:41:04,192 step [1259], lr [0.0000188], embedding loss [ 0.7177], quantization loss [ 0.0316],  0.51 sec/batch.
2022-10-19 02:41:06,339 step [1260], lr [0.0000188], embedding loss [ 0.7296], quantization loss [ 0.0349],  0.52 sec/batch.
2022-10-19 02:41:08,455 step [1261], lr [0.0000188], embedding loss [ 0.7217], quantization loss [ 0.0383],  0.51 sec/batch.
2022-10-19 02:41:10,634 step [1262], lr [0.0000188], embedding loss [ 0.7278], quantization loss [ 0.0350],  0.51 sec/batch.
2022-10-19 02:41:12,847 step [1263], lr [0.0000188], embedding loss [ 0.7222], quantization loss [ 0.0363],  0.52 sec/batch.
2022-10-19 02:41:14,986 step [1264], lr [0.0000188], embedding loss [ 0.7154], quantization loss [ 0.0344],  0.50 sec/batch.
2022-10-19 02:41:17,139 step [1265], lr [0.0000188], embedding loss [ 0.7133], quantization loss [ 0.0349],  0.50 sec/batch.
2022-10-19 02:41:19,260 step [1266], lr [0.0000188], embedding loss [ 0.7357], quantization loss [ 0.0344],  0.51 sec/batch.
2022-10-19 02:41:21,408 step [1267], lr [0.0000188], embedding loss [ 0.7244], quantization loss [ 0.0353],  0.52 sec/batch.
2022-10-19 02:41:23,528 step [1268], lr [0.0000188], embedding loss [ 0.7095], quantization loss [ 0.0327],  0.51 sec/batch.
2022-10-19 02:41:25,753 step [1269], lr [0.0000188], embedding loss [ 0.7264], quantization loss [ 0.0321],  0.53 sec/batch.
2022-10-19 02:41:27,928 step [1270], lr [0.0000188], embedding loss [ 0.7016], quantization loss [ 0.0330],  0.51 sec/batch.
2022-10-19 02:41:30,108 step [1271], lr [0.0000188], embedding loss [ 0.7246], quantization loss [ 0.0361],  0.52 sec/batch.
2022-10-19 02:41:32,206 step [1272], lr [0.0000188], embedding loss [ 0.7320], quantization loss [ 0.0362],  0.51 sec/batch.
2022-10-19 02:41:34,454 step [1273], lr [0.0000188], embedding loss [ 0.7264], quantization loss [ 0.0338],  0.52 sec/batch.
2022-10-19 02:41:36,640 step [1274], lr [0.0000188], embedding loss [ 0.7274], quantization loss [ 0.0350],  0.52 sec/batch.
2022-10-19 02:41:38,808 step [1275], lr [0.0000188], embedding loss [ 0.7195], quantization loss [ 0.0333],  0.54 sec/batch.
2022-10-19 02:41:41,042 step [1276], lr [0.0000188], embedding loss [ 0.7412], quantization loss [ 0.0376],  0.51 sec/batch.
2022-10-19 02:41:43,214 step [1277], lr [0.0000188], embedding loss [ 0.7291], quantization loss [ 0.0309],  0.51 sec/batch.
2022-10-19 02:41:45,496 step [1278], lr [0.0000188], embedding loss [ 0.7264], quantization loss [ 0.0315],  0.52 sec/batch.
2022-10-19 02:41:47,690 step [1279], lr [0.0000188], embedding loss [ 0.7249], quantization loss [ 0.0303],  0.51 sec/batch.
2022-10-19 02:41:49,864 step [1280], lr [0.0000188], embedding loss [ 0.7202], quantization loss [ 0.0348],  0.51 sec/batch.
2022-10-19 02:41:52,035 step [1281], lr [0.0000188], embedding loss [ 0.7316], quantization loss [ 0.0295],  0.52 sec/batch.
2022-10-19 02:41:52,035 update codes and centers iter(1/1).
2022-10-19 02:41:55,192 number of update_code wrong: 0.
2022-10-19 02:41:58,635 non zero codewords: 1024.
2022-10-19 02:41:58,636 finish center update, duration: 6.60 sec.
2022-10-19 02:42:00,728 step [1282], lr [0.0000188], embedding loss [ 0.7093], quantization loss [ 0.0340],  0.52 sec/batch.
2022-10-19 02:42:02,855 step [1283], lr [0.0000188], embedding loss [ 0.7270], quantization loss [ 0.0347],  0.51 sec/batch.
2022-10-19 02:42:05,203 step [1284], lr [0.0000188], embedding loss [ 0.7187], quantization loss [ 0.0327],  0.53 sec/batch.
2022-10-19 02:42:07,338 step [1285], lr [0.0000188], embedding loss [ 0.7159], quantization loss [ 0.0330],  0.51 sec/batch.
2022-10-19 02:42:09,508 step [1286], lr [0.0000188], embedding loss [ 0.7256], quantization loss [ 0.0371],  0.51 sec/batch.
2022-10-19 02:42:11,612 step [1287], lr [0.0000188], embedding loss [ 0.7205], quantization loss [ 0.0327],  0.51 sec/batch.
2022-10-19 02:42:13,795 step [1288], lr [0.0000188], embedding loss [ 0.7179], quantization loss [ 0.0324],  0.51 sec/batch.
2022-10-19 02:42:15,986 step [1289], lr [0.0000188], embedding loss [ 0.7097], quantization loss [ 0.0316],  0.51 sec/batch.
2022-10-19 02:42:18,085 step [1290], lr [0.0000188], embedding loss [ 0.7284], quantization loss [ 0.0337],  0.51 sec/batch.
2022-10-19 02:42:20,167 step [1291], lr [0.0000188], embedding loss [ 0.7262], quantization loss [ 0.0316],  0.51 sec/batch.
2022-10-19 02:42:22,346 step [1292], lr [0.0000188], embedding loss [ 0.7306], quantization loss [ 0.0349],  0.51 sec/batch.
2022-10-19 02:42:24,543 step [1293], lr [0.0000188], embedding loss [ 0.7189], quantization loss [ 0.0330],  0.51 sec/batch.
2022-10-19 02:42:26,677 step [1294], lr [0.0000188], embedding loss [ 0.7199], quantization loss [ 0.0361],  0.51 sec/batch.
2022-10-19 02:42:28,822 step [1295], lr [0.0000188], embedding loss [ 0.7268], quantization loss [ 0.0342],  0.51 sec/batch.
2022-10-19 02:42:30,900 step [1296], lr [0.0000188], embedding loss [ 0.7220], quantization loss [ 0.0353],  0.50 sec/batch.
2022-10-19 02:42:33,105 step [1297], lr [0.0000188], embedding loss [ 0.7312], quantization loss [ 0.0342],  0.52 sec/batch.
2022-10-19 02:42:35,249 step [1298], lr [0.0000188], embedding loss [ 0.7160], quantization loss [ 0.0324],  0.51 sec/batch.
2022-10-19 02:42:37,411 step [1299], lr [0.0000188], embedding loss [ 0.7138], quantization loss [ 0.0321],  0.51 sec/batch.
2022-10-19 02:42:39,514 step [1300], lr [0.0000188], embedding loss [ 0.7195], quantization loss [ 0.0323],  0.51 sec/batch.
2022-10-19 02:42:41,655 step [1301], lr [0.0000188], embedding loss [ 0.7221], quantization loss [ 0.0317],  0.51 sec/batch.
2022-10-19 02:42:43,821 step [1302], lr [0.0000188], embedding loss [ 0.7325], quantization loss [ 0.0333],  0.50 sec/batch.
2022-10-19 02:42:46,068 step [1303], lr [0.0000188], embedding loss [ 0.7208], quantization loss [ 0.0351],  0.51 sec/batch.
2022-10-19 02:42:48,241 step [1304], lr [0.0000188], embedding loss [ 0.7186], quantization loss [ 0.0360],  0.51 sec/batch.
2022-10-19 02:42:50,359 step [1305], lr [0.0000188], embedding loss [ 0.7255], quantization loss [ 0.0342],  0.50 sec/batch.
2022-10-19 02:42:52,526 step [1306], lr [0.0000188], embedding loss [ 0.7146], quantization loss [ 0.0368],  0.51 sec/batch.
2022-10-19 02:42:54,689 step [1307], lr [0.0000188], embedding loss [ 0.7223], quantization loss [ 0.0319],  0.50 sec/batch.
2022-10-19 02:42:56,809 step [1308], lr [0.0000188], embedding loss [ 0.7287], quantization loss [ 0.0345],  0.51 sec/batch.
2022-10-19 02:42:58,945 step [1309], lr [0.0000188], embedding loss [ 0.7162], quantization loss [ 0.0337],  0.51 sec/batch.
2022-10-19 02:43:01,030 step [1310], lr [0.0000188], embedding loss [ 0.7223], quantization loss [ 0.0349],  0.50 sec/batch.
2022-10-19 02:43:03,279 step [1311], lr [0.0000188], embedding loss [ 0.7154], quantization loss [ 0.0367],  0.57 sec/batch.
2022-10-19 02:43:05,655 step [1312], lr [0.0000188], embedding loss [ 0.7219], quantization loss [ 0.0327],  0.54 sec/batch.
2022-10-19 02:43:07,955 step [1313], lr [0.0000188], embedding loss [ 0.7183], quantization loss [ 0.0337],  0.57 sec/batch.
2022-10-19 02:43:10,393 step [1314], lr [0.0000188], embedding loss [ 0.7175], quantization loss [ 0.0361],  0.53 sec/batch.
2022-10-19 02:43:12,666 step [1315], lr [0.0000188], embedding loss [ 0.7157], quantization loss [ 0.0308],  0.52 sec/batch.
2022-10-19 02:43:14,842 step [1316], lr [0.0000188], embedding loss [ 0.7268], quantization loss [ 0.0387],  0.55 sec/batch.
2022-10-19 02:43:17,015 step [1317], lr [0.0000188], embedding loss [ 0.7207], quantization loss [ 0.0316],  0.49 sec/batch.
2022-10-19 02:43:19,198 step [1318], lr [0.0000188], embedding loss [ 0.7133], quantization loss [ 0.0357],  0.49 sec/batch.
2022-10-19 02:43:21,402 step [1319], lr [0.0000188], embedding loss [ 0.7211], quantization loss [ 0.0335],  0.51 sec/batch.
2022-10-19 02:43:23,626 step [1320], lr [0.0000188], embedding loss [ 0.7206], quantization loss [ 0.0326],  0.51 sec/batch.
2022-10-19 02:43:25,815 step [1321], lr [0.0000188], embedding loss [ 0.7318], quantization loss [ 0.0362],  0.51 sec/batch.
2022-10-19 02:43:28,013 step [1322], lr [0.0000188], embedding loss [ 0.7233], quantization loss [ 0.0358],  0.50 sec/batch.
2022-10-19 02:43:30,147 step [1323], lr [0.0000188], embedding loss [ 0.7216], quantization loss [ 0.0309],  0.50 sec/batch.
2022-10-19 02:43:32,257 step [1324], lr [0.0000188], embedding loss [ 0.7189], quantization loss [ 0.0331],  0.49 sec/batch.
2022-10-19 02:43:34,459 step [1325], lr [0.0000188], embedding loss [ 0.7150], quantization loss [ 0.0320],  0.51 sec/batch.
2022-10-19 02:43:36,635 step [1326], lr [0.0000188], embedding loss [ 0.7149], quantization loss [ 0.0322],  0.50 sec/batch.
2022-10-19 02:43:38,784 step [1327], lr [0.0000188], embedding loss [ 0.7217], quantization loss [ 0.0343],  0.50 sec/batch.
2022-10-19 02:43:41,003 step [1328], lr [0.0000188], embedding loss [ 0.7249], quantization loss [ 0.0357],  0.52 sec/batch.
2022-10-19 02:43:43,176 step [1329], lr [0.0000188], embedding loss [ 0.7175], quantization loss [ 0.0324],  0.50 sec/batch.
2022-10-19 02:43:45,344 step [1330], lr [0.0000188], embedding loss [ 0.7330], quantization loss [ 0.0311],  0.50 sec/batch.
2022-10-19 02:43:47,589 step [1331], lr [0.0000188], embedding loss [ 0.7059], quantization loss [ 0.0346],  0.52 sec/batch.
2022-10-19 02:43:49,847 step [1332], lr [0.0000188], embedding loss [ 0.7128], quantization loss [ 0.0346],  0.52 sec/batch.
2022-10-19 02:43:52,064 step [1333], lr [0.0000188], embedding loss [ 0.7171], quantization loss [ 0.0315],  0.51 sec/batch.
2022-10-19 02:43:54,213 step [1334], lr [0.0000188], embedding loss [ 0.7216], quantization loss [ 0.0333],  0.51 sec/batch.
2022-10-19 02:43:56,368 step [1335], lr [0.0000188], embedding loss [ 0.7156], quantization loss [ 0.0328],  0.51 sec/batch.
2022-10-19 02:43:58,540 step [1336], lr [0.0000188], embedding loss [ 0.7282], quantization loss [ 0.0345],  0.50 sec/batch.
2022-10-19 02:44:00,659 step [1337], lr [0.0000188], embedding loss [ 0.7235], quantization loss [ 0.0321],  0.51 sec/batch.
2022-10-19 02:44:02,843 step [1338], lr [0.0000188], embedding loss [ 0.7201], quantization loss [ 0.0353],  0.51 sec/batch.
2022-10-19 02:44:05,017 step [1339], lr [0.0000188], embedding loss [ 0.7041], quantization loss [ 0.0327],  0.51 sec/batch.
2022-10-19 02:44:07,138 step [1340], lr [0.0000188], embedding loss [ 0.7171], quantization loss [ 0.0335],  0.48 sec/batch.
2022-10-19 02:44:09,249 step [1341], lr [0.0000188], embedding loss [ 0.7171], quantization loss [ 0.0310],  0.49 sec/batch.
2022-10-19 02:44:11,350 step [1342], lr [0.0000188], embedding loss [ 0.7139], quantization loss [ 0.0339],  0.50 sec/batch.
2022-10-19 02:44:13,814 step [1343], lr [0.0000188], embedding loss [ 0.7244], quantization loss [ 0.0333],  0.57 sec/batch.
2022-10-19 02:44:16,105 step [1344], lr [0.0000188], embedding loss [ 0.7192], quantization loss [ 0.0335],  0.52 sec/batch.
2022-10-19 02:44:18,391 step [1345], lr [0.0000188], embedding loss [ 0.7280], quantization loss [ 0.0296],  0.56 sec/batch.
2022-10-19 02:44:20,619 step [1346], lr [0.0000188], embedding loss [ 0.7284], quantization loss [ 0.0352],  0.53 sec/batch.
2022-10-19 02:44:22,972 step [1347], lr [0.0000188], embedding loss [ 0.7195], quantization loss [ 0.0316],  0.54 sec/batch.
2022-10-19 02:44:25,288 step [1348], lr [0.0000188], embedding loss [ 0.7261], quantization loss [ 0.0318],  0.53 sec/batch.
2022-10-19 02:44:27,495 step [1349], lr [0.0000188], embedding loss [ 0.7244], quantization loss [ 0.0334],  0.52 sec/batch.
2022-10-19 02:44:29,623 step [1350], lr [0.0000188], embedding loss [ 0.7210], quantization loss [ 0.0339],  0.53 sec/batch.
2022-10-19 02:44:31,862 step [1351], lr [0.0000188], embedding loss [ 0.7087], quantization loss [ 0.0304],  0.54 sec/batch.
2022-10-19 02:44:34,115 step [1352], lr [0.0000188], embedding loss [ 0.7204], quantization loss [ 0.0304],  0.51 sec/batch.
2022-10-19 02:44:36,373 step [1353], lr [0.0000188], embedding loss [ 0.7127], quantization loss [ 0.0351],  0.53 sec/batch.
2022-10-19 02:44:38,487 step [1354], lr [0.0000188], embedding loss [ 0.7235], quantization loss [ 0.0308],  0.51 sec/batch.
2022-10-19 02:44:40,658 step [1355], lr [0.0000188], embedding loss [ 0.7263], quantization loss [ 0.0334],  0.51 sec/batch.
2022-10-19 02:44:42,898 step [1356], lr [0.0000188], embedding loss [ 0.7291], quantization loss [ 0.0314],  0.54 sec/batch.
2022-10-19 02:44:45,134 step [1357], lr [0.0000188], embedding loss [ 0.7195], quantization loss [ 0.0339],  0.52 sec/batch.
2022-10-19 02:44:47,224 step [1358], lr [0.0000188], embedding loss [ 0.7243], quantization loss [ 0.0326],  0.50 sec/batch.
2022-10-19 02:44:49,360 step [1359], lr [0.0000188], embedding loss [ 0.7201], quantization loss [ 0.0336],  0.51 sec/batch.
2022-10-19 02:44:51,562 step [1360], lr [0.0000188], embedding loss [ 0.7156], quantization loss [ 0.0327],  0.54 sec/batch.
2022-10-19 02:44:53,846 step [1361], lr [0.0000188], embedding loss [ 0.7157], quantization loss [ 0.0313],  0.52 sec/batch.
2022-10-19 02:44:53,846 update codes and centers iter(1/1).
2022-10-19 02:44:57,104 number of update_code wrong: 0.
2022-10-19 02:45:00,481 non zero codewords: 1024.
2022-10-19 02:45:00,481 finish center update, duration: 6.63 sec.
2022-10-19 02:45:02,606 step [1362], lr [0.0000188], embedding loss [ 0.7227], quantization loss [ 0.0346],  0.52 sec/batch.
2022-10-19 02:45:04,853 step [1363], lr [0.0000188], embedding loss [ 0.7156], quantization loss [ 0.0336],  0.52 sec/batch.
2022-10-19 02:45:07,074 step [1364], lr [0.0000188], embedding loss [ 0.7152], quantization loss [ 0.0354],  0.52 sec/batch.
2022-10-19 02:45:09,256 step [1365], lr [0.0000188], embedding loss [ 0.7291], quantization loss [ 0.0355],  0.51 sec/batch.
2022-10-19 02:45:11,412 step [1366], lr [0.0000188], embedding loss [ 0.7165], quantization loss [ 0.0332],  0.51 sec/batch.
2022-10-19 02:45:13,620 step [1367], lr [0.0000188], embedding loss [ 0.7051], quantization loss [ 0.0342],  0.52 sec/batch.
2022-10-19 02:45:15,990 step [1368], lr [0.0000188], embedding loss [ 0.7360], quantization loss [ 0.0318],  0.53 sec/batch.
2022-10-19 02:45:18,227 step [1369], lr [0.0000188], embedding loss [ 0.7190], quantization loss [ 0.0318],  0.52 sec/batch.
2022-10-19 02:45:20,419 step [1370], lr [0.0000188], embedding loss [ 0.7217], quantization loss [ 0.0345],  0.52 sec/batch.
2022-10-19 02:45:22,622 step [1371], lr [0.0000188], embedding loss [ 0.7222], quantization loss [ 0.0334],  0.52 sec/batch.
2022-10-19 02:45:24,974 step [1372], lr [0.0000188], embedding loss [ 0.7278], quantization loss [ 0.0345],  0.51 sec/batch.
2022-10-19 02:45:27,105 step [1373], lr [0.0000188], embedding loss [ 0.7107], quantization loss [ 0.0328],  0.50 sec/batch.
2022-10-19 02:45:29,239 step [1374], lr [0.0000188], embedding loss [ 0.7303], quantization loss [ 0.0353],  0.50 sec/batch.
2022-10-19 02:45:31,352 step [1375], lr [0.0000188], embedding loss [ 0.7166], quantization loss [ 0.0314],  0.50 sec/batch.
2022-10-19 02:45:33,475 step [1376], lr [0.0000188], embedding loss [ 0.7244], quantization loss [ 0.0361],  0.51 sec/batch.
2022-10-19 02:45:35,587 step [1377], lr [0.0000188], embedding loss [ 0.7057], quantization loss [ 0.0331],  0.54 sec/batch.
2022-10-19 02:45:37,779 step [1378], lr [0.0000188], embedding loss [ 0.7245], quantization loss [ 0.0327],  0.52 sec/batch.
2022-10-19 02:45:40,077 step [1379], lr [0.0000188], embedding loss [ 0.7025], quantization loss [ 0.0348],  0.52 sec/batch.
2022-10-19 02:45:42,376 step [1380], lr [0.0000188], embedding loss [ 0.7317], quantization loss [ 0.0325],  0.52 sec/batch.
2022-10-19 02:45:44,568 step [1381], lr [0.0000188], embedding loss [ 0.7158], quantization loss [ 0.0298],  0.50 sec/batch.
2022-10-19 02:45:46,864 step [1382], lr [0.0000188], embedding loss [ 0.7403], quantization loss [ 0.0330],  0.52 sec/batch.
2022-10-19 02:45:49,083 step [1383], lr [0.0000188], embedding loss [ 0.7156], quantization loss [ 0.0374],  0.52 sec/batch.
2022-10-19 02:45:51,291 step [1384], lr [0.0000188], embedding loss [ 0.7177], quantization loss [ 0.0340],  0.51 sec/batch.
2022-10-19 02:45:53,486 step [1385], lr [0.0000188], embedding loss [ 0.7184], quantization loss [ 0.0321],  0.51 sec/batch.
2022-10-19 02:45:55,690 step [1386], lr [0.0000188], embedding loss [ 0.7191], quantization loss [ 0.0322],  0.52 sec/batch.
2022-10-19 02:45:57,930 step [1387], lr [0.0000188], embedding loss [ 0.7193], quantization loss [ 0.0301],  0.52 sec/batch.
2022-10-19 02:46:00,210 step [1388], lr [0.0000188], embedding loss [ 0.7282], quantization loss [ 0.0318],  0.52 sec/batch.
2022-10-19 02:46:02,454 step [1389], lr [0.0000188], embedding loss [ 0.7244], quantization loss [ 0.0316],  0.51 sec/batch.
2022-10-19 02:46:04,698 step [1390], lr [0.0000188], embedding loss [ 0.7213], quantization loss [ 0.0335],  0.52 sec/batch.
2022-10-19 02:46:06,975 step [1391], lr [0.0000188], embedding loss [ 0.7293], quantization loss [ 0.0311],  0.52 sec/batch.
2022-10-19 02:46:09,229 step [1392], lr [0.0000188], embedding loss [ 0.7221], quantization loss [ 0.0344],  0.52 sec/batch.
2022-10-19 02:46:11,382 step [1393], lr [0.0000188], embedding loss [ 0.7170], quantization loss [ 0.0326],  0.51 sec/batch.
2022-10-19 02:46:13,591 step [1394], lr [0.0000188], embedding loss [ 0.7165], quantization loss [ 0.0302],  0.52 sec/batch.
2022-10-19 02:46:15,832 step [1395], lr [0.0000188], embedding loss [ 0.7107], quantization loss [ 0.0354],  0.52 sec/batch.
2022-10-19 02:46:18,067 step [1396], lr [0.0000188], embedding loss [ 0.7079], quantization loss [ 0.0323],  0.51 sec/batch.
2022-10-19 02:46:20,266 step [1397], lr [0.0000188], embedding loss [ 0.7172], quantization loss [ 0.0343],  0.50 sec/batch.
2022-10-19 02:46:22,452 step [1398], lr [0.0000188], embedding loss [ 0.7228], quantization loss [ 0.0326],  0.51 sec/batch.
2022-10-19 02:46:24,679 step [1399], lr [0.0000188], embedding loss [ 0.7217], quantization loss [ 0.0360],  0.52 sec/batch.
2022-10-19 02:46:26,869 step [1400], lr [0.0000188], embedding loss [ 0.7197], quantization loss [ 0.0351],  0.51 sec/batch.
2022-10-19 02:46:29,051 step [1401], lr [0.0000188], embedding loss [ 0.7177], quantization loss [ 0.0329],  0.51 sec/batch.
2022-10-19 02:46:31,207 step [1402], lr [0.0000188], embedding loss [ 0.7319], quantization loss [ 0.0313],  0.50 sec/batch.
2022-10-19 02:46:33,424 step [1403], lr [0.0000188], embedding loss [ 0.7215], quantization loss [ 0.0359],  0.52 sec/batch.
2022-10-19 02:46:35,667 step [1404], lr [0.0000188], embedding loss [ 0.7221], quantization loss [ 0.0319],  0.52 sec/batch.
2022-10-19 02:46:37,886 step [1405], lr [0.0000188], embedding loss [ 0.7262], quantization loss [ 0.0340],  0.51 sec/batch.
2022-10-19 02:46:40,119 step [1406], lr [0.0000188], embedding loss [ 0.7256], quantization loss [ 0.0325],  0.51 sec/batch.
2022-10-19 02:46:42,332 step [1407], lr [0.0000188], embedding loss [ 0.7196], quantization loss [ 0.0356],  0.51 sec/batch.
2022-10-19 02:46:44,498 step [1408], lr [0.0000188], embedding loss [ 0.7091], quantization loss [ 0.0344],  0.50 sec/batch.
2022-10-19 02:46:46,720 step [1409], lr [0.0000188], embedding loss [ 0.7213], quantization loss [ 0.0331],  0.51 sec/batch.
2022-10-19 02:46:48,945 step [1410], lr [0.0000188], embedding loss [ 0.7202], quantization loss [ 0.0350],  0.52 sec/batch.
2022-10-19 02:46:51,122 step [1411], lr [0.0000188], embedding loss [ 0.7092], quantization loss [ 0.0321],  0.51 sec/batch.
2022-10-19 02:46:53,358 step [1412], lr [0.0000188], embedding loss [ 0.7266], quantization loss [ 0.0340],  0.51 sec/batch.
2022-10-19 02:46:55,535 step [1413], lr [0.0000188], embedding loss [ 0.7135], quantization loss [ 0.0348],  0.51 sec/batch.
2022-10-19 02:46:57,700 step [1414], lr [0.0000188], embedding loss [ 0.6997], quantization loss [ 0.0336],  0.50 sec/batch.
2022-10-19 02:46:59,853 step [1415], lr [0.0000188], embedding loss [ 0.7102], quantization loss [ 0.0311],  0.50 sec/batch.
2022-10-19 02:47:02,092 step [1416], lr [0.0000188], embedding loss [ 0.7248], quantization loss [ 0.0344],  0.52 sec/batch.
2022-10-19 02:47:04,258 step [1417], lr [0.0000188], embedding loss [ 0.7096], quantization loss [ 0.0314],  0.49 sec/batch.
2022-10-19 02:47:06,383 step [1418], lr [0.0000188], embedding loss [ 0.7305], quantization loss [ 0.0338],  0.49 sec/batch.
2022-10-19 02:47:08,591 step [1419], lr [0.0000188], embedding loss [ 0.7164], quantization loss [ 0.0332],  0.51 sec/batch.
2022-10-19 02:47:10,765 step [1420], lr [0.0000188], embedding loss [ 0.7152], quantization loss [ 0.0333],  0.51 sec/batch.
2022-10-19 02:47:12,920 step [1421], lr [0.0000188], embedding loss [ 0.7187], quantization loss [ 0.0336],  0.50 sec/batch.
2022-10-19 02:47:15,042 step [1422], lr [0.0000188], embedding loss [ 0.7203], quantization loss [ 0.0338],  0.51 sec/batch.
2022-10-19 02:47:17,164 step [1423], lr [0.0000188], embedding loss [ 0.7185], quantization loss [ 0.0332],  0.50 sec/batch.
2022-10-19 02:47:19,335 step [1424], lr [0.0000188], embedding loss [ 0.7206], quantization loss [ 0.0330],  0.51 sec/batch.
2022-10-19 02:47:21,582 step [1425], lr [0.0000188], embedding loss [ 0.7313], quantization loss [ 0.0304],  0.52 sec/batch.
2022-10-19 02:47:23,782 step [1426], lr [0.0000188], embedding loss [ 0.7231], quantization loss [ 0.0338],  0.52 sec/batch.
2022-10-19 02:47:25,989 step [1427], lr [0.0000188], embedding loss [ 0.7228], quantization loss [ 0.0312],  0.50 sec/batch.
2022-10-19 02:47:28,254 step [1428], lr [0.0000188], embedding loss [ 0.7255], quantization loss [ 0.0351],  0.51 sec/batch.
2022-10-19 02:47:30,490 step [1429], lr [0.0000188], embedding loss [ 0.7240], quantization loss [ 0.0326],  0.51 sec/batch.
2022-10-19 02:47:32,759 step [1430], lr [0.0000188], embedding loss [ 0.7207], quantization loss [ 0.0312],  0.51 sec/batch.
2022-10-19 02:47:34,945 step [1431], lr [0.0000188], embedding loss [ 0.7102], quantization loss [ 0.0346],  0.50 sec/batch.
2022-10-19 02:47:37,088 step [1432], lr [0.0000188], embedding loss [ 0.7225], quantization loss [ 0.0364],  0.49 sec/batch.
2022-10-19 02:47:39,262 step [1433], lr [0.0000188], embedding loss [ 0.7219], quantization loss [ 0.0300],  0.51 sec/batch.
2022-10-19 02:47:41,411 step [1434], lr [0.0000188], embedding loss [ 0.7162], quantization loss [ 0.0319],  0.51 sec/batch.
2022-10-19 02:47:43,597 step [1435], lr [0.0000188], embedding loss [ 0.7162], quantization loss [ 0.0351],  0.52 sec/batch.
2022-10-19 02:47:45,831 step [1436], lr [0.0000188], embedding loss [ 0.7170], quantization loss [ 0.0343],  0.51 sec/batch.
2022-10-19 02:47:48,049 step [1437], lr [0.0000188], embedding loss [ 0.7130], quantization loss [ 0.0316],  0.51 sec/batch.
2022-10-19 02:47:50,269 step [1438], lr [0.0000188], embedding loss [ 0.7195], quantization loss [ 0.0310],  0.51 sec/batch.
2022-10-19 02:47:52,418 step [1439], lr [0.0000188], embedding loss [ 0.7255], quantization loss [ 0.0337],  0.51 sec/batch.
2022-10-19 02:47:54,557 step [1440], lr [0.0000188], embedding loss [ 0.7157], quantization loss [ 0.0347],  0.51 sec/batch.
2022-10-19 02:47:56,735 step [1441], lr [0.0000188], embedding loss [ 0.7219], quantization loss [ 0.0327],  0.51 sec/batch.
2022-10-19 02:47:56,735 update codes and centers iter(1/1).
2022-10-19 02:47:59,955 number of update_code wrong: 0.
2022-10-19 02:48:03,455 non zero codewords: 1024.
2022-10-19 02:48:03,456 finish center update, duration: 6.72 sec.
2022-10-19 02:48:05,688 step [1442], lr [0.0000188], embedding loss [ 0.7229], quantization loss [ 0.0323],  0.53 sec/batch.
2022-10-19 02:48:07,933 step [1443], lr [0.0000188], embedding loss [ 0.7144], quantization loss [ 0.0361],  0.52 sec/batch.
2022-10-19 02:48:10,373 step [1444], lr [0.0000188], embedding loss [ 0.7167], quantization loss [ 0.0362],  0.51 sec/batch.
2022-10-19 02:48:12,582 step [1445], lr [0.0000188], embedding loss [ 0.7279], quantization loss [ 0.0301],  0.50 sec/batch.
2022-10-19 02:48:14,823 step [1446], lr [0.0000188], embedding loss [ 0.7132], quantization loss [ 0.0330],  0.51 sec/batch.
2022-10-19 02:48:17,089 step [1447], lr [0.0000188], embedding loss [ 0.7224], quantization loss [ 0.0321],  0.52 sec/batch.
2022-10-19 02:48:19,288 step [1448], lr [0.0000188], embedding loss [ 0.7116], quantization loss [ 0.0316],  0.52 sec/batch.
2022-10-19 02:48:21,701 step [1449], lr [0.0000188], embedding loss [ 0.7141], quantization loss [ 0.0324],  0.53 sec/batch.
2022-10-19 02:48:24,054 step [1450], lr [0.0000188], embedding loss [ 0.7255], quantization loss [ 0.0338],  0.51 sec/batch.
2022-10-19 02:48:26,229 step [1451], lr [0.0000188], embedding loss [ 0.7121], quantization loss [ 0.0316],  0.51 sec/batch.
2022-10-19 02:48:28,471 step [1452], lr [0.0000188], embedding loss [ 0.7278], quantization loss [ 0.0315],  0.51 sec/batch.
2022-10-19 02:48:30,707 step [1453], lr [0.0000188], embedding loss [ 0.7159], quantization loss [ 0.0342],  0.51 sec/batch.
2022-10-19 02:48:32,900 step [1454], lr [0.0000188], embedding loss [ 0.7112], quantization loss [ 0.0333],  0.52 sec/batch.
2022-10-19 02:48:35,093 step [1455], lr [0.0000188], embedding loss [ 0.7320], quantization loss [ 0.0329],  0.51 sec/batch.
2022-10-19 02:48:37,367 step [1456], lr [0.0000188], embedding loss [ 0.7328], quantization loss [ 0.0340],  0.51 sec/batch.
2022-10-19 02:48:39,589 step [1457], lr [0.0000188], embedding loss [ 0.7235], quantization loss [ 0.0312],  0.55 sec/batch.
2022-10-19 02:48:41,903 step [1458], lr [0.0000188], embedding loss [ 0.7286], quantization loss [ 0.0314],  0.52 sec/batch.
2022-10-19 02:48:44,256 step [1459], lr [0.0000188], embedding loss [ 0.7249], quantization loss [ 0.0315],  0.54 sec/batch.
2022-10-19 02:48:46,453 step [1460], lr [0.0000188], embedding loss [ 0.7149], quantization loss [ 0.0353],  0.52 sec/batch.
2022-10-19 02:48:48,749 step [1461], lr [0.0000188], embedding loss [ 0.7252], quantization loss [ 0.0311],  0.53 sec/batch.
2022-10-19 02:48:51,125 step [1462], lr [0.0000188], embedding loss [ 0.7067], quantization loss [ 0.0355],  0.52 sec/batch.
2022-10-19 02:48:53,400 step [1463], lr [0.0000188], embedding loss [ 0.7087], quantization loss [ 0.0340],  0.55 sec/batch.
2022-10-19 02:48:55,553 step [1464], lr [0.0000188], embedding loss [ 0.7150], quantization loss [ 0.0332],  0.56 sec/batch.
2022-10-19 02:48:58,008 step [1465], lr [0.0000188], embedding loss [ 0.7167], quantization loss [ 0.0339],  0.56 sec/batch.
2022-10-19 02:49:00,218 step [1466], lr [0.0000188], embedding loss [ 0.7170], quantization loss [ 0.0309],  0.51 sec/batch.
2022-10-19 02:49:02,351 step [1467], lr [0.0000188], embedding loss [ 0.7237], quantization loss [ 0.0334],  0.51 sec/batch.
2022-10-19 02:49:04,833 step [1468], lr [0.0000188], embedding loss [ 0.7155], quantization loss [ 0.0298],  0.58 sec/batch.
2022-10-19 02:49:07,046 step [1469], lr [0.0000188], embedding loss [ 0.7236], quantization loss [ 0.0294],  0.51 sec/batch.
2022-10-19 02:49:09,360 step [1470], lr [0.0000188], embedding loss [ 0.7138], quantization loss [ 0.0336],  0.52 sec/batch.
2022-10-19 02:49:11,658 step [1471], lr [0.0000188], embedding loss [ 0.7190], quantization loss [ 0.0345],  0.55 sec/batch.
2022-10-19 02:49:13,929 step [1472], lr [0.0000188], embedding loss [ 0.7164], quantization loss [ 0.0349],  0.53 sec/batch.
2022-10-19 02:49:16,111 step [1473], lr [0.0000188], embedding loss [ 0.7225], quantization loss [ 0.0334],  0.52 sec/batch.
2022-10-19 02:49:18,324 step [1474], lr [0.0000188], embedding loss [ 0.7288], quantization loss [ 0.0359],  0.51 sec/batch.
2022-10-19 02:49:20,407 step [1475], lr [0.0000188], embedding loss [ 0.7111], quantization loss [ 0.0363],  0.51 sec/batch.
2022-10-19 02:49:22,597 step [1476], lr [0.0000188], embedding loss [ 0.7117], quantization loss [ 0.0314],  0.52 sec/batch.
2022-10-19 02:49:25,041 step [1477], lr [0.0000188], embedding loss [ 0.7283], quantization loss [ 0.0325],  0.56 sec/batch.
2022-10-19 02:49:27,292 step [1478], lr [0.0000188], embedding loss [ 0.7205], quantization loss [ 0.0338],  0.50 sec/batch.
2022-10-19 02:49:29,527 step [1479], lr [0.0000188], embedding loss [ 0.7080], quantization loss [ 0.0342],  0.52 sec/batch.
2022-10-19 02:49:31,679 step [1480], lr [0.0000188], embedding loss [ 0.7181], quantization loss [ 0.0350],  0.51 sec/batch.
2022-10-19 02:49:33,868 step [1481], lr [0.0000188], embedding loss [ 0.7167], quantization loss [ 0.0315],  0.50 sec/batch.
2022-10-19 02:49:36,099 step [1482], lr [0.0000188], embedding loss [ 0.7141], quantization loss [ 0.0372],  0.49 sec/batch.
2022-10-19 02:49:38,244 step [1483], lr [0.0000188], embedding loss [ 0.7238], quantization loss [ 0.0336],  0.51 sec/batch.
2022-10-19 02:49:40,585 step [1484], lr [0.0000188], embedding loss [ 0.7223], quantization loss [ 0.0332],  0.52 sec/batch.
2022-10-19 02:49:42,912 step [1485], lr [0.0000188], embedding loss [ 0.7200], quantization loss [ 0.0298],  0.51 sec/batch.
2022-10-19 02:49:45,201 step [1486], lr [0.0000188], embedding loss [ 0.7196], quantization loss [ 0.0320],  0.52 sec/batch.
2022-10-19 02:49:47,347 step [1487], lr [0.0000188], embedding loss [ 0.7156], quantization loss [ 0.0293],  0.51 sec/batch.
2022-10-19 02:49:49,694 step [1488], lr [0.0000188], embedding loss [ 0.7199], quantization loss [ 0.0330],  0.52 sec/batch.
2022-10-19 02:49:51,961 step [1489], lr [0.0000188], embedding loss [ 0.7201], quantization loss [ 0.0322],  0.52 sec/batch.
2022-10-19 02:49:54,192 step [1490], lr [0.0000188], embedding loss [ 0.7134], quantization loss [ 0.0323],  0.51 sec/batch.
2022-10-19 02:49:56,388 step [1491], lr [0.0000188], embedding loss [ 0.7178], quantization loss [ 0.0315],  0.52 sec/batch.
2022-10-19 02:49:58,729 step [1492], lr [0.0000188], embedding loss [ 0.7172], quantization loss [ 0.0313],  0.51 sec/batch.
2022-10-19 02:50:01,095 step [1493], lr [0.0000188], embedding loss [ 0.7164], quantization loss [ 0.0321],  0.52 sec/batch.
2022-10-19 02:50:03,296 step [1494], lr [0.0000188], embedding loss [ 0.7072], quantization loss [ 0.0334],  0.51 sec/batch.
2022-10-19 02:50:05,556 step [1495], lr [0.0000188], embedding loss [ 0.7228], quantization loss [ 0.0347],  0.52 sec/batch.
2022-10-19 02:50:07,826 step [1496], lr [0.0000188], embedding loss [ 0.7143], quantization loss [ 0.0330],  0.51 sec/batch.
2022-10-19 02:50:10,150 step [1497], lr [0.0000188], embedding loss [ 0.7130], quantization loss [ 0.0313],  0.52 sec/batch.
2022-10-19 02:50:12,356 step [1498], lr [0.0000188], embedding loss [ 0.7145], quantization loss [ 0.0341],  0.52 sec/batch.
2022-10-19 02:50:14,592 step [1499], lr [0.0000188], embedding loss [ 0.7203], quantization loss [ 0.0371],  0.51 sec/batch.
2022-10-19 02:50:16,855 step [1500], lr [0.0000188], embedding loss [ 0.7117], quantization loss [ 0.0306],  0.52 sec/batch.
2022-10-19 02:50:16,856 finish training iterations and begin saving model.
2022-10-19 02:50:24,892 finish model saving.
2022-10-19 02:50:24,892 finish training, model saved under ./checkpoints/nuswide_WSDQH_nbits=32_adaMargin_gamma=1_lambda=0.0001_221018.npy.
2022-10-19 04:06:12,708 prepare dataset.
2022-10-19 04:06:20,352 prepare data loader.
2022-10-19 04:06:20,352 Initializing DataLoader.
2022-10-19 04:06:20,356 DataLoader already.
2022-10-19 04:06:20,356 Initializing DataLoader.
2022-10-19 04:06:20,360 DataLoader already.
2022-10-19 04:06:20,360 prepare model.
2022-10-19 04:06:20,535 Number of semantic embeddings: 928.
2022-10-19 04:06:39,341 begin validation.
2022-10-19 04:07:10,629 finish query feature extraction, duration: 31.29 sec.
2022-10-19 04:22:37,884 finish database feature extraction, duration: 927.25 sec.
2022-10-19 04:22:37,884 compute quantization codes for query.
2022-10-19 04:22:40,644 number of update_code wrong: 0.
2022-10-19 04:22:40,644 finish query encoding, duration: 2.76 sec.
2022-10-19 04:22:40,644 compute quantization codes for database.
2022-10-19 04:23:36,725 number of update_code wrong: 0.
2022-10-19 04:23:36,725 finish database encoding, duration: 56.08 sec.
2022-10-19 04:23:36,725 save retrieval information: codes, features, reconstructions of queries and database.
2022-10-19 04:23:38,275 begin to calculate MAP@5000.
2022-10-19 04:23:38,275 begin to calculate AQD mAP@5000.
2022-10-19 04:24:14,691 AQD mAP@5000 = [0.7295], duration: 36.42 sec.
2022-10-19 04:24:14,692 begin to calculate SQD mAP@5000.
2022-10-19 04:24:51,920 SQD mAP@5000 = [0.7282], duration: 37.23 sec.
2022-10-19 04:24:51,920 begin to calculate feats mAP@5000.
2022-10-19 04:25:30,167 feats mAP@5000 = [0.7311], duration: 38.25 sec.
2022-10-19 04:25:30,168 finish validation.
2022-10-20 11:38:00,505 prepare dataset.
2022-10-20 11:38:08,160 prepare data loader.
2022-10-20 11:38:08,160 Initializing DataLoader.
2022-10-20 11:38:08,165 DataLoader already.
2022-10-20 11:38:08,165 Initializing DataLoader.
2022-10-20 11:38:08,169 DataLoader already.
2022-10-20 11:38:08,169 prepare model.
2022-10-20 11:38:08,372 Number of semantic embeddings: 928.
2022-10-20 11:38:26,523 begin validation.
2022-10-20 11:38:57,700 finish query feature extraction, duration: 31.18 sec.
2022-10-20 11:54:20,272 finish database feature extraction, duration: 922.57 sec.
2022-10-20 11:54:20,272 compute quantization codes for query.
2022-10-20 11:54:22,907 number of update_code wrong: 0.
2022-10-20 11:54:22,907 finish query encoding, duration: 2.64 sec.
2022-10-20 11:54:22,908 compute quantization codes for database.
2022-10-20 11:55:19,747 number of update_code wrong: 0.
2022-10-20 11:55:19,747 finish database encoding, duration: 56.84 sec.
2022-10-20 11:55:19,747 save retrieval information: codes, features, reconstructions of queries and database.
2022-10-20 11:55:21,172 begin to calculate MAP@5000.
2022-10-20 11:55:21,172 begin to calculate AQD mAP@5000.
2022-10-20 11:55:45,143 AQD mAP@5000 = [0.7292], duration: 23.97 sec.
2022-10-20 11:55:45,143 begin to calculate SQD mAP@5000.
2022-10-20 11:56:09,558 SQD mAP@5000 = [0.7276], duration: 24.41 sec.
2022-10-20 11:56:09,558 begin to calculate feats mAP@5000.
2022-10-20 11:56:33,395 feats mAP@5000 = [0.7312], duration: 23.84 sec.
2022-10-20 11:56:33,396 finish validation.
