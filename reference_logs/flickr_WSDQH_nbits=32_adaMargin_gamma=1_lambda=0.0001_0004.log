2022-10-20 21:24:52,159 prepare dataset.
2022-10-20 21:25:07,151 prepare data loader.
2022-10-20 21:25:07,152 Initializing DataLoader.
2022-10-20 21:25:07,156 DataLoader already.
2022-10-20 21:25:07,156 prepare model.
2022-10-20 21:25:07,385 Number of semantic embeddings: 1178.
2022-10-20 21:25:14,927 From /data/wangjinpeng/anaconda3/envs/py37torch/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where.
2022-10-20 21:25:27,917 begin training.
2022-10-20 21:25:48,203 step [   1], lr [0.0003000], embedding loss [ 0.8954], quantization loss [ 0.0000], 17.57 sec/batch.
2022-10-20 21:25:51,489 step [   2], lr [0.0003000], embedding loss [ 0.8759], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:25:54,608 step [   3], lr [0.0003000], embedding loss [ 0.8597], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-20 21:25:57,849 step [   4], lr [0.0003000], embedding loss [ 0.8474], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:26:01,083 step [   5], lr [0.0003000], embedding loss [ 0.8512], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:26:04,367 step [   6], lr [0.0003000], embedding loss [ 0.8467], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:26:07,664 step [   7], lr [0.0003000], embedding loss [ 0.8414], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:26:10,932 step [   8], lr [0.0003000], embedding loss [ 0.8491], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 21:26:14,217 step [   9], lr [0.0003000], embedding loss [ 0.8445], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:26:17,407 step [  10], lr [0.0003000], embedding loss [ 0.8474], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:26:20,716 step [  11], lr [0.0003000], embedding loss [ 0.8450], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:26:24,044 step [  12], lr [0.0003000], embedding loss [ 0.8366], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:26:27,329 step [  13], lr [0.0003000], embedding loss [ 0.8385], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:26:30,692 step [  14], lr [0.0003000], embedding loss [ 0.8473], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:26:34,013 step [  15], lr [0.0003000], embedding loss [ 0.8416], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:26:37,295 step [  16], lr [0.0003000], embedding loss [ 0.8364], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 21:26:40,490 step [  17], lr [0.0003000], embedding loss [ 0.8400], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:26:43,755 step [  18], lr [0.0003000], embedding loss [ 0.8428], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:26:47,099 step [  19], lr [0.0003000], embedding loss [ 0.8365], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:26:50,462 step [  20], lr [0.0003000], embedding loss [ 0.8402], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:26:53,802 step [  21], lr [0.0003000], embedding loss [ 0.8380], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:26:57,096 step [  22], lr [0.0003000], embedding loss [ 0.8445], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 21:27:00,390 step [  23], lr [0.0003000], embedding loss [ 0.8383], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-20 21:27:03,664 step [  24], lr [0.0003000], embedding loss [ 0.8410], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:27:06,981 step [  25], lr [0.0003000], embedding loss [ 0.8379], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:27:10,336 step [  26], lr [0.0003000], embedding loss [ 0.8400], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:27:13,747 step [  27], lr [0.0003000], embedding loss [ 0.8336], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-20 21:27:17,123 step [  28], lr [0.0003000], embedding loss [ 0.8393], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:27:20,426 step [  29], lr [0.0003000], embedding loss [ 0.8328], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:27:23,750 step [  30], lr [0.0003000], embedding loss [ 0.8438], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:27:27,035 step [  31], lr [0.0003000], embedding loss [ 0.8408], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:27:30,382 step [  32], lr [0.0003000], embedding loss [ 0.8362], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:27:33,743 step [  33], lr [0.0003000], embedding loss [ 0.8402], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:27:37,074 step [  34], lr [0.0003000], embedding loss [ 0.8381], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 21:27:40,427 step [  35], lr [0.0003000], embedding loss [ 0.8346], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:27:43,749 step [  36], lr [0.0003000], embedding loss [ 0.8330], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:27:47,024 step [  37], lr [0.0003000], embedding loss [ 0.8334], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:27:50,334 step [  38], lr [0.0003000], embedding loss [ 0.8393], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-20 21:27:53,663 step [  39], lr [0.0003000], embedding loss [ 0.8412], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:27:56,948 step [  40], lr [0.0003000], embedding loss [ 0.8379], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-20 21:28:00,230 step [  41], lr [0.0003000], embedding loss [ 0.8304], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-20 21:28:03,530 step [  42], lr [0.0003000], embedding loss [ 0.8370], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:28:06,838 step [  43], lr [0.0003000], embedding loss [ 0.8365], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:28:10,157 step [  44], lr [0.0003000], embedding loss [ 0.8352], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:28:13,570 step [  45], lr [0.0003000], embedding loss [ 0.8356], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:28:16,921 step [  46], lr [0.0003000], embedding loss [ 0.8278], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:28:20,278 step [  47], lr [0.0003000], embedding loss [ 0.8318], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:28:23,608 step [  48], lr [0.0003000], embedding loss [ 0.8320], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:28:26,899 step [  49], lr [0.0003000], embedding loss [ 0.8349], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:28:30,220 step [  50], lr [0.0003000], embedding loss [ 0.8349], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:28:33,531 step [  51], lr [0.0003000], embedding loss [ 0.8300], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-20 21:28:36,816 step [  52], lr [0.0003000], embedding loss [ 0.8307], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-20 21:28:40,104 step [  53], lr [0.0003000], embedding loss [ 0.8254], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:28:43,365 step [  54], lr [0.0003000], embedding loss [ 0.8255], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-20 21:28:46,678 step [  55], lr [0.0003000], embedding loss [ 0.8294], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:28:50,059 step [  56], lr [0.0003000], embedding loss [ 0.8327], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:28:53,347 step [  57], lr [0.0003000], embedding loss [ 0.8328], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-20 21:28:56,641 step [  58], lr [0.0003000], embedding loss [ 0.8357], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-20 21:28:59,891 step [  59], lr [0.0003000], embedding loss [ 0.8201], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:29:03,199 step [  60], lr [0.0003000], embedding loss [ 0.8299], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:29:06,468 step [  61], lr [0.0003000], embedding loss [ 0.8326], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:29:09,736 step [  62], lr [0.0003000], embedding loss [ 0.8243], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:29:13,127 step [  63], lr [0.0003000], embedding loss [ 0.8379], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:29:16,460 step [  64], lr [0.0003000], embedding loss [ 0.8266], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:29:19,829 step [  65], lr [0.0003000], embedding loss [ 0.8330], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:29:23,153 step [  66], lr [0.0003000], embedding loss [ 0.8234], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:29:26,479 step [  67], lr [0.0003000], embedding loss [ 0.8283], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:29:29,817 step [  68], lr [0.0003000], embedding loss [ 0.8258], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:29:33,166 step [  69], lr [0.0003000], embedding loss [ 0.8410], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:29:36,463 step [  70], lr [0.0003000], embedding loss [ 0.8399], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:29:39,743 step [  71], lr [0.0003000], embedding loss [ 0.8295], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-20 21:29:43,090 step [  72], lr [0.0003000], embedding loss [ 0.8268], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:29:45,002 step [  73], lr [0.0003000], embedding loss [ 0.8387], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:29:46,964 step [  74], lr [0.0003000], embedding loss [ 0.8278], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:29:48,909 step [  75], lr [0.0003000], embedding loss [ 0.8386], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:29:50,817 step [  76], lr [0.0003000], embedding loss [ 0.8254], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:29:52,692 step [  77], lr [0.0003000], embedding loss [ 0.8330], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:29:54,584 step [  78], lr [0.0003000], embedding loss [ 0.8311], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:29:56,500 step [  79], lr [0.0003000], embedding loss [ 0.8323], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:29:58,386 step [  80], lr [0.0003000], embedding loss [ 0.8338], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:30:00,258 step [  81], lr [0.0003000], embedding loss [ 0.8297], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:30:02,144 step [  82], lr [0.0003000], embedding loss [ 0.8278], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:30:04,062 step [  83], lr [0.0003000], embedding loss [ 0.8253], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:30:05,988 step [  84], lr [0.0003000], embedding loss [ 0.8174], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:30:07,894 step [  85], lr [0.0003000], embedding loss [ 0.8323], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:30:09,772 step [  86], lr [0.0003000], embedding loss [ 0.8246], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:30:11,691 step [  87], lr [0.0003000], embedding loss [ 0.8263], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:30:13,569 step [  88], lr [0.0003000], embedding loss [ 0.8249], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:30:15,491 step [  89], lr [0.0003000], embedding loss [ 0.8288], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:30:17,387 step [  90], lr [0.0003000], embedding loss [ 0.8206], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:30:19,251 step [  91], lr [0.0003000], embedding loss [ 0.8251], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:30:21,131 step [  92], lr [0.0003000], embedding loss [ 0.8321], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:30:23,058 step [  93], lr [0.0003000], embedding loss [ 0.8243], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:30:24,928 step [  94], lr [0.0003000], embedding loss [ 0.8338], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:30:26,850 step [  95], lr [0.0003000], embedding loss [ 0.8240], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:30:28,735 step [  96], lr [0.0003000], embedding loss [ 0.8158], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:30:30,653 step [  97], lr [0.0003000], embedding loss [ 0.8269], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:30:32,530 step [  98], lr [0.0003000], embedding loss [ 0.8321], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-20 21:30:34,400 step [  99], lr [0.0003000], embedding loss [ 0.8157], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:30:36,347 step [ 100], lr [0.0003000], embedding loss [ 0.8244], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:30:38,246 step [ 101], lr [0.0003000], embedding loss [ 0.8311], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:30:40,119 step [ 102], lr [0.0003000], embedding loss [ 0.8258], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:30:41,975 step [ 103], lr [0.0003000], embedding loss [ 0.8261], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:30:43,845 step [ 104], lr [0.0003000], embedding loss [ 0.8254], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:30:45,763 step [ 105], lr [0.0003000], embedding loss [ 0.8305], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:30:47,669 step [ 106], lr [0.0003000], embedding loss [ 0.8262], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:30:49,543 step [ 107], lr [0.0003000], embedding loss [ 0.8365], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:30:51,463 step [ 108], lr [0.0003000], embedding loss [ 0.8276], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:30:53,368 step [ 109], lr [0.0003000], embedding loss [ 0.8251], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:30:55,279 step [ 110], lr [0.0003000], embedding loss [ 0.8279], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:30:57,187 step [ 111], lr [0.0003000], embedding loss [ 0.8266], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:30:59,099 step [ 112], lr [0.0003000], embedding loss [ 0.8280], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:31:00,966 step [ 113], lr [0.0003000], embedding loss [ 0.8325], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:31:02,834 step [ 114], lr [0.0003000], embedding loss [ 0.8246], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-20 21:31:04,687 step [ 115], lr [0.0003000], embedding loss [ 0.8291], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:31:06,579 step [ 116], lr [0.0003000], embedding loss [ 0.8264], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:31:08,458 step [ 117], lr [0.0003000], embedding loss [ 0.8285], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:31:10,357 step [ 118], lr [0.0003000], embedding loss [ 0.8130], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:31:12,271 step [ 119], lr [0.0003000], embedding loss [ 0.8265], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:31:14,192 step [ 120], lr [0.0003000], embedding loss [ 0.8266], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:31:16,060 step [ 121], lr [0.0003000], embedding loss [ 0.8242], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:31:17,979 step [ 122], lr [0.0003000], embedding loss [ 0.8347], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:31:19,908 step [ 123], lr [0.0003000], embedding loss [ 0.8241], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:31:21,843 step [ 124], lr [0.0003000], embedding loss [ 0.8303], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:31:23,780 step [ 125], lr [0.0003000], embedding loss [ 0.8144], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:31:25,661 step [ 126], lr [0.0003000], embedding loss [ 0.8224], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:31:27,593 step [ 127], lr [0.0003000], embedding loss [ 0.8208], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:31:29,512 step [ 128], lr [0.0003000], embedding loss [ 0.8230], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:31:31,387 step [ 129], lr [0.0003000], embedding loss [ 0.8222], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:31:33,300 step [ 130], lr [0.0003000], embedding loss [ 0.8160], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:31:35,180 step [ 131], lr [0.0003000], embedding loss [ 0.8252], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:31:37,107 step [ 132], lr [0.0003000], embedding loss [ 0.8188], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:31:39,004 step [ 133], lr [0.0003000], embedding loss [ 0.8233], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:31:40,913 step [ 134], lr [0.0003000], embedding loss [ 0.8219], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:31:42,814 step [ 135], lr [0.0003000], embedding loss [ 0.8217], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:31:44,765 step [ 136], lr [0.0003000], embedding loss [ 0.8225], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 21:31:46,663 step [ 137], lr [0.0003000], embedding loss [ 0.8265], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:31:48,575 step [ 138], lr [0.0003000], embedding loss [ 0.8272], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-20 21:31:50,435 step [ 139], lr [0.0003000], embedding loss [ 0.8160], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:31:52,343 step [ 140], lr [0.0003000], embedding loss [ 0.8208], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:31:54,287 step [ 141], lr [0.0003000], embedding loss [ 0.8232], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:31:56,195 step [ 142], lr [0.0003000], embedding loss [ 0.8261], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 21:31:58,088 step [ 143], lr [0.0003000], embedding loss [ 0.8229], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:31:59,965 step [ 144], lr [0.0003000], embedding loss [ 0.8259], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:32:01,858 step [ 145], lr [0.0003000], embedding loss [ 0.8162], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:32:03,748 step [ 146], lr [0.0003000], embedding loss [ 0.8193], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:32:05,615 step [ 147], lr [0.0003000], embedding loss [ 0.8261], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 21:32:05,616 initialize centers iter(1/1).
2022-10-20 21:32:14,925 finish center initialization, duration: 9.31 sec.
2022-10-20 21:32:14,925 update codes and centers iter(1/1).
2022-10-20 21:32:23,531 number of update_code wrong: 72.
2022-10-20 21:32:27,515 non zero codewords: 1024.
2022-10-20 21:32:27,516 finish center update, duration: 12.59 sec.
2022-10-20 21:32:29,401 step [ 148], lr [0.0003000], embedding loss [ 0.8166], quantization loss [ 0.1661],  0.55 sec/batch.
2022-10-20 21:32:31,300 step [ 149], lr [0.0003000], embedding loss [ 0.8219], quantization loss [ 0.2684],  0.56 sec/batch.
2022-10-20 21:32:33,191 step [ 150], lr [0.0003000], embedding loss [ 0.8396], quantization loss [ 0.2147],  0.55 sec/batch.
2022-10-20 21:32:35,090 step [ 151], lr [0.0003000], embedding loss [ 0.8324], quantization loss [ 0.1803],  0.55 sec/batch.
2022-10-20 21:32:36,980 step [ 152], lr [0.0003000], embedding loss [ 0.8212], quantization loss [ 0.1459],  0.55 sec/batch.
2022-10-20 21:32:38,845 step [ 153], lr [0.0003000], embedding loss [ 0.8293], quantization loss [ 0.1576],  0.55 sec/batch.
2022-10-20 21:32:40,784 step [ 154], lr [0.0003000], embedding loss [ 0.8351], quantization loss [ 0.1621],  0.56 sec/batch.
2022-10-20 21:32:42,668 step [ 155], lr [0.0003000], embedding loss [ 0.8283], quantization loss [ 0.1293],  0.55 sec/batch.
2022-10-20 21:32:44,606 step [ 156], lr [0.0003000], embedding loss [ 0.8286], quantization loss [ 0.1195],  0.56 sec/batch.
2022-10-20 21:32:46,507 step [ 157], lr [0.0003000], embedding loss [ 0.8210], quantization loss [ 0.1262],  0.55 sec/batch.
2022-10-20 21:32:48,443 step [ 158], lr [0.0003000], embedding loss [ 0.8166], quantization loss [ 0.1178],  0.55 sec/batch.
2022-10-20 21:32:50,359 step [ 159], lr [0.0003000], embedding loss [ 0.8298], quantization loss [ 0.1135],  0.55 sec/batch.
2022-10-20 21:32:52,229 step [ 160], lr [0.0003000], embedding loss [ 0.8199], quantization loss [ 0.1176],  0.55 sec/batch.
2022-10-20 21:32:54,147 step [ 161], lr [0.0003000], embedding loss [ 0.8275], quantization loss [ 0.1261],  0.55 sec/batch.
2022-10-20 21:32:56,027 step [ 162], lr [0.0003000], embedding loss [ 0.8258], quantization loss [ 0.1242],  0.54 sec/batch.
2022-10-20 21:32:57,880 step [ 163], lr [0.0003000], embedding loss [ 0.8300], quantization loss [ 0.1254],  0.55 sec/batch.
2022-10-20 21:32:59,766 step [ 164], lr [0.0003000], embedding loss [ 0.8255], quantization loss [ 0.1129],  0.55 sec/batch.
2022-10-20 21:33:01,636 step [ 165], lr [0.0003000], embedding loss [ 0.8229], quantization loss [ 0.1103],  0.54 sec/batch.
2022-10-20 21:33:03,520 step [ 166], lr [0.0003000], embedding loss [ 0.8280], quantization loss [ 0.0995],  0.54 sec/batch.
2022-10-20 21:33:05,397 step [ 167], lr [0.0003000], embedding loss [ 0.8267], quantization loss [ 0.0981],  0.56 sec/batch.
2022-10-20 21:33:07,336 step [ 168], lr [0.0003000], embedding loss [ 0.8260], quantization loss [ 0.1033],  0.55 sec/batch.
2022-10-20 21:33:09,223 step [ 169], lr [0.0003000], embedding loss [ 0.8238], quantization loss [ 0.1124],  0.55 sec/batch.
2022-10-20 21:33:11,133 step [ 170], lr [0.0003000], embedding loss [ 0.8287], quantization loss [ 0.1027],  0.55 sec/batch.
2022-10-20 21:33:13,072 step [ 171], lr [0.0003000], embedding loss [ 0.8251], quantization loss [ 0.1022],  0.55 sec/batch.
2022-10-20 21:33:14,968 step [ 172], lr [0.0003000], embedding loss [ 0.8210], quantization loss [ 0.1071],  0.56 sec/batch.
2022-10-20 21:33:16,862 step [ 173], lr [0.0003000], embedding loss [ 0.8126], quantization loss [ 0.1086],  0.55 sec/batch.
2022-10-20 21:33:18,778 step [ 174], lr [0.0003000], embedding loss [ 0.8317], quantization loss [ 0.1033],  0.56 sec/batch.
2022-10-20 21:33:20,671 step [ 175], lr [0.0003000], embedding loss [ 0.8130], quantization loss [ 0.0993],  0.54 sec/batch.
2022-10-20 21:33:22,549 step [ 176], lr [0.0003000], embedding loss [ 0.8286], quantization loss [ 0.0892],  0.55 sec/batch.
2022-10-20 21:33:24,464 step [ 177], lr [0.0003000], embedding loss [ 0.8263], quantization loss [ 0.0900],  0.56 sec/batch.
2022-10-20 21:33:26,386 step [ 178], lr [0.0003000], embedding loss [ 0.8208], quantization loss [ 0.0870],  0.55 sec/batch.
2022-10-20 21:33:28,291 step [ 179], lr [0.0003000], embedding loss [ 0.8243], quantization loss [ 0.0905],  0.55 sec/batch.
2022-10-20 21:33:30,197 step [ 180], lr [0.0003000], embedding loss [ 0.8224], quantization loss [ 0.0892],  0.56 sec/batch.
2022-10-20 21:33:32,098 step [ 181], lr [0.0003000], embedding loss [ 0.8228], quantization loss [ 0.0988],  0.55 sec/batch.
2022-10-20 21:33:34,007 step [ 182], lr [0.0003000], embedding loss [ 0.8242], quantization loss [ 0.0963],  0.56 sec/batch.
2022-10-20 21:33:35,898 step [ 183], lr [0.0003000], embedding loss [ 0.8257], quantization loss [ 0.0859],  0.55 sec/batch.
2022-10-20 21:33:37,801 step [ 184], lr [0.0003000], embedding loss [ 0.8236], quantization loss [ 0.0941],  0.55 sec/batch.
2022-10-20 21:33:39,715 step [ 185], lr [0.0003000], embedding loss [ 0.8268], quantization loss [ 0.0905],  0.56 sec/batch.
2022-10-20 21:33:41,654 step [ 186], lr [0.0003000], embedding loss [ 0.8230], quantization loss [ 0.0987],  0.58 sec/batch.
2022-10-20 21:33:43,593 step [ 187], lr [0.0003000], embedding loss [ 0.8202], quantization loss [ 0.0966],  0.55 sec/batch.
2022-10-20 21:33:45,512 step [ 188], lr [0.0003000], embedding loss [ 0.8324], quantization loss [ 0.1041],  0.56 sec/batch.
2022-10-20 21:33:47,396 step [ 189], lr [0.0003000], embedding loss [ 0.8179], quantization loss [ 0.0868],  0.55 sec/batch.
2022-10-20 21:33:49,340 step [ 190], lr [0.0003000], embedding loss [ 0.8201], quantization loss [ 0.0997],  0.55 sec/batch.
2022-10-20 21:33:51,285 step [ 191], lr [0.0003000], embedding loss [ 0.8253], quantization loss [ 0.1065],  0.55 sec/batch.
2022-10-20 21:33:53,229 step [ 192], lr [0.0003000], embedding loss [ 0.8138], quantization loss [ 0.0948],  0.55 sec/batch.
2022-10-20 21:33:55,117 step [ 193], lr [0.0003000], embedding loss [ 0.8229], quantization loss [ 0.0864],  0.55 sec/batch.
2022-10-20 21:33:57,040 step [ 194], lr [0.0003000], embedding loss [ 0.8258], quantization loss [ 0.0878],  0.56 sec/batch.
2022-10-20 21:33:58,952 step [ 195], lr [0.0003000], embedding loss [ 0.8267], quantization loss [ 0.0919],  0.55 sec/batch.
2022-10-20 21:34:00,901 step [ 196], lr [0.0003000], embedding loss [ 0.8161], quantization loss [ 0.1054],  0.56 sec/batch.
2022-10-20 21:34:02,818 step [ 197], lr [0.0003000], embedding loss [ 0.8196], quantization loss [ 0.0849],  0.55 sec/batch.
2022-10-20 21:34:04,775 step [ 198], lr [0.0003000], embedding loss [ 0.8210], quantization loss [ 0.0895],  0.55 sec/batch.
2022-10-20 21:34:06,654 step [ 199], lr [0.0003000], embedding loss [ 0.8219], quantization loss [ 0.0928],  0.54 sec/batch.
2022-10-20 21:34:08,534 step [ 200], lr [0.0003000], embedding loss [ 0.8257], quantization loss [ 0.0934],  0.55 sec/batch.
2022-10-20 21:34:10,429 step [ 201], lr [0.0003000], embedding loss [ 0.8231], quantization loss [ 0.0814],  0.55 sec/batch.
2022-10-20 21:34:12,317 step [ 202], lr [0.0003000], embedding loss [ 0.8245], quantization loss [ 0.0939],  0.55 sec/batch.
2022-10-20 21:34:14,208 step [ 203], lr [0.0003000], embedding loss [ 0.8286], quantization loss [ 0.0908],  0.56 sec/batch.
2022-10-20 21:34:39,775 step [ 204], lr [0.0003000], embedding loss [ 0.8271], quantization loss [ 0.0981],  1.91 sec/batch.
2022-10-20 21:34:43,754 step [ 205], lr [0.0003000], embedding loss [ 0.8137], quantization loss [ 0.0836],  2.54 sec/batch.
2022-10-20 21:34:47,717 step [ 206], lr [0.0003000], embedding loss [ 0.8179], quantization loss [ 0.1044],  1.96 sec/batch.
2022-10-20 21:34:51,795 step [ 207], lr [0.0003000], embedding loss [ 0.8217], quantization loss [ 0.1007],  2.63 sec/batch.
2022-10-20 21:34:55,140 step [ 208], lr [0.0003000], embedding loss [ 0.8215], quantization loss [ 0.0961],  1.93 sec/batch.
2022-10-20 21:34:59,116 step [ 209], lr [0.0003000], embedding loss [ 0.8240], quantization loss [ 0.0800],  1.97 sec/batch.
2022-10-20 21:35:03,058 step [ 210], lr [0.0003000], embedding loss [ 0.8261], quantization loss [ 0.0870],  1.97 sec/batch.
2022-10-20 21:35:06,866 step [ 211], lr [0.0003000], embedding loss [ 0.8165], quantization loss [ 0.0879],  1.90 sec/batch.
2022-10-20 21:35:11,297 step [ 212], lr [0.0003000], embedding loss [ 0.8224], quantization loss [ 0.0821],  2.51 sec/batch.
2022-10-20 21:35:15,057 step [ 213], lr [0.0003000], embedding loss [ 0.8230], quantization loss [ 0.0836],  2.35 sec/batch.
2022-10-20 21:35:18,804 step [ 214], lr [0.0003000], embedding loss [ 0.8252], quantization loss [ 0.1023],  1.82 sec/batch.
2022-10-20 21:35:23,248 step [ 215], lr [0.0003000], embedding loss [ 0.8259], quantization loss [ 0.0893],  2.52 sec/batch.
2022-10-20 21:35:27,048 step [ 216], lr [0.0003000], embedding loss [ 0.8127], quantization loss [ 0.0906],  1.90 sec/batch.
2022-10-20 21:35:31,422 step [ 217], lr [0.0003000], embedding loss [ 0.8260], quantization loss [ 0.0795],  2.48 sec/batch.
2022-10-20 21:35:35,246 step [ 218], lr [0.0003000], embedding loss [ 0.8279], quantization loss [ 0.0892],  1.90 sec/batch.
2022-10-20 21:35:39,703 step [ 219], lr [0.0003000], embedding loss [ 0.8217], quantization loss [ 0.0999],  2.50 sec/batch.
2022-10-20 21:35:44,069 step [ 220], lr [0.0003000], embedding loss [ 0.8228], quantization loss [ 0.0961],  2.45 sec/batch.
2022-10-20 21:35:47,951 step [ 221], lr [0.0003000], embedding loss [ 0.8268], quantization loss [ 0.0891],  2.49 sec/batch.
2022-10-20 21:35:52,347 step [ 222], lr [0.0003000], embedding loss [ 0.8277], quantization loss [ 0.0848],  2.46 sec/batch.
2022-10-20 21:35:56,554 step [ 223], lr [0.0003000], embedding loss [ 0.8286], quantization loss [ 0.0767],  2.31 sec/batch.
2022-10-20 21:36:00,243 step [ 224], lr [0.0003000], embedding loss [ 0.8215], quantization loss [ 0.0847],  2.29 sec/batch.
2022-10-20 21:36:04,581 step [ 225], lr [0.0003000], embedding loss [ 0.8211], quantization loss [ 0.0959],  2.44 sec/batch.
2022-10-20 21:36:08,401 step [ 226], lr [0.0003000], embedding loss [ 0.8160], quantization loss [ 0.0758],  1.91 sec/batch.
2022-10-20 21:36:12,815 step [ 227], lr [0.0003000], embedding loss [ 0.8204], quantization loss [ 0.0938],  2.48 sec/batch.
2022-10-20 21:36:17,187 step [ 228], lr [0.0003000], embedding loss [ 0.8084], quantization loss [ 0.0968],  2.45 sec/batch.
2022-10-20 21:36:21,073 step [ 229], lr [0.0003000], embedding loss [ 0.8244], quantization loss [ 0.0844],  1.91 sec/batch.
2022-10-20 21:36:25,550 step [ 230], lr [0.0003000], embedding loss [ 0.8209], quantization loss [ 0.0886],  2.51 sec/batch.
2022-10-20 21:36:30,000 step [ 231], lr [0.0003000], embedding loss [ 0.8230], quantization loss [ 0.0837],  2.51 sec/batch.
2022-10-20 21:36:34,323 step [ 232], lr [0.0003000], embedding loss [ 0.8246], quantization loss [ 0.0891],  2.40 sec/batch.
2022-10-20 21:36:38,175 step [ 233], lr [0.0003000], embedding loss [ 0.8258], quantization loss [ 0.0863],  2.46 sec/batch.
2022-10-20 21:36:42,301 step [ 234], lr [0.0003000], embedding loss [ 0.8196], quantization loss [ 0.0889],  2.68 sec/batch.
2022-10-20 21:36:46,744 step [ 235], lr [0.0003000], embedding loss [ 0.8268], quantization loss [ 0.0791],  2.44 sec/batch.
2022-10-20 21:36:51,203 step [ 236], lr [0.0003000], embedding loss [ 0.8159], quantization loss [ 0.0834],  2.51 sec/batch.
2022-10-20 21:36:55,049 step [ 237], lr [0.0003000], embedding loss [ 0.8150], quantization loss [ 0.0923],  2.43 sec/batch.
2022-10-20 21:36:59,509 step [ 238], lr [0.0003000], embedding loss [ 0.8210], quantization loss [ 0.0884],  2.54 sec/batch.
2022-10-20 21:37:03,882 step [ 239], lr [0.0003000], embedding loss [ 0.8320], quantization loss [ 0.0835],  2.45 sec/batch.
2022-10-20 21:37:07,753 step [ 240], lr [0.0003000], embedding loss [ 0.8277], quantization loss [ 0.0921],  2.45 sec/batch.
2022-10-20 21:37:12,146 step [ 241], lr [0.0003000], embedding loss [ 0.8194], quantization loss [ 0.0852],  2.45 sec/batch.
2022-10-20 21:37:16,361 step [ 242], lr [0.0003000], embedding loss [ 0.8221], quantization loss [ 0.0952],  2.32 sec/batch.
2022-10-20 21:37:20,698 step [ 243], lr [0.0003000], embedding loss [ 0.8392], quantization loss [ 0.0819],  2.43 sec/batch.
2022-10-20 21:37:24,464 step [ 244], lr [0.0003000], embedding loss [ 0.8290], quantization loss [ 0.0811],  2.40 sec/batch.
2022-10-20 21:37:28,803 step [ 245], lr [0.0003000], embedding loss [ 0.8184], quantization loss [ 0.0951],  2.44 sec/batch.
2022-10-20 21:37:33,106 step [ 246], lr [0.0003000], embedding loss [ 0.8226], quantization loss [ 0.0809],  2.42 sec/batch.
2022-10-20 21:37:37,110 step [ 247], lr [0.0003000], embedding loss [ 0.8186], quantization loss [ 0.0928],  2.52 sec/batch.
2022-10-20 21:37:41,527 step [ 248], lr [0.0003000], embedding loss [ 0.8188], quantization loss [ 0.0842],  2.43 sec/batch.
2022-10-20 21:37:45,325 step [ 249], lr [0.0003000], embedding loss [ 0.8218], quantization loss [ 0.1208],  2.42 sec/batch.
2022-10-20 21:37:49,780 step [ 250], lr [0.0003000], embedding loss [ 0.8175], quantization loss [ 0.0835],  2.52 sec/batch.
2022-10-20 21:37:54,303 step [ 251], lr [0.0003000], embedding loss [ 0.8242], quantization loss [ 0.0836],  2.56 sec/batch.
2022-10-20 21:37:58,834 step [ 252], lr [0.0003000], embedding loss [ 0.8209], quantization loss [ 0.0899],  2.55 sec/batch.
2022-10-20 21:38:02,645 step [ 253], lr [0.0003000], embedding loss [ 0.8274], quantization loss [ 0.0863],  2.42 sec/batch.
2022-10-20 21:38:07,076 step [ 254], lr [0.0003000], embedding loss [ 0.8263], quantization loss [ 0.0934],  2.47 sec/batch.
2022-10-20 21:38:11,010 step [ 255], lr [0.0003000], embedding loss [ 0.8238], quantization loss [ 0.0881],  2.52 sec/batch.
2022-10-20 21:38:15,032 step [ 256], lr [0.0003000], embedding loss [ 0.8214], quantization loss [ 0.0911],  2.61 sec/batch.
2022-10-20 21:38:19,047 step [ 257], lr [0.0003000], embedding loss [ 0.8165], quantization loss [ 0.0943],  2.53 sec/batch.
2022-10-20 21:38:23,429 step [ 258], lr [0.0003000], embedding loss [ 0.8233], quantization loss [ 0.0810],  2.45 sec/batch.
2022-10-20 21:38:26,897 step [ 259], lr [0.0003000], embedding loss [ 0.8223], quantization loss [ 0.0846],  2.01 sec/batch.
2022-10-20 21:38:30,916 step [ 260], lr [0.0003000], embedding loss [ 0.8132], quantization loss [ 0.1010],  2.59 sec/batch.
2022-10-20 21:38:35,429 step [ 261], lr [0.0003000], embedding loss [ 0.8143], quantization loss [ 0.0923],  2.56 sec/batch.
2022-10-20 21:38:39,893 step [ 262], lr [0.0003000], embedding loss [ 0.8273], quantization loss [ 0.0898],  2.48 sec/batch.
2022-10-20 21:38:43,814 step [ 263], lr [0.0003000], embedding loss [ 0.8254], quantization loss [ 0.0746],  2.50 sec/batch.
2022-10-20 21:38:48,126 step [ 264], lr [0.0003000], embedding loss [ 0.8279], quantization loss [ 0.0867],  2.37 sec/batch.
2022-10-20 21:38:52,284 step [ 265], lr [0.0003000], embedding loss [ 0.8147], quantization loss [ 0.0822],  2.41 sec/batch.
2022-10-20 21:38:57,090 step [ 266], lr [0.0003000], embedding loss [ 0.8230], quantization loss [ 0.0962],  2.55 sec/batch.
2022-10-20 21:39:00,896 step [ 267], lr [0.0003000], embedding loss [ 0.8171], quantization loss [ 0.0850],  2.42 sec/batch.
2022-10-20 21:39:05,266 step [ 268], lr [0.0003000], embedding loss [ 0.8283], quantization loss [ 0.0785],  2.43 sec/batch.
2022-10-20 21:39:09,132 step [ 269], lr [0.0003000], embedding loss [ 0.8219], quantization loss [ 0.0703],  1.93 sec/batch.
2022-10-20 21:39:13,552 step [ 270], lr [0.0003000], embedding loss [ 0.8186], quantization loss [ 0.0904],  2.46 sec/batch.
2022-10-20 21:39:17,699 step [ 271], lr [0.0003000], embedding loss [ 0.8229], quantization loss [ 0.0761],  2.63 sec/batch.
2022-10-20 21:39:21,868 step [ 272], lr [0.0003000], embedding loss [ 0.8239], quantization loss [ 0.0820],  2.57 sec/batch.
2022-10-20 21:39:26,400 step [ 273], lr [0.0003000], embedding loss [ 0.8278], quantization loss [ 0.0965],  2.57 sec/batch.
2022-10-20 21:39:30,368 step [ 274], lr [0.0003000], embedding loss [ 0.8178], quantization loss [ 0.0828],  2.58 sec/batch.
2022-10-20 21:39:34,848 step [ 275], lr [0.0003000], embedding loss [ 0.8213], quantization loss [ 0.0812],  2.53 sec/batch.
2022-10-20 21:39:38,731 step [ 276], lr [0.0003000], embedding loss [ 0.8176], quantization loss [ 0.0799],  2.51 sec/batch.
2022-10-20 21:39:43,233 step [ 277], lr [0.0003000], embedding loss [ 0.8193], quantization loss [ 0.0884],  2.52 sec/batch.
2022-10-20 21:39:47,646 step [ 278], lr [0.0003000], embedding loss [ 0.8216], quantization loss [ 0.0768],  2.41 sec/batch.
2022-10-20 21:39:51,521 step [ 279], lr [0.0003000], embedding loss [ 0.8179], quantization loss [ 0.0840],  2.49 sec/batch.
2022-10-20 21:39:55,919 step [ 280], lr [0.0003000], embedding loss [ 0.8324], quantization loss [ 0.0765],  2.44 sec/batch.
2022-10-20 21:39:59,732 step [ 281], lr [0.0003000], embedding loss [ 0.8097], quantization loss [ 0.1062],  1.89 sec/batch.
2022-10-20 21:40:03,832 step [ 282], lr [0.0003000], embedding loss [ 0.8239], quantization loss [ 0.0774],  2.52 sec/batch.
2022-10-20 21:40:08,369 step [ 283], lr [0.0003000], embedding loss [ 0.8144], quantization loss [ 0.0833],  2.42 sec/batch.
2022-10-20 21:40:12,233 step [ 284], lr [0.0003000], embedding loss [ 0.8212], quantization loss [ 0.0902],  2.45 sec/batch.
2022-10-20 21:40:16,183 step [ 285], lr [0.0003000], embedding loss [ 0.8276], quantization loss [ 0.0779],  2.51 sec/batch.
2022-10-20 21:40:20,116 step [ 286], lr [0.0003000], embedding loss [ 0.8194], quantization loss [ 0.0846],  1.97 sec/batch.
2022-10-20 21:40:24,614 step [ 287], lr [0.0003000], embedding loss [ 0.8165], quantization loss [ 0.0734],  2.49 sec/batch.
2022-10-20 21:40:28,056 step [ 288], lr [0.0003000], embedding loss [ 0.8331], quantization loss [ 0.0851],  1.96 sec/batch.
2022-10-20 21:40:32,521 step [ 289], lr [0.0003000], embedding loss [ 0.8212], quantization loss [ 0.0777],  2.41 sec/batch.
2022-10-20 21:40:36,493 step [ 290], lr [0.0003000], embedding loss [ 0.8238], quantization loss [ 0.0884],  2.55 sec/batch.
2022-10-20 21:40:40,343 step [ 291], lr [0.0003000], embedding loss [ 0.8221], quantization loss [ 0.0837],  2.48 sec/batch.
2022-10-20 21:40:44,283 step [ 292], lr [0.0003000], embedding loss [ 0.8287], quantization loss [ 0.0889],  2.00 sec/batch.
2022-10-20 21:40:48,810 step [ 293], lr [0.0003000], embedding loss [ 0.8202], quantization loss [ 0.0735],  2.55 sec/batch.
2022-10-20 21:40:48,811 update codes and centers iter(1/1).
2022-10-20 21:40:56,626 number of update_code wrong: 0.
2022-10-20 21:40:59,546 non zero codewords: 1024.
2022-10-20 21:40:59,546 finish center update, duration: 10.74 sec.
2022-10-20 21:41:03,374 step [ 294], lr [0.0003000], embedding loss [ 0.8300], quantization loss [ 0.0250],  2.42 sec/batch.
2022-10-20 21:41:07,222 step [ 295], lr [0.0003000], embedding loss [ 0.8246], quantization loss [ 0.0225],  2.45 sec/batch.
2022-10-20 21:41:11,733 step [ 296], lr [0.0003000], embedding loss [ 0.8251], quantization loss [ 0.0229],  2.57 sec/batch.
2022-10-20 21:41:15,732 step [ 297], lr [0.0003000], embedding loss [ 0.8180], quantization loss [ 0.0221],  2.56 sec/batch.
2022-10-20 21:41:20,302 step [ 298], lr [0.0003000], embedding loss [ 0.8133], quantization loss [ 0.0227],  2.60 sec/batch.
2022-10-20 21:41:24,229 step [ 299], lr [0.0003000], embedding loss [ 0.8224], quantization loss [ 0.0206],  1.95 sec/batch.
2022-10-20 21:41:28,746 step [ 300], lr [0.0003000], embedding loss [ 0.8145], quantization loss [ 0.0248],  2.51 sec/batch.
2022-10-20 21:41:33,157 step [ 301], lr [0.0001500], embedding loss [ 0.8167], quantization loss [ 0.0228],  2.47 sec/batch.
2022-10-20 21:41:37,101 step [ 302], lr [0.0001500], embedding loss [ 0.8252], quantization loss [ 0.0220],  2.57 sec/batch.
2022-10-20 21:41:41,599 step [ 303], lr [0.0001500], embedding loss [ 0.8297], quantization loss [ 0.0222],  2.54 sec/batch.
2022-10-20 21:41:45,591 step [ 304], lr [0.0001500], embedding loss [ 0.8269], quantization loss [ 0.0208],  2.60 sec/batch.
2022-10-20 21:41:49,257 step [ 305], lr [0.0001500], embedding loss [ 0.8241], quantization loss [ 0.0216],  1.70 sec/batch.
2022-10-20 21:41:52,236 step [ 306], lr [0.0001500], embedding loss [ 0.8246], quantization loss [ 0.0213],  1.66 sec/batch.
2022-10-20 21:41:55,400 step [ 307], lr [0.0001500], embedding loss [ 0.8168], quantization loss [ 0.0220],  1.82 sec/batch.
2022-10-20 21:41:58,344 step [ 308], lr [0.0001500], embedding loss [ 0.8227], quantization loss [ 0.0197],  1.57 sec/batch.
2022-10-20 21:42:02,096 step [ 309], lr [0.0001500], embedding loss [ 0.8274], quantization loss [ 0.0202],  2.34 sec/batch.
2022-10-20 21:42:05,868 step [ 310], lr [0.0001500], embedding loss [ 0.8151], quantization loss [ 0.0223],  2.38 sec/batch.
2022-10-20 21:42:09,628 step [ 311], lr [0.0001500], embedding loss [ 0.8265], quantization loss [ 0.0228],  2.38 sec/batch.
2022-10-20 21:42:13,369 step [ 312], lr [0.0001500], embedding loss [ 0.8291], quantization loss [ 0.0206],  2.34 sec/batch.
2022-10-20 21:42:17,649 step [ 313], lr [0.0001500], embedding loss [ 0.8208], quantization loss [ 0.0199],  2.87 sec/batch.
2022-10-20 21:42:21,465 step [ 314], lr [0.0001500], embedding loss [ 0.8306], quantization loss [ 0.0212],  2.40 sec/batch.
2022-10-20 21:42:25,837 step [ 315], lr [0.0001500], embedding loss [ 0.8291], quantization loss [ 0.0214],  2.21 sec/batch.
2022-10-20 21:42:30,358 step [ 316], lr [0.0001500], embedding loss [ 0.8194], quantization loss [ 0.0217],  3.13 sec/batch.
2022-10-20 21:42:34,072 step [ 317], lr [0.0001500], embedding loss [ 0.8232], quantization loss [ 0.0213],  2.32 sec/batch.
2022-10-20 21:42:38,615 step [ 318], lr [0.0001500], embedding loss [ 0.8283], quantization loss [ 0.0213],  2.38 sec/batch.
2022-10-20 21:42:43,059 step [ 319], lr [0.0001500], embedding loss [ 0.8225], quantization loss [ 0.0213],  2.99 sec/batch.
2022-10-20 21:42:47,517 step [ 320], lr [0.0001500], embedding loss [ 0.8202], quantization loss [ 0.0205],  3.10 sec/batch.
2022-10-20 21:42:51,803 step [ 321], lr [0.0001500], embedding loss [ 0.8179], quantization loss [ 0.0207],  2.93 sec/batch.
2022-10-20 21:42:56,262 step [ 322], lr [0.0001500], embedding loss [ 0.8264], quantization loss [ 0.0193],  3.06 sec/batch.
2022-10-20 21:43:00,539 step [ 323], lr [0.0001500], embedding loss [ 0.8227], quantization loss [ 0.0198],  2.84 sec/batch.
2022-10-20 21:43:04,944 step [ 324], lr [0.0001500], embedding loss [ 0.8251], quantization loss [ 0.0218],  3.01 sec/batch.
2022-10-20 21:43:09,251 step [ 325], lr [0.0001500], embedding loss [ 0.8226], quantization loss [ 0.0215],  2.94 sec/batch.
2022-10-20 21:43:13,767 step [ 326], lr [0.0001500], embedding loss [ 0.8247], quantization loss [ 0.0211],  3.11 sec/batch.
2022-10-20 21:43:18,296 step [ 327], lr [0.0001500], embedding loss [ 0.8244], quantization loss [ 0.0195],  2.37 sec/batch.
2022-10-20 21:43:23,309 step [ 328], lr [0.0001500], embedding loss [ 0.8117], quantization loss [ 0.0200],  2.83 sec/batch.
2022-10-20 21:43:27,648 step [ 329], lr [0.0001500], embedding loss [ 0.8190], quantization loss [ 0.0222],  2.92 sec/batch.
2022-10-20 21:43:32,146 step [ 330], lr [0.0001500], embedding loss [ 0.8204], quantization loss [ 0.0193],  3.10 sec/batch.
2022-10-20 21:43:36,728 step [ 331], lr [0.0001500], embedding loss [ 0.8110], quantization loss [ 0.0211],  3.17 sec/batch.
2022-10-20 21:43:41,082 step [ 332], lr [0.0001500], embedding loss [ 0.8154], quantization loss [ 0.0215],  3.00 sec/batch.
2022-10-20 21:43:45,230 step [ 333], lr [0.0001500], embedding loss [ 0.8196], quantization loss [ 0.0203],  2.14 sec/batch.
2022-10-20 21:43:49,619 step [ 334], lr [0.0001500], embedding loss [ 0.8230], quantization loss [ 0.0196],  2.28 sec/batch.
2022-10-20 21:43:54,788 step [ 335], lr [0.0001500], embedding loss [ 0.8220], quantization loss [ 0.0193],  2.99 sec/batch.
2022-10-20 21:43:59,126 step [ 336], lr [0.0001500], embedding loss [ 0.8172], quantization loss [ 0.0197],  2.94 sec/batch.
2022-10-20 21:44:03,554 step [ 337], lr [0.0001500], embedding loss [ 0.8235], quantization loss [ 0.0213],  3.04 sec/batch.
2022-10-20 21:44:07,858 step [ 338], lr [0.0001500], embedding loss [ 0.8159], quantization loss [ 0.0212],  2.92 sec/batch.
2022-10-20 21:44:12,145 step [ 339], lr [0.0001500], embedding loss [ 0.8248], quantization loss [ 0.0194],  2.20 sec/batch.
2022-10-20 21:44:16,655 step [ 340], lr [0.0001500], embedding loss [ 0.8158], quantization loss [ 0.0197],  2.31 sec/batch.
2022-10-20 21:44:21,868 step [ 341], lr [0.0001500], embedding loss [ 0.8203], quantization loss [ 0.0202],  3.03 sec/batch.
2022-10-20 21:44:26,264 step [ 342], lr [0.0001500], embedding loss [ 0.8274], quantization loss [ 0.0197],  3.03 sec/batch.
2022-10-20 21:44:30,838 step [ 343], lr [0.0001500], embedding loss [ 0.8171], quantization loss [ 0.0201],  3.18 sec/batch.
2022-10-20 21:44:35,317 step [ 344], lr [0.0001500], embedding loss [ 0.8240], quantization loss [ 0.0196],  2.29 sec/batch.
2022-10-20 21:44:40,384 step [ 345], lr [0.0001500], embedding loss [ 0.8246], quantization loss [ 0.0188],  2.86 sec/batch.
2022-10-20 21:44:44,635 step [ 346], lr [0.0001500], embedding loss [ 0.8148], quantization loss [ 0.0205],  2.84 sec/batch.
2022-10-20 21:44:49,010 step [ 347], lr [0.0001500], embedding loss [ 0.8248], quantization loss [ 0.0198],  2.98 sec/batch.
2022-10-20 21:44:53,271 step [ 348], lr [0.0001500], embedding loss [ 0.8234], quantization loss [ 0.0205],  2.15 sec/batch.
2022-10-20 21:44:58,511 step [ 349], lr [0.0001500], embedding loss [ 0.8230], quantization loss [ 0.0197],  3.06 sec/batch.
2022-10-20 21:45:02,925 step [ 350], lr [0.0001500], embedding loss [ 0.8178], quantization loss [ 0.0216],  3.02 sec/batch.
2022-10-20 21:45:07,348 step [ 351], lr [0.0001500], embedding loss [ 0.8233], quantization loss [ 0.0205],  2.28 sec/batch.
2022-10-20 21:45:12,445 step [ 352], lr [0.0001500], embedding loss [ 0.8240], quantization loss [ 0.0201],  2.89 sec/batch.
2022-10-20 21:45:16,699 step [ 353], lr [0.0001500], embedding loss [ 0.8197], quantization loss [ 0.0195],  2.86 sec/batch.
2022-10-20 21:45:20,980 step [ 354], lr [0.0001500], embedding loss [ 0.8168], quantization loss [ 0.0213],  2.87 sec/batch.
2022-10-20 21:45:25,954 step [ 355], lr [0.0001500], embedding loss [ 0.8315], quantization loss [ 0.0196],  2.87 sec/batch.
2022-10-20 21:45:30,324 step [ 356], lr [0.0001500], embedding loss [ 0.8119], quantization loss [ 0.0212],  2.97 sec/batch.
2022-10-20 21:45:34,722 step [ 357], lr [0.0001500], embedding loss [ 0.8177], quantization loss [ 0.0205],  2.24 sec/batch.
2022-10-20 21:45:39,796 step [ 358], lr [0.0001500], embedding loss [ 0.8172], quantization loss [ 0.0209],  2.91 sec/batch.
2022-10-20 21:45:44,135 step [ 359], lr [0.0001500], embedding loss [ 0.8163], quantization loss [ 0.0223],  2.92 sec/batch.
2022-10-20 21:45:49,351 step [ 360], lr [0.0001500], embedding loss [ 0.8177], quantization loss [ 0.0188],  3.03 sec/batch.
2022-10-20 21:45:53,760 step [ 361], lr [0.0001500], embedding loss [ 0.8220], quantization loss [ 0.0195],  2.99 sec/batch.
2022-10-20 21:45:58,035 step [ 362], lr [0.0001500], embedding loss [ 0.8192], quantization loss [ 0.0189],  2.17 sec/batch.
2022-10-20 21:46:03,085 step [ 363], lr [0.0001500], embedding loss [ 0.8173], quantization loss [ 0.0208],  2.86 sec/batch.
2022-10-20 21:46:07,578 step [ 364], lr [0.0001500], embedding loss [ 0.8231], quantization loss [ 0.0196],  3.08 sec/batch.
2022-10-20 21:46:12,761 step [ 365], lr [0.0001500], embedding loss [ 0.8193], quantization loss [ 0.0191],  3.03 sec/batch.
2022-10-20 21:46:17,160 step [ 366], lr [0.0001500], embedding loss [ 0.8278], quantization loss [ 0.0195],  2.99 sec/batch.
2022-10-20 21:46:21,568 step [ 367], lr [0.0001500], embedding loss [ 0.8272], quantization loss [ 0.0187],  2.26 sec/batch.
2022-10-20 21:46:26,860 step [ 368], lr [0.0001500], embedding loss [ 0.8196], quantization loss [ 0.0200],  3.11 sec/batch.
2022-10-20 21:46:31,341 step [ 369], lr [0.0001500], embedding loss [ 0.8257], quantization loss [ 0.0195],  3.06 sec/batch.
2022-10-20 21:46:36,371 step [ 370], lr [0.0001500], embedding loss [ 0.8180], quantization loss [ 0.0184],  2.87 sec/batch.
2022-10-20 21:46:40,662 step [ 371], lr [0.0001500], embedding loss [ 0.8271], quantization loss [ 0.0184],  2.88 sec/batch.
2022-10-20 21:46:45,879 step [ 372], lr [0.0001500], embedding loss [ 0.8110], quantization loss [ 0.0203],  3.05 sec/batch.
2022-10-20 21:46:50,325 step [ 373], lr [0.0001500], embedding loss [ 0.8312], quantization loss [ 0.0173],  3.04 sec/batch.
2022-10-20 21:46:54,780 step [ 374], lr [0.0001500], embedding loss [ 0.8300], quantization loss [ 0.0175],  2.29 sec/batch.
2022-10-20 21:46:59,716 step [ 375], lr [0.0001500], embedding loss [ 0.8218], quantization loss [ 0.0172],  2.88 sec/batch.
2022-10-20 21:47:03,912 step [ 376], lr [0.0001500], embedding loss [ 0.8123], quantization loss [ 0.0180],  2.85 sec/batch.
2022-10-20 21:47:08,810 step [ 377], lr [0.0001500], embedding loss [ 0.8146], quantization loss [ 0.0198],  2.85 sec/batch.
2022-10-20 21:47:12,952 step [ 378], lr [0.0001500], embedding loss [ 0.8172], quantization loss [ 0.0189],  2.84 sec/batch.
2022-10-20 21:47:18,139 step [ 379], lr [0.0001500], embedding loss [ 0.8153], quantization loss [ 0.0191],  3.07 sec/batch.
2022-10-20 21:47:22,583 step [ 380], lr [0.0001500], embedding loss [ 0.8207], quantization loss [ 0.0188],  3.07 sec/batch.
2022-10-20 21:47:27,608 step [ 381], lr [0.0001500], embedding loss [ 0.8265], quantization loss [ 0.0188],  2.89 sec/batch.
2022-10-20 21:47:31,821 step [ 382], lr [0.0001500], embedding loss [ 0.8255], quantization loss [ 0.0191],  2.83 sec/batch.
2022-10-20 21:47:36,028 step [ 383], lr [0.0001500], embedding loss [ 0.8151], quantization loss [ 0.0194],  2.11 sec/batch.
2022-10-20 21:47:41,370 step [ 384], lr [0.0001500], embedding loss [ 0.8274], quantization loss [ 0.0201],  3.16 sec/batch.
2022-10-20 21:47:45,617 step [ 385], lr [0.0001500], embedding loss [ 0.8217], quantization loss [ 0.0187],  2.20 sec/batch.
2022-10-20 21:47:50,626 step [ 386], lr [0.0001500], embedding loss [ 0.8249], quantization loss [ 0.0183],  2.92 sec/batch.
2022-10-20 21:47:54,788 step [ 387], lr [0.0001500], embedding loss [ 0.8140], quantization loss [ 0.0187],  2.14 sec/batch.
2022-10-20 21:47:59,914 step [ 388], lr [0.0001500], embedding loss [ 0.8299], quantization loss [ 0.0208],  2.90 sec/batch.
2022-10-20 21:48:04,759 step [ 389], lr [0.0001500], embedding loss [ 0.8203], quantization loss [ 0.0191],  2.67 sec/batch.
2022-10-20 21:48:10,513 step [ 390], lr [0.0001500], embedding loss [ 0.8183], quantization loss [ 0.0199],  3.40 sec/batch.
2022-10-20 21:48:15,341 step [ 391], lr [0.0001500], embedding loss [ 0.8219], quantization loss [ 0.0199],  2.39 sec/batch.
2022-10-20 21:48:21,205 step [ 392], lr [0.0001500], embedding loss [ 0.8259], quantization loss [ 0.0194],  3.29 sec/batch.
2022-10-20 21:48:25,802 step [ 393], lr [0.0001500], embedding loss [ 0.8270], quantization loss [ 0.0209],  2.47 sec/batch.
2022-10-20 21:48:31,079 step [ 394], lr [0.0001500], embedding loss [ 0.8181], quantization loss [ 0.0216],  3.07 sec/batch.
2022-10-20 21:48:35,849 step [ 395], lr [0.0001500], embedding loss [ 0.8210], quantization loss [ 0.0199],  2.32 sec/batch.
2022-10-20 21:48:41,290 step [ 396], lr [0.0001500], embedding loss [ 0.8195], quantization loss [ 0.0191],  3.22 sec/batch.
2022-10-20 21:48:45,514 step [ 397], lr [0.0001500], embedding loss [ 0.8232], quantization loss [ 0.0210],  2.17 sec/batch.
2022-10-20 21:48:50,662 step [ 398], lr [0.0001500], embedding loss [ 0.8204], quantization loss [ 0.0197],  3.08 sec/batch.
2022-10-20 21:48:55,645 step [ 399], lr [0.0001500], embedding loss [ 0.8212], quantization loss [ 0.0201],  2.86 sec/batch.
2022-10-20 21:49:00,133 step [ 400], lr [0.0001500], embedding loss [ 0.8148], quantization loss [ 0.0190],  3.04 sec/batch.
2022-10-20 21:49:05,765 step [ 401], lr [0.0001500], embedding loss [ 0.8127], quantization loss [ 0.0206],  3.34 sec/batch.
2022-10-20 21:49:10,356 step [ 402], lr [0.0001500], embedding loss [ 0.8173], quantization loss [ 0.0188],  3.20 sec/batch.
2022-10-20 21:49:15,349 step [ 403], lr [0.0001500], embedding loss [ 0.8260], quantization loss [ 0.0191],  2.85 sec/batch.
2022-10-20 21:49:19,729 step [ 404], lr [0.0001500], embedding loss [ 0.8290], quantization loss [ 0.0195],  2.94 sec/batch.
2022-10-20 21:49:25,394 step [ 405], lr [0.0001500], embedding loss [ 0.8244], quantization loss [ 0.0189],  3.21 sec/batch.
2022-10-20 21:49:30,226 step [ 406], lr [0.0001500], embedding loss [ 0.8211], quantization loss [ 0.0179],  3.40 sec/batch.
2022-10-20 21:49:35,351 step [ 407], lr [0.0001500], embedding loss [ 0.8216], quantization loss [ 0.0210],  2.95 sec/batch.
2022-10-20 21:49:39,855 step [ 408], lr [0.0001500], embedding loss [ 0.8278], quantization loss [ 0.0191],  3.07 sec/batch.
2022-10-20 21:49:45,311 step [ 409], lr [0.0001500], embedding loss [ 0.8248], quantization loss [ 0.0176],  3.13 sec/batch.
2022-10-20 21:49:49,625 step [ 410], lr [0.0001500], embedding loss [ 0.8218], quantization loss [ 0.0201],  2.23 sec/batch.
2022-10-20 21:49:54,804 step [ 411], lr [0.0001500], embedding loss [ 0.8287], quantization loss [ 0.0213],  3.04 sec/batch.
2022-10-20 21:49:59,046 step [ 412], lr [0.0001500], embedding loss [ 0.8228], quantization loss [ 0.0199],  2.18 sec/batch.
2022-10-20 21:50:04,335 step [ 413], lr [0.0001500], embedding loss [ 0.8120], quantization loss [ 0.0186],  3.06 sec/batch.
2022-10-20 21:50:09,740 step [ 414], lr [0.0001500], embedding loss [ 0.8266], quantization loss [ 0.0185],  3.27 sec/batch.
2022-10-20 21:50:14,324 step [ 415], lr [0.0001500], embedding loss [ 0.8247], quantization loss [ 0.0188],  3.15 sec/batch.
2022-10-20 21:50:19,676 step [ 416], lr [0.0001500], embedding loss [ 0.8164], quantization loss [ 0.0194],  3.16 sec/batch.
2022-10-20 21:50:24,016 step [ 417], lr [0.0001500], embedding loss [ 0.8247], quantization loss [ 0.0188],  2.90 sec/batch.
2022-10-20 21:50:29,034 step [ 418], lr [0.0001500], embedding loss [ 0.8248], quantization loss [ 0.0204],  2.88 sec/batch.
2022-10-20 21:50:33,328 step [ 419], lr [0.0001500], embedding loss [ 0.8300], quantization loss [ 0.0192],  2.85 sec/batch.
2022-10-20 21:50:38,274 step [ 420], lr [0.0001500], embedding loss [ 0.8193], quantization loss [ 0.0187],  2.85 sec/batch.
2022-10-20 21:50:42,641 step [ 421], lr [0.0001500], embedding loss [ 0.8183], quantization loss [ 0.0196],  2.95 sec/batch.
2022-10-20 21:50:47,517 step [ 422], lr [0.0001500], embedding loss [ 0.8146], quantization loss [ 0.0187],  2.85 sec/batch.
2022-10-20 21:50:51,769 step [ 423], lr [0.0001500], embedding loss [ 0.8133], quantization loss [ 0.0180],  2.84 sec/batch.
2022-10-20 21:50:56,758 step [ 424], lr [0.0001500], embedding loss [ 0.8293], quantization loss [ 0.0202],  2.90 sec/batch.
2022-10-20 21:51:01,081 step [ 425], lr [0.0001500], embedding loss [ 0.8190], quantization loss [ 0.0186],  2.89 sec/batch.
2022-10-20 21:51:06,054 step [ 426], lr [0.0001500], embedding loss [ 0.8268], quantization loss [ 0.0201],  2.88 sec/batch.
2022-10-20 21:51:10,594 step [ 427], lr [0.0001500], embedding loss [ 0.8220], quantization loss [ 0.0195],  2.36 sec/batch.
2022-10-20 21:51:15,818 step [ 428], lr [0.0001500], embedding loss [ 0.8203], quantization loss [ 0.0187],  3.03 sec/batch.
2022-10-20 21:51:20,304 step [ 429], lr [0.0001500], embedding loss [ 0.8188], quantization loss [ 0.0187],  2.30 sec/batch.
2022-10-20 21:51:25,373 step [ 430], lr [0.0001500], embedding loss [ 0.8176], quantization loss [ 0.0198],  2.88 sec/batch.
2022-10-20 21:51:30,375 step [ 431], lr [0.0001500], embedding loss [ 0.8140], quantization loss [ 0.0197],  2.84 sec/batch.
2022-10-20 21:51:34,836 step [ 432], lr [0.0001500], embedding loss [ 0.8181], quantization loss [ 0.0171],  3.03 sec/batch.
2022-10-20 21:51:39,984 step [ 433], lr [0.0001500], embedding loss [ 0.8200], quantization loss [ 0.0191],  2.93 sec/batch.
2022-10-20 21:51:44,513 step [ 434], lr [0.0001500], embedding loss [ 0.8183], quantization loss [ 0.0179],  3.10 sec/batch.
2022-10-20 21:51:49,475 step [ 435], lr [0.0001500], embedding loss [ 0.8166], quantization loss [ 0.0205],  2.87 sec/batch.
2022-10-20 21:51:53,890 step [ 436], lr [0.0001500], embedding loss [ 0.8250], quantization loss [ 0.0193],  2.98 sec/batch.
2022-10-20 21:51:59,162 step [ 437], lr [0.0001500], embedding loss [ 0.8197], quantization loss [ 0.0173],  3.08 sec/batch.
2022-10-20 21:52:03,501 step [ 438], lr [0.0001500], embedding loss [ 0.8217], quantization loss [ 0.0179],  2.92 sec/batch.
2022-10-20 21:52:08,689 step [ 439], lr [0.0001500], embedding loss [ 0.8258], quantization loss [ 0.0179],  3.00 sec/batch.
2022-10-20 21:52:08,689 update codes and centers iter(1/1).
2022-10-20 21:52:16,448 number of update_code wrong: 0.
2022-10-20 21:52:19,228 non zero codewords: 1024.
2022-10-20 21:52:19,229 finish center update, duration: 10.54 sec.
2022-10-20 21:52:23,380 step [ 440], lr [0.0001500], embedding loss [ 0.8217], quantization loss [ 0.0147],  2.17 sec/batch.
2022-10-20 21:52:28,299 step [ 441], lr [0.0001500], embedding loss [ 0.8161], quantization loss [ 0.0136],  2.85 sec/batch.
2022-10-20 21:52:33,244 step [ 442], lr [0.0001500], embedding loss [ 0.8215], quantization loss [ 0.0140],  2.83 sec/batch.
2022-10-20 21:52:37,457 step [ 443], lr [0.0001500], embedding loss [ 0.8274], quantization loss [ 0.0132],  2.85 sec/batch.
2022-10-20 21:52:42,397 step [ 444], lr [0.0001500], embedding loss [ 0.8207], quantization loss [ 0.0144],  2.83 sec/batch.
2022-10-20 21:52:46,861 step [ 445], lr [0.0001500], embedding loss [ 0.8257], quantization loss [ 0.0138],  3.06 sec/batch.
2022-10-20 21:52:51,952 step [ 446], lr [0.0001500], embedding loss [ 0.8143], quantization loss [ 0.0125],  2.91 sec/batch.
2022-10-20 21:52:56,203 step [ 447], lr [0.0001500], embedding loss [ 0.8135], quantization loss [ 0.0138],  2.89 sec/batch.
2022-10-20 21:53:01,215 step [ 448], lr [0.0001500], embedding loss [ 0.8236], quantization loss [ 0.0137],  2.88 sec/batch.
2022-10-20 21:53:05,415 step [ 449], lr [0.0001500], embedding loss [ 0.8161], quantization loss [ 0.0138],  2.14 sec/batch.
2022-10-20 21:53:10,616 step [ 450], lr [0.0001500], embedding loss [ 0.8184], quantization loss [ 0.0139],  2.95 sec/batch.
2022-10-20 21:53:15,698 step [ 451], lr [0.0001500], embedding loss [ 0.8230], quantization loss [ 0.0131],  2.91 sec/batch.
2022-10-20 21:53:20,240 step [ 452], lr [0.0001500], embedding loss [ 0.8223], quantization loss [ 0.0124],  3.11 sec/batch.
2022-10-20 21:53:25,490 step [ 453], lr [0.0001500], embedding loss [ 0.8254], quantization loss [ 0.0136],  3.07 sec/batch.
2022-10-20 21:53:29,765 step [ 454], lr [0.0001500], embedding loss [ 0.8271], quantization loss [ 0.0130],  2.85 sec/batch.
2022-10-20 21:53:34,766 step [ 455], lr [0.0001500], embedding loss [ 0.8263], quantization loss [ 0.0133],  2.87 sec/batch.
2022-10-20 21:53:39,064 step [ 456], lr [0.0001500], embedding loss [ 0.8214], quantization loss [ 0.0141],  2.20 sec/batch.
2022-10-20 21:53:44,174 step [ 457], lr [0.0001500], embedding loss [ 0.8239], quantization loss [ 0.0130],  2.89 sec/batch.
2022-10-20 21:53:49,186 step [ 458], lr [0.0001500], embedding loss [ 0.8228], quantization loss [ 0.0131],  2.85 sec/batch.
2022-10-20 21:53:53,409 step [ 459], lr [0.0001500], embedding loss [ 0.8074], quantization loss [ 0.0132],  2.83 sec/batch.
2022-10-20 21:53:58,358 step [ 460], lr [0.0001500], embedding loss [ 0.8181], quantization loss [ 0.0139],  2.84 sec/batch.
2022-10-20 21:54:02,832 step [ 461], lr [0.0001500], embedding loss [ 0.8277], quantization loss [ 0.0155],  3.05 sec/batch.
2022-10-20 21:54:07,842 step [ 462], lr [0.0001500], embedding loss [ 0.8217], quantization loss [ 0.0123],  2.90 sec/batch.
2022-10-20 21:54:12,111 step [ 463], lr [0.0001500], embedding loss [ 0.8233], quantization loss [ 0.0130],  2.86 sec/batch.
2022-10-20 21:54:17,368 step [ 464], lr [0.0001500], embedding loss [ 0.8177], quantization loss [ 0.0135],  3.07 sec/batch.
2022-10-20 21:54:21,684 step [ 465], lr [0.0001500], embedding loss [ 0.8187], quantization loss [ 0.0140],  2.18 sec/batch.
2022-10-20 21:54:26,969 step [ 466], lr [0.0001500], embedding loss [ 0.8170], quantization loss [ 0.0125],  3.06 sec/batch.
2022-10-20 21:54:32,064 step [ 467], lr [0.0001500], embedding loss [ 0.8207], quantization loss [ 0.0133],  2.93 sec/batch.
2022-10-20 21:54:36,514 step [ 468], lr [0.0001500], embedding loss [ 0.8159], quantization loss [ 0.0132],  2.99 sec/batch.
2022-10-20 21:54:41,542 step [ 469], lr [0.0001500], embedding loss [ 0.8214], quantization loss [ 0.0133],  2.86 sec/batch.
2022-10-20 21:54:45,961 step [ 470], lr [0.0001500], embedding loss [ 0.8171], quantization loss [ 0.0126],  3.03 sec/batch.
2022-10-20 21:54:51,075 step [ 471], lr [0.0001500], embedding loss [ 0.8251], quantization loss [ 0.0136],  2.93 sec/batch.
2022-10-20 21:54:55,420 step [ 472], lr [0.0001500], embedding loss [ 0.8171], quantization loss [ 0.0128],  2.17 sec/batch.
2022-10-20 21:55:00,489 step [ 473], lr [0.0001500], embedding loss [ 0.8174], quantization loss [ 0.0141],  2.86 sec/batch.
2022-10-20 21:55:05,391 step [ 474], lr [0.0001500], embedding loss [ 0.8194], quantization loss [ 0.0124],  2.83 sec/batch.
2022-10-20 21:55:09,808 step [ 475], lr [0.0001500], embedding loss [ 0.8186], quantization loss [ 0.0124],  3.04 sec/batch.
2022-10-20 21:55:14,937 step [ 476], lr [0.0001500], embedding loss [ 0.8123], quantization loss [ 0.0145],  2.94 sec/batch.
2022-10-20 21:55:19,424 step [ 477], lr [0.0001500], embedding loss [ 0.8132], quantization loss [ 0.0126],  3.05 sec/batch.
2022-10-20 21:55:24,576 step [ 478], lr [0.0001500], embedding loss [ 0.8208], quantization loss [ 0.0138],  2.96 sec/batch.
2022-10-20 21:55:28,882 step [ 479], lr [0.0001500], embedding loss [ 0.8132], quantization loss [ 0.0118],  2.89 sec/batch.
2022-10-20 21:55:34,074 step [ 480], lr [0.0001500], embedding loss [ 0.8140], quantization loss [ 0.0117],  3.07 sec/batch.
2022-10-20 21:55:39,386 step [ 481], lr [0.0001500], embedding loss [ 0.8286], quantization loss [ 0.0126],  3.09 sec/batch.
2022-10-20 21:55:43,669 step [ 482], lr [0.0001500], embedding loss [ 0.8199], quantization loss [ 0.0127],  2.87 sec/batch.
2022-10-20 21:55:48,710 step [ 483], lr [0.0001500], embedding loss [ 0.8212], quantization loss [ 0.0128],  2.83 sec/batch.
2022-10-20 21:55:53,169 step [ 484], lr [0.0001500], embedding loss [ 0.8237], quantization loss [ 0.0125],  3.01 sec/batch.
2022-10-20 21:55:58,220 step [ 485], lr [0.0001500], embedding loss [ 0.8235], quantization loss [ 0.0121],  2.89 sec/batch.
2022-10-20 21:56:03,341 step [ 486], lr [0.0001500], embedding loss [ 0.8228], quantization loss [ 0.0128],  2.96 sec/batch.
2022-10-20 21:56:07,541 step [ 487], lr [0.0001500], embedding loss [ 0.8288], quantization loss [ 0.0132],  2.83 sec/batch.
2022-10-20 21:56:12,595 step [ 488], lr [0.0001500], embedding loss [ 0.8239], quantization loss [ 0.0127],  2.86 sec/batch.
2022-10-20 21:56:16,849 step [ 489], lr [0.0001500], embedding loss [ 0.8295], quantization loss [ 0.0128],  2.83 sec/batch.
2022-10-20 21:56:21,820 step [ 490], lr [0.0001500], embedding loss [ 0.8234], quantization loss [ 0.0128],  2.87 sec/batch.
2022-10-20 21:56:27,070 step [ 491], lr [0.0001500], embedding loss [ 0.8143], quantization loss [ 0.0135],  3.05 sec/batch.
2022-10-20 21:56:31,473 step [ 492], lr [0.0001500], embedding loss [ 0.8141], quantization loss [ 0.0129],  2.94 sec/batch.
2022-10-20 21:56:36,608 step [ 493], lr [0.0001500], embedding loss [ 0.8148], quantization loss [ 0.0134],  2.90 sec/batch.
2022-10-20 21:56:40,923 step [ 494], lr [0.0001500], embedding loss [ 0.8283], quantization loss [ 0.0137],  2.87 sec/batch.
2022-10-20 21:56:46,202 step [ 495], lr [0.0001500], embedding loss [ 0.8204], quantization loss [ 0.0120],  3.06 sec/batch.
2022-10-20 21:56:50,791 step [ 496], lr [0.0001500], embedding loss [ 0.8265], quantization loss [ 0.0130],  2.34 sec/batch.
2022-10-20 21:56:55,910 step [ 497], lr [0.0001500], embedding loss [ 0.8294], quantization loss [ 0.0132],  2.96 sec/batch.
2022-10-20 21:57:01,096 step [ 498], lr [0.0001500], embedding loss [ 0.8327], quantization loss [ 0.0134],  2.98 sec/batch.
2022-10-20 21:57:05,611 step [ 499], lr [0.0001500], embedding loss [ 0.8186], quantization loss [ 0.0121],  3.09 sec/batch.
2022-10-20 21:57:11,067 step [ 500], lr [0.0001500], embedding loss [ 0.8230], quantization loss [ 0.0132],  3.18 sec/batch.
2022-10-20 21:57:15,576 step [ 501], lr [0.0001500], embedding loss [ 0.8302], quantization loss [ 0.0133],  3.08 sec/batch.
2022-10-20 21:57:21,411 step [ 502], lr [0.0001500], embedding loss [ 0.8216], quantization loss [ 0.0128],  3.60 sec/batch.
2022-10-20 21:57:26,969 step [ 503], lr [0.0001500], embedding loss [ 0.8206], quantization loss [ 0.0129],  3.30 sec/batch.
2022-10-20 21:57:31,938 step [ 504], lr [0.0001500], embedding loss [ 0.8281], quantization loss [ 0.0136],  3.51 sec/batch.
2022-10-20 21:57:37,729 step [ 505], lr [0.0001500], embedding loss [ 0.8236], quantization loss [ 0.0131],  3.46 sec/batch.
2022-10-20 21:57:42,651 step [ 506], lr [0.0001500], embedding loss [ 0.8125], quantization loss [ 0.0138],  2.60 sec/batch.
2022-10-20 21:57:48,362 step [ 507], lr [0.0001500], embedding loss [ 0.8227], quantization loss [ 0.0130],  3.41 sec/batch.
2022-10-20 21:57:54,125 step [ 508], lr [0.0001500], embedding loss [ 0.8195], quantization loss [ 0.0127],  3.46 sec/batch.
2022-10-20 21:57:59,150 step [ 509], lr [0.0001500], embedding loss [ 0.8202], quantization loss [ 0.0126],  3.58 sec/batch.
2022-10-20 21:58:04,847 step [ 510], lr [0.0001500], embedding loss [ 0.8222], quantization loss [ 0.0131],  3.32 sec/batch.
2022-10-20 21:58:09,514 step [ 511], lr [0.0001500], embedding loss [ 0.8248], quantization loss [ 0.0129],  3.30 sec/batch.
2022-10-20 21:58:15,024 step [ 512], lr [0.0001500], embedding loss [ 0.8221], quantization loss [ 0.0134],  3.22 sec/batch.
2022-10-20 21:58:20,508 step [ 513], lr [0.0001500], embedding loss [ 0.8165], quantization loss [ 0.0138],  3.20 sec/batch.
2022-10-20 21:58:25,143 step [ 514], lr [0.0001500], embedding loss [ 0.8272], quantization loss [ 0.0126],  3.20 sec/batch.
2022-10-20 21:58:30,740 step [ 515], lr [0.0001500], embedding loss [ 0.8165], quantization loss [ 0.0125],  3.32 sec/batch.
2022-10-20 21:58:35,588 step [ 516], lr [0.0001500], embedding loss [ 0.8231], quantization loss [ 0.0120],  3.33 sec/batch.
2022-10-20 21:58:41,196 step [ 517], lr [0.0001500], embedding loss [ 0.8247], quantization loss [ 0.0127],  3.36 sec/batch.
2022-10-20 21:58:46,505 step [ 518], lr [0.0001500], embedding loss [ 0.8266], quantization loss [ 0.0128],  3.20 sec/batch.
2022-10-20 21:58:51,331 step [ 519], lr [0.0001500], embedding loss [ 0.8252], quantization loss [ 0.0118],  3.36 sec/batch.
2022-10-20 21:58:56,856 step [ 520], lr [0.0001500], embedding loss [ 0.8154], quantization loss [ 0.0120],  3.23 sec/batch.
2022-10-20 21:59:01,481 step [ 521], lr [0.0001500], embedding loss [ 0.8200], quantization loss [ 0.0135],  3.18 sec/batch.
2022-10-20 21:59:07,129 step [ 522], lr [0.0001500], embedding loss [ 0.8133], quantization loss [ 0.0129],  3.23 sec/batch.
2022-10-20 21:59:12,924 step [ 523], lr [0.0001500], embedding loss [ 0.8182], quantization loss [ 0.0123],  3.43 sec/batch.
2022-10-20 21:59:17,513 step [ 524], lr [0.0001500], embedding loss [ 0.8215], quantization loss [ 0.0134],  3.16 sec/batch.
2022-10-20 21:59:23,007 step [ 525], lr [0.0001500], embedding loss [ 0.8302], quantization loss [ 0.0130],  3.19 sec/batch.
2022-10-20 21:59:27,865 step [ 526], lr [0.0001500], embedding loss [ 0.8121], quantization loss [ 0.0126],  3.43 sec/batch.
2022-10-20 21:59:33,395 step [ 527], lr [0.0001500], embedding loss [ 0.8165], quantization loss [ 0.0132],  3.29 sec/batch.
2022-10-20 21:59:38,964 step [ 528], lr [0.0001500], embedding loss [ 0.8202], quantization loss [ 0.0125],  3.32 sec/batch.
2022-10-20 21:59:43,738 step [ 529], lr [0.0001500], embedding loss [ 0.8166], quantization loss [ 0.0127],  3.35 sec/batch.
2022-10-20 21:59:49,544 step [ 530], lr [0.0001500], embedding loss [ 0.8179], quantization loss [ 0.0123],  3.50 sec/batch.
2022-10-20 21:59:54,501 step [ 531], lr [0.0001500], embedding loss [ 0.8146], quantization loss [ 0.0121],  3.51 sec/batch.
2022-10-20 22:00:00,454 step [ 532], lr [0.0001500], embedding loss [ 0.8235], quantization loss [ 0.0121],  3.53 sec/batch.
2022-10-20 22:00:06,069 step [ 533], lr [0.0001500], embedding loss [ 0.8277], quantization loss [ 0.0127],  3.29 sec/batch.
2022-10-20 22:00:11,018 step [ 534], lr [0.0001500], embedding loss [ 0.8283], quantization loss [ 0.0117],  3.49 sec/batch.
2022-10-20 22:00:16,761 step [ 535], lr [0.0001500], embedding loss [ 0.8263], quantization loss [ 0.0135],  3.38 sec/batch.
2022-10-20 22:00:22,643 step [ 536], lr [0.0001500], embedding loss [ 0.8218], quantization loss [ 0.0140],  3.48 sec/batch.
2022-10-20 22:00:27,394 step [ 537], lr [0.0001500], embedding loss [ 0.8270], quantization loss [ 0.0141],  3.30 sec/batch.
2022-10-20 22:00:33,023 step [ 538], lr [0.0001500], embedding loss [ 0.8177], quantization loss [ 0.0128],  3.29 sec/batch.
2022-10-20 22:00:37,579 step [ 539], lr [0.0001500], embedding loss [ 0.8185], quantization loss [ 0.0127],  3.19 sec/batch.
2022-10-20 22:00:43,137 step [ 540], lr [0.0001500], embedding loss [ 0.8210], quantization loss [ 0.0131],  3.26 sec/batch.
2022-10-20 22:00:48,644 step [ 541], lr [0.0001500], embedding loss [ 0.8182], quantization loss [ 0.0118],  3.23 sec/batch.
2022-10-20 22:00:54,683 step [ 542], lr [0.0001500], embedding loss [ 0.8225], quantization loss [ 0.0124],  4.62 sec/batch.
2022-10-20 22:00:57,497 step [ 543], lr [0.0001500], embedding loss [ 0.8135], quantization loss [ 0.0127],  0.59 sec/batch.
2022-10-20 22:01:03,242 step [ 544], lr [0.0001500], embedding loss [ 0.8163], quantization loss [ 0.0122],  3.20 sec/batch.
2022-10-20 22:01:07,892 step [ 545], lr [0.0001500], embedding loss [ 0.8204], quantization loss [ 0.0124],  3.22 sec/batch.
2022-10-20 22:01:13,593 step [ 546], lr [0.0001500], embedding loss [ 0.8174], quantization loss [ 0.0134],  3.32 sec/batch.
2022-10-20 22:01:19,266 step [ 547], lr [0.0001500], embedding loss [ 0.8163], quantization loss [ 0.0126],  3.36 sec/batch.
2022-10-20 22:01:24,017 step [ 548], lr [0.0001500], embedding loss [ 0.8190], quantization loss [ 0.0136],  3.29 sec/batch.
2022-10-20 22:01:29,485 step [ 549], lr [0.0001500], embedding loss [ 0.8167], quantization loss [ 0.0123],  3.23 sec/batch.
2022-10-20 22:01:34,411 step [ 550], lr [0.0001500], embedding loss [ 0.8192], quantization loss [ 0.0127],  2.53 sec/batch.
2022-10-20 22:01:40,074 step [ 551], lr [0.0001500], embedding loss [ 0.8273], quantization loss [ 0.0120],  3.27 sec/batch.
2022-10-20 22:01:45,692 step [ 552], lr [0.0001500], embedding loss [ 0.8192], quantization loss [ 0.0134],  3.30 sec/batch.
2022-10-20 22:01:50,547 step [ 553], lr [0.0001500], embedding loss [ 0.8211], quantization loss [ 0.0122],  3.41 sec/batch.
2022-10-20 22:01:56,241 step [ 554], lr [0.0001500], embedding loss [ 0.8263], quantization loss [ 0.0134],  3.39 sec/batch.
2022-10-20 22:02:01,870 step [ 555], lr [0.0001500], embedding loss [ 0.8249], quantization loss [ 0.0130],  3.35 sec/batch.
2022-10-20 22:02:07,859 step [ 556], lr [0.0001500], embedding loss [ 0.8186], quantization loss [ 0.0131],  4.59 sec/batch.
2022-10-20 22:02:13,100 step [ 557], lr [0.0001500], embedding loss [ 0.8174], quantization loss [ 0.0122],  3.70 sec/batch.
2022-10-20 22:02:17,859 step [ 558], lr [0.0001500], embedding loss [ 0.8289], quantization loss [ 0.0124],  3.15 sec/batch.
2022-10-20 22:02:23,556 step [ 559], lr [0.0001500], embedding loss [ 0.8178], quantization loss [ 0.0128],  3.21 sec/batch.
2022-10-20 22:02:29,084 step [ 560], lr [0.0001500], embedding loss [ 0.8242], quantization loss [ 0.0125],  3.22 sec/batch.
2022-10-20 22:02:33,633 step [ 561], lr [0.0001500], embedding loss [ 0.8152], quantization loss [ 0.0134],  3.15 sec/batch.
2022-10-20 22:02:39,393 step [ 562], lr [0.0001500], embedding loss [ 0.8200], quantization loss [ 0.0122],  3.36 sec/batch.
2022-10-20 22:02:44,168 step [ 563], lr [0.0001500], embedding loss [ 0.8163], quantization loss [ 0.0123],  3.28 sec/batch.
2022-10-20 22:02:49,916 step [ 564], lr [0.0001500], embedding loss [ 0.8140], quantization loss [ 0.0115],  3.43 sec/batch.
2022-10-20 22:02:55,574 step [ 565], lr [0.0001500], embedding loss [ 0.8159], quantization loss [ 0.0128],  3.31 sec/batch.
2022-10-20 22:03:00,472 step [ 566], lr [0.0001500], embedding loss [ 0.8268], quantization loss [ 0.0136],  3.49 sec/batch.
2022-10-20 22:03:06,143 step [ 567], lr [0.0001500], embedding loss [ 0.8176], quantization loss [ 0.0124],  3.33 sec/batch.
2022-10-20 22:03:11,633 step [ 568], lr [0.0001500], embedding loss [ 0.8232], quantization loss [ 0.0138],  3.24 sec/batch.
2022-10-20 22:03:16,340 step [ 569], lr [0.0001500], embedding loss [ 0.8147], quantization loss [ 0.0127],  3.26 sec/batch.
2022-10-20 22:03:20,326 step [ 570], lr [0.0001500], embedding loss [ 0.8276], quantization loss [ 0.0120],  1.65 sec/batch.
2022-10-20 22:03:25,120 step [ 571], lr [0.0001500], embedding loss [ 0.8209], quantization loss [ 0.0125],  3.31 sec/batch.
2022-10-20 22:03:31,328 step [ 572], lr [0.0001500], embedding loss [ 0.8219], quantization loss [ 0.0107],  3.87 sec/batch.
2022-10-20 22:03:37,257 step [ 573], lr [0.0001500], embedding loss [ 0.8134], quantization loss [ 0.0126],  3.49 sec/batch.
2022-10-20 22:03:43,051 step [ 574], lr [0.0001500], embedding loss [ 0.8230], quantization loss [ 0.0132],  3.43 sec/batch.
2022-10-20 22:03:47,826 step [ 575], lr [0.0001500], embedding loss [ 0.8239], quantization loss [ 0.0126],  3.37 sec/batch.
2022-10-20 22:03:53,373 step [ 576], lr [0.0001500], embedding loss [ 0.8267], quantization loss [ 0.0118],  3.21 sec/batch.
2022-10-20 22:03:58,746 step [ 577], lr [0.0001500], embedding loss [ 0.8305], quantization loss [ 0.0129],  3.17 sec/batch.
2022-10-20 22:04:04,515 step [ 578], lr [0.0001500], embedding loss [ 0.8233], quantization loss [ 0.0134],  3.37 sec/batch.
2022-10-20 22:04:09,412 step [ 579], lr [0.0001500], embedding loss [ 0.8268], quantization loss [ 0.0121],  3.44 sec/batch.
2022-10-20 22:04:15,076 step [ 580], lr [0.0001500], embedding loss [ 0.8229], quantization loss [ 0.0115],  3.33 sec/batch.
2022-10-20 22:04:20,708 step [ 581], lr [0.0001500], embedding loss [ 0.8198], quantization loss [ 0.0120],  3.24 sec/batch.
2022-10-20 22:04:26,445 step [ 582], lr [0.0001500], embedding loss [ 0.8149], quantization loss [ 0.0116],  3.41 sec/batch.
2022-10-20 22:04:31,205 step [ 583], lr [0.0001500], embedding loss [ 0.8288], quantization loss [ 0.0127],  3.29 sec/batch.
2022-10-20 22:04:36,923 step [ 584], lr [0.0001500], embedding loss [ 0.8158], quantization loss [ 0.0123],  3.38 sec/batch.
2022-10-20 22:04:42,392 step [ 585], lr [0.0001500], embedding loss [ 0.8182], quantization loss [ 0.0124],  3.20 sec/batch.
2022-10-20 22:04:42,392 update codes and centers iter(1/1).
2022-10-20 22:04:50,202 number of update_code wrong: 0.
2022-10-20 22:04:53,008 non zero codewords: 1024.
2022-10-20 22:04:53,008 finish center update, duration: 10.62 sec.
2022-10-20 22:04:58,342 step [ 586], lr [0.0001500], embedding loss [ 0.8236], quantization loss [ 0.0117],  3.17 sec/batch.
2022-10-20 22:05:02,991 step [ 587], lr [0.0001500], embedding loss [ 0.8180], quantization loss [ 0.0116],  3.23 sec/batch.
2022-10-20 22:05:08,797 step [ 588], lr [0.0001500], embedding loss [ 0.8195], quantization loss [ 0.0112],  3.53 sec/batch.
2022-10-20 22:05:14,283 step [ 589], lr [0.0001500], embedding loss [ 0.8200], quantization loss [ 0.0114],  3.23 sec/batch.
2022-10-20 22:05:19,828 step [ 590], lr [0.0001500], embedding loss [ 0.8210], quantization loss [ 0.0117],  3.22 sec/batch.
2022-10-20 22:05:24,547 step [ 591], lr [0.0001500], embedding loss [ 0.8153], quantization loss [ 0.0113],  3.25 sec/batch.
2022-10-20 22:05:30,093 step [ 592], lr [0.0001500], embedding loss [ 0.8117], quantization loss [ 0.0116],  3.22 sec/batch.
2022-10-20 22:05:35,723 step [ 593], lr [0.0001500], embedding loss [ 0.8211], quantization loss [ 0.0112],  3.25 sec/batch.
2022-10-20 22:05:41,180 step [ 594], lr [0.0001500], embedding loss [ 0.8139], quantization loss [ 0.0116],  3.17 sec/batch.
2022-10-20 22:05:46,991 step [ 595], lr [0.0001500], embedding loss [ 0.8199], quantization loss [ 0.0117],  4.36 sec/batch.
2022-10-20 22:05:51,906 step [ 596], lr [0.0001500], embedding loss [ 0.8206], quantization loss [ 0.0116],  3.46 sec/batch.
2022-10-20 22:05:57,859 step [ 597], lr [0.0001500], embedding loss [ 0.8101], quantization loss [ 0.0109],  3.63 sec/batch.
2022-10-20 22:06:04,566 step [ 598], lr [0.0001500], embedding loss [ 0.8209], quantization loss [ 0.0112],  3.48 sec/batch.
2022-10-20 22:06:10,285 step [ 599], lr [0.0001500], embedding loss [ 0.8207], quantization loss [ 0.0108],  3.39 sec/batch.
2022-10-20 22:06:15,003 step [ 600], lr [0.0001500], embedding loss [ 0.8227], quantization loss [ 0.0124],  3.28 sec/batch.
2022-10-20 22:06:20,484 step [ 601], lr [0.0000750], embedding loss [ 0.8226], quantization loss [ 0.0105],  3.24 sec/batch.
2022-10-20 22:06:26,144 step [ 602], lr [0.0000750], embedding loss [ 0.8228], quantization loss [ 0.0103],  3.36 sec/batch.
2022-10-20 22:06:31,573 step [ 603], lr [0.0000750], embedding loss [ 0.8168], quantization loss [ 0.0101],  3.15 sec/batch.
2022-10-20 22:06:36,272 step [ 604], lr [0.0000750], embedding loss [ 0.8136], quantization loss [ 0.0118],  3.24 sec/batch.
2022-10-20 22:06:42,130 step [ 605], lr [0.0000750], embedding loss [ 0.8241], quantization loss [ 0.0116],  3.48 sec/batch.
2022-10-20 22:06:47,663 step [ 606], lr [0.0000750], embedding loss [ 0.8240], quantization loss [ 0.0113],  3.25 sec/batch.
2022-10-20 22:06:53,124 step [ 607], lr [0.0000750], embedding loss [ 0.8182], quantization loss [ 0.0124],  3.19 sec/batch.
2022-10-20 22:06:57,785 step [ 608], lr [0.0000750], embedding loss [ 0.8281], quantization loss [ 0.0112],  3.21 sec/batch.
2022-10-20 22:07:03,157 step [ 609], lr [0.0000750], embedding loss [ 0.8191], quantization loss [ 0.0117],  3.19 sec/batch.
2022-10-20 22:07:08,595 step [ 610], lr [0.0000750], embedding loss [ 0.8287], quantization loss [ 0.0107],  3.18 sec/batch.
2022-10-20 22:07:13,968 step [ 611], lr [0.0000750], embedding loss [ 0.8227], quantization loss [ 0.0119],  3.18 sec/batch.
2022-10-20 22:07:18,629 step [ 612], lr [0.0000750], embedding loss [ 0.8295], quantization loss [ 0.0110],  3.21 sec/batch.
2022-10-20 22:07:24,199 step [ 613], lr [0.0000750], embedding loss [ 0.8229], quantization loss [ 0.0113],  3.27 sec/batch.
2022-10-20 22:07:29,933 step [ 614], lr [0.0000750], embedding loss [ 0.8097], quantization loss [ 0.0118],  3.40 sec/batch.
2022-10-20 22:07:35,600 step [ 615], lr [0.0000750], embedding loss [ 0.8187], quantization loss [ 0.0107],  3.32 sec/batch.
2022-10-20 22:07:40,422 step [ 616], lr [0.0000750], embedding loss [ 0.8142], quantization loss [ 0.0123],  3.35 sec/batch.
2022-10-20 22:07:46,040 step [ 617], lr [0.0000750], embedding loss [ 0.8217], quantization loss [ 0.0110],  3.26 sec/batch.
2022-10-20 22:07:51,788 step [ 618], lr [0.0000750], embedding loss [ 0.8206], quantization loss [ 0.0107],  3.40 sec/batch.
2022-10-20 22:07:57,249 step [ 619], lr [0.0000750], embedding loss [ 0.8253], quantization loss [ 0.0117],  3.17 sec/batch.
2022-10-20 22:08:02,247 step [ 620], lr [0.0000750], embedding loss [ 0.8258], quantization loss [ 0.0101],  3.38 sec/batch.
2022-10-20 22:08:07,886 step [ 621], lr [0.0000750], embedding loss [ 0.8225], quantization loss [ 0.0115],  3.28 sec/batch.
2022-10-20 22:08:13,539 step [ 622], lr [0.0000750], embedding loss [ 0.8174], quantization loss [ 0.0108],  3.34 sec/batch.
2022-10-20 22:08:19,175 step [ 623], lr [0.0000750], embedding loss [ 0.8265], quantization loss [ 0.0103],  3.29 sec/batch.
2022-10-20 22:08:25,356 step [ 624], lr [0.0000750], embedding loss [ 0.8271], quantization loss [ 0.0116],  4.71 sec/batch.
2022-10-20 22:08:29,967 step [ 625], lr [0.0000750], embedding loss [ 0.8192], quantization loss [ 0.0105],  3.23 sec/batch.
2022-10-20 22:08:35,588 step [ 626], lr [0.0000750], embedding loss [ 0.8245], quantization loss [ 0.0107],  3.27 sec/batch.
2022-10-20 22:08:41,101 step [ 627], lr [0.0000750], embedding loss [ 0.8166], quantization loss [ 0.0113],  3.23 sec/batch.
2022-10-20 22:08:46,550 step [ 628], lr [0.0000750], embedding loss [ 0.8218], quantization loss [ 0.0107],  3.19 sec/batch.
2022-10-20 22:08:51,443 step [ 629], lr [0.0000750], embedding loss [ 0.8179], quantization loss [ 0.0108],  3.44 sec/batch.
2022-10-20 22:08:57,175 step [ 630], lr [0.0000750], embedding loss [ 0.8197], quantization loss [ 0.0103],  3.38 sec/batch.
2022-10-20 22:09:02,581 step [ 631], lr [0.0000750], embedding loss [ 0.8146], quantization loss [ 0.0110],  3.17 sec/batch.
2022-10-20 22:09:08,175 step [ 632], lr [0.0000750], embedding loss [ 0.8244], quantization loss [ 0.0106],  3.27 sec/batch.
2022-10-20 22:09:12,691 step [ 633], lr [0.0000750], embedding loss [ 0.8309], quantization loss [ 0.0105],  3.14 sec/batch.
2022-10-20 22:09:18,397 step [ 634], lr [0.0000750], embedding loss [ 0.8227], quantization loss [ 0.0108],  3.36 sec/batch.
2022-10-20 22:09:24,140 step [ 635], lr [0.0000750], embedding loss [ 0.8200], quantization loss [ 0.0113],  3.38 sec/batch.
2022-10-20 22:09:31,051 step [ 636], lr [0.0000750], embedding loss [ 0.8241], quantization loss [ 0.0111],  4.57 sec/batch.
2022-10-20 22:09:35,936 step [ 637], lr [0.0000750], embedding loss [ 0.8151], quantization loss [ 0.0112],  3.40 sec/batch.
2022-10-20 22:09:43,022 step [ 638], lr [0.0000750], embedding loss [ 0.8229], quantization loss [ 0.0111],  4.71 sec/batch.
2022-10-20 22:09:47,891 step [ 639], lr [0.0000750], embedding loss [ 0.8212], quantization loss [ 0.0113],  3.39 sec/batch.
2022-10-20 22:09:53,397 step [ 640], lr [0.0000750], embedding loss [ 0.8267], quantization loss [ 0.0100],  3.22 sec/batch.
2022-10-20 22:09:58,900 step [ 641], lr [0.0000750], embedding loss [ 0.8285], quantization loss [ 0.0103],  3.21 sec/batch.
2022-10-20 22:10:04,475 step [ 642], lr [0.0000750], embedding loss [ 0.8234], quantization loss [ 0.0113],  3.19 sec/batch.
2022-10-20 22:10:09,177 step [ 643], lr [0.0000750], embedding loss [ 0.8235], quantization loss [ 0.0107],  3.21 sec/batch.
2022-10-20 22:10:14,742 step [ 644], lr [0.0000750], embedding loss [ 0.8217], quantization loss [ 0.0110],  3.22 sec/batch.
2022-10-20 22:10:20,403 step [ 645], lr [0.0000750], embedding loss [ 0.8278], quantization loss [ 0.0110],  3.26 sec/batch.
2022-10-20 22:10:25,941 step [ 646], lr [0.0000750], embedding loss [ 0.8141], quantization loss [ 0.0109],  3.23 sec/batch.
2022-10-20 22:10:30,722 step [ 647], lr [0.0000750], embedding loss [ 0.8121], quantization loss [ 0.0108],  3.31 sec/batch.
2022-10-20 22:10:36,324 step [ 648], lr [0.0000750], embedding loss [ 0.8181], quantization loss [ 0.0110],  3.25 sec/batch.
2022-10-20 22:10:42,052 step [ 649], lr [0.0000750], embedding loss [ 0.8210], quantization loss [ 0.0115],  3.39 sec/batch.
2022-10-20 22:10:49,283 step [ 650], lr [0.0000750], embedding loss [ 0.8151], quantization loss [ 0.0110],  3.55 sec/batch.
2022-10-20 22:10:53,860 step [ 651], lr [0.0000750], embedding loss [ 0.8195], quantization loss [ 0.0105],  3.21 sec/batch.
2022-10-20 22:10:59,514 step [ 652], lr [0.0000750], embedding loss [ 0.8216], quantization loss [ 0.0113],  3.29 sec/batch.
2022-10-20 22:11:05,261 step [ 653], lr [0.0000750], embedding loss [ 0.8224], quantization loss [ 0.0104],  3.41 sec/batch.
2022-10-20 22:11:10,732 step [ 654], lr [0.0000750], embedding loss [ 0.8092], quantization loss [ 0.0112],  3.19 sec/batch.
2022-10-20 22:11:15,718 step [ 655], lr [0.0000750], embedding loss [ 0.8220], quantization loss [ 0.0103],  3.45 sec/batch.
2022-10-20 22:11:21,496 step [ 656], lr [0.0000750], embedding loss [ 0.8225], quantization loss [ 0.0105],  3.44 sec/batch.
2022-10-20 22:11:27,161 step [ 657], lr [0.0000750], embedding loss [ 0.8170], quantization loss [ 0.0103],  3.30 sec/batch.
2022-10-20 22:11:32,601 step [ 658], lr [0.0000750], embedding loss [ 0.8151], quantization loss [ 0.0101],  3.17 sec/batch.
2022-10-20 22:11:37,237 step [ 659], lr [0.0000750], embedding loss [ 0.8140], quantization loss [ 0.0105],  3.18 sec/batch.
2022-10-20 22:11:42,726 step [ 660], lr [0.0000750], embedding loss [ 0.8215], quantization loss [ 0.0108],  3.22 sec/batch.
2022-10-20 22:11:48,319 step [ 661], lr [0.0000750], embedding loss [ 0.8141], quantization loss [ 0.0107],  3.21 sec/batch.
2022-10-20 22:11:53,665 step [ 662], lr [0.0000750], embedding loss [ 0.8227], quantization loss [ 0.0107],  3.19 sec/batch.
2022-10-20 22:11:59,858 step [ 663], lr [0.0000750], embedding loss [ 0.8147], quantization loss [ 0.0112],  4.72 sec/batch.
2022-10-20 22:12:04,682 step [ 664], lr [0.0000750], embedding loss [ 0.8190], quantization loss [ 0.0108],  3.39 sec/batch.
2022-10-20 22:12:10,765 step [ 665], lr [0.0000750], embedding loss [ 0.8268], quantization loss [ 0.0106],  3.75 sec/batch.
2022-10-20 22:12:17,199 step [ 666], lr [0.0000750], embedding loss [ 0.8252], quantization loss [ 0.0114],  3.17 sec/batch.
2022-10-20 22:12:22,728 step [ 667], lr [0.0000750], embedding loss [ 0.8256], quantization loss [ 0.0107],  3.25 sec/batch.
2022-10-20 22:12:27,462 step [ 668], lr [0.0000750], embedding loss [ 0.8248], quantization loss [ 0.0111],  3.24 sec/batch.
2022-10-20 22:12:33,046 step [ 669], lr [0.0000750], embedding loss [ 0.8160], quantization loss [ 0.0113],  3.21 sec/batch.
2022-10-20 22:12:38,718 step [ 670], lr [0.0000750], embedding loss [ 0.8191], quantization loss [ 0.0101],  3.30 sec/batch.
2022-10-20 22:12:44,076 step [ 671], lr [0.0000750], embedding loss [ 0.8137], quantization loss [ 0.0095],  3.19 sec/batch.
2022-10-20 22:12:48,714 step [ 672], lr [0.0000750], embedding loss [ 0.8274], quantization loss [ 0.0109],  3.17 sec/batch.
2022-10-20 22:12:54,429 step [ 673], lr [0.0000750], embedding loss [ 0.8257], quantization loss [ 0.0112],  3.40 sec/batch.
2022-10-20 22:13:00,155 step [ 674], lr [0.0000750], embedding loss [ 0.8234], quantization loss [ 0.0108],  3.36 sec/batch.
2022-10-20 22:13:05,951 step [ 675], lr [0.0000750], embedding loss [ 0.8193], quantization loss [ 0.0106],  3.44 sec/batch.
2022-10-20 22:13:10,843 step [ 676], lr [0.0000750], embedding loss [ 0.8164], quantization loss [ 0.0101],  3.38 sec/batch.
2022-10-20 22:13:17,781 step [ 677], lr [0.0000750], embedding loss [ 0.8150], quantization loss [ 0.0110],  4.68 sec/batch.
2022-10-20 22:13:23,561 step [ 678], lr [0.0000750], embedding loss [ 0.8273], quantization loss [ 0.0108],  3.41 sec/batch.
2022-10-20 22:13:30,573 step [ 679], lr [0.0000750], embedding loss [ 0.8244], quantization loss [ 0.0098],  4.62 sec/batch.
2022-10-20 22:13:35,450 step [ 680], lr [0.0000750], embedding loss [ 0.8282], quantization loss [ 0.0104],  3.35 sec/batch.
2022-10-20 22:13:41,373 step [ 681], lr [0.0000750], embedding loss [ 0.8246], quantization loss [ 0.0105],  3.47 sec/batch.
2022-10-20 22:13:46,920 step [ 682], lr [0.0000750], embedding loss [ 0.8232], quantization loss [ 0.0098],  3.18 sec/batch.
2022-10-20 22:13:52,940 step [ 683], lr [0.0000750], embedding loss [ 0.8149], quantization loss [ 0.0105],  3.58 sec/batch.
2022-10-20 22:13:57,987 step [ 684], lr [0.0000750], embedding loss [ 0.8188], quantization loss [ 0.0104],  3.56 sec/batch.
2022-10-20 22:14:04,018 step [ 685], lr [0.0000750], embedding loss [ 0.8315], quantization loss [ 0.0102],  3.62 sec/batch.
2022-10-20 22:14:09,760 step [ 686], lr [0.0000750], embedding loss [ 0.8158], quantization loss [ 0.0106],  3.37 sec/batch.
2022-10-20 22:14:15,239 step [ 687], lr [0.0000750], embedding loss [ 0.8241], quantization loss [ 0.0109],  3.25 sec/batch.
2022-10-20 22:14:20,141 step [ 688], lr [0.0000750], embedding loss [ 0.8122], quantization loss [ 0.0115],  3.44 sec/batch.
2022-10-20 22:14:25,984 step [ 689], lr [0.0000750], embedding loss [ 0.8228], quantization loss [ 0.0106],  3.49 sec/batch.
2022-10-20 22:14:32,657 step [ 690], lr [0.0000750], embedding loss [ 0.8206], quantization loss [ 0.0110],  4.35 sec/batch.
2022-10-20 22:14:38,307 step [ 691], lr [0.0000750], embedding loss [ 0.8161], quantization loss [ 0.0121],  3.20 sec/batch.
2022-10-20 22:14:43,378 step [ 692], lr [0.0000750], embedding loss [ 0.8238], quantization loss [ 0.0108],  3.48 sec/batch.
2022-10-20 22:14:49,104 step [ 693], lr [0.0000750], embedding loss [ 0.8271], quantization loss [ 0.0107],  3.35 sec/batch.
2022-10-20 22:14:54,644 step [ 694], lr [0.0000750], embedding loss [ 0.8194], quantization loss [ 0.0107],  3.19 sec/batch.
2022-10-20 22:15:00,239 step [ 695], lr [0.0000750], embedding loss [ 0.8167], quantization loss [ 0.0111],  3.27 sec/batch.
2022-10-20 22:15:05,241 step [ 696], lr [0.0000750], embedding loss [ 0.8225], quantization loss [ 0.0108],  3.50 sec/batch.
2022-10-20 22:15:10,645 step [ 697], lr [0.0000750], embedding loss [ 0.8264], quantization loss [ 0.0114],  3.19 sec/batch.
2022-10-20 22:15:16,353 step [ 698], lr [0.0000750], embedding loss [ 0.8248], quantization loss [ 0.0100],  3.33 sec/batch.
2022-10-20 22:15:22,021 step [ 699], lr [0.0000750], embedding loss [ 0.8213], quantization loss [ 0.0112],  3.32 sec/batch.
2022-10-20 22:15:26,764 step [ 700], lr [0.0000750], embedding loss [ 0.8181], quantization loss [ 0.0119],  3.22 sec/batch.
2022-10-20 22:15:32,410 step [ 701], lr [0.0000750], embedding loss [ 0.8137], quantization loss [ 0.0106],  3.24 sec/batch.
2022-10-20 22:15:38,161 step [ 702], lr [0.0000750], embedding loss [ 0.8273], quantization loss [ 0.0109],  3.36 sec/batch.
2022-10-20 22:15:44,026 step [ 703], lr [0.0000750], embedding loss [ 0.8297], quantization loss [ 0.0095],  3.50 sec/batch.
2022-10-20 22:15:49,132 step [ 704], lr [0.0000750], embedding loss [ 0.8225], quantization loss [ 0.0110],  3.53 sec/batch.
2022-10-20 22:15:55,110 step [ 705], lr [0.0000750], embedding loss [ 0.8131], quantization loss [ 0.0110],  3.58 sec/batch.
2022-10-20 22:16:01,314 step [ 706], lr [0.0000750], embedding loss [ 0.8246], quantization loss [ 0.0101],  3.37 sec/batch.
2022-10-20 22:16:06,828 step [ 707], lr [0.0000750], embedding loss [ 0.8221], quantization loss [ 0.0107],  3.18 sec/batch.
2022-10-20 22:16:11,578 step [ 708], lr [0.0000750], embedding loss [ 0.8183], quantization loss [ 0.0114],  3.22 sec/batch.
2022-10-20 22:16:17,512 step [ 709], lr [0.0000750], embedding loss [ 0.8153], quantization loss [ 0.0108],  3.55 sec/batch.
2022-10-20 22:16:22,958 step [ 710], lr [0.0000750], embedding loss [ 0.8285], quantization loss [ 0.0107],  3.18 sec/batch.
2022-10-20 22:16:28,736 step [ 711], lr [0.0000750], embedding loss [ 0.8227], quantization loss [ 0.0105],  3.42 sec/batch.
2022-10-20 22:16:33,378 step [ 712], lr [0.0000750], embedding loss [ 0.8193], quantization loss [ 0.0113],  3.20 sec/batch.
2022-10-20 22:16:39,028 step [ 713], lr [0.0000750], embedding loss [ 0.8206], quantization loss [ 0.0103],  3.34 sec/batch.
2022-10-20 22:16:44,662 step [ 714], lr [0.0000750], embedding loss [ 0.8194], quantization loss [ 0.0115],  3.36 sec/batch.
2022-10-20 22:16:50,309 step [ 715], lr [0.0000750], embedding loss [ 0.8198], quantization loss [ 0.0110],  3.26 sec/batch.
2022-10-20 22:16:55,173 step [ 716], lr [0.0000750], embedding loss [ 0.8105], quantization loss [ 0.0102],  3.32 sec/batch.
2022-10-20 22:17:02,292 step [ 717], lr [0.0000750], embedding loss [ 0.8172], quantization loss [ 0.0115],  4.78 sec/batch.
2022-10-20 22:17:08,185 step [ 718], lr [0.0000750], embedding loss [ 0.8206], quantization loss [ 0.0117],  3.49 sec/batch.
2022-10-20 22:17:13,721 step [ 719], lr [0.0000750], embedding loss [ 0.8219], quantization loss [ 0.0106],  3.19 sec/batch.
2022-10-20 22:17:18,615 step [ 720], lr [0.0000750], embedding loss [ 0.8153], quantization loss [ 0.0113],  3.36 sec/batch.
2022-10-20 22:17:24,240 step [ 721], lr [0.0000750], embedding loss [ 0.8139], quantization loss [ 0.0107],  3.24 sec/batch.
2022-10-20 22:17:30,098 step [ 722], lr [0.0000750], embedding loss [ 0.8251], quantization loss [ 0.0112],  3.38 sec/batch.
2022-10-20 22:17:35,193 step [ 723], lr [0.0000750], embedding loss [ 0.8152], quantization loss [ 0.0110],  3.60 sec/batch.
2022-10-20 22:17:41,283 step [ 724], lr [0.0000750], embedding loss [ 0.8271], quantization loss [ 0.0096],  3.64 sec/batch.
2022-10-20 22:17:47,198 step [ 725], lr [0.0000750], embedding loss [ 0.8197], quantization loss [ 0.0105],  3.47 sec/batch.
2022-10-20 22:17:53,166 step [ 726], lr [0.0000750], embedding loss [ 0.8143], quantization loss [ 0.0111],  3.55 sec/batch.
2022-10-20 22:17:58,142 step [ 727], lr [0.0000750], embedding loss [ 0.8284], quantization loss [ 0.0111],  3.49 sec/batch.
2022-10-20 22:18:04,079 step [ 728], lr [0.0000750], embedding loss [ 0.8184], quantization loss [ 0.0108],  3.52 sec/batch.
2022-10-20 22:18:10,126 step [ 729], lr [0.0000750], embedding loss [ 0.8282], quantization loss [ 0.0105],  3.61 sec/batch.
2022-10-20 22:18:16,158 step [ 730], lr [0.0000750], embedding loss [ 0.8207], quantization loss [ 0.0105],  3.63 sec/batch.
2022-10-20 22:18:21,181 step [ 731], lr [0.0000750], embedding loss [ 0.8119], quantization loss [ 0.0113],  3.53 sec/batch.
2022-10-20 22:18:21,181 update codes and centers iter(1/1).
2022-10-20 22:18:28,914 number of update_code wrong: 0.
2022-10-20 22:18:31,803 non zero codewords: 1024.
2022-10-20 22:18:31,804 finish center update, duration: 10.62 sec.
2022-10-20 22:18:37,395 step [ 732], lr [0.0000750], embedding loss [ 0.8225], quantization loss [ 0.0099],  3.33 sec/batch.
2022-10-20 22:18:43,087 step [ 733], lr [0.0000750], embedding loss [ 0.8286], quantization loss [ 0.0092],  3.31 sec/batch.
2022-10-20 22:18:48,760 step [ 734], lr [0.0000750], embedding loss [ 0.8203], quantization loss [ 0.0100],  3.25 sec/batch.
2022-10-20 22:18:53,466 step [ 735], lr [0.0000750], embedding loss [ 0.8173], quantization loss [ 0.0093],  3.28 sec/batch.
2022-10-20 22:18:59,370 step [ 736], lr [0.0000750], embedding loss [ 0.8191], quantization loss [ 0.0100],  3.48 sec/batch.
2022-10-20 22:19:05,175 step [ 737], lr [0.0000750], embedding loss [ 0.8219], quantization loss [ 0.0114],  3.42 sec/batch.
2022-10-20 22:19:10,794 step [ 738], lr [0.0000750], embedding loss [ 0.8165], quantization loss [ 0.0092],  3.27 sec/batch.
2022-10-20 22:19:15,796 step [ 739], lr [0.0000750], embedding loss [ 0.8250], quantization loss [ 0.0086],  3.50 sec/batch.
2022-10-20 22:19:21,485 step [ 740], lr [0.0000750], embedding loss [ 0.8195], quantization loss [ 0.0094],  3.29 sec/batch.
2022-10-20 22:19:27,094 step [ 741], lr [0.0000750], embedding loss [ 0.8152], quantization loss [ 0.0097],  3.29 sec/batch.
2022-10-20 22:19:32,725 step [ 742], lr [0.0000750], embedding loss [ 0.8157], quantization loss [ 0.0103],  3.24 sec/batch.
2022-10-20 22:19:37,498 step [ 743], lr [0.0000750], embedding loss [ 0.8224], quantization loss [ 0.0097],  3.28 sec/batch.
2022-10-20 22:19:43,268 step [ 744], lr [0.0000750], embedding loss [ 0.8092], quantization loss [ 0.0092],  3.35 sec/batch.
2022-10-20 22:19:49,032 step [ 745], lr [0.0000750], embedding loss [ 0.8235], quantization loss [ 0.0094],  3.35 sec/batch.
2022-10-20 22:19:54,779 step [ 746], lr [0.0000750], embedding loss [ 0.8226], quantization loss [ 0.0098],  3.38 sec/batch.
2022-10-20 22:19:59,848 step [ 747], lr [0.0000750], embedding loss [ 0.8261], quantization loss [ 0.0095],  3.57 sec/batch.
2022-10-20 22:20:05,852 step [ 748], lr [0.0000750], embedding loss [ 0.8277], quantization loss [ 0.0093],  3.59 sec/batch.
2022-10-20 22:20:11,618 step [ 749], lr [0.0000750], embedding loss [ 0.8187], quantization loss [ 0.0102],  3.36 sec/batch.
2022-10-20 22:20:17,174 step [ 750], lr [0.0000750], embedding loss [ 0.8199], quantization loss [ 0.0099],  3.28 sec/batch.
2022-10-20 22:20:21,978 step [ 751], lr [0.0000750], embedding loss [ 0.8215], quantization loss [ 0.0103],  3.36 sec/batch.
2022-10-20 22:20:28,037 step [ 752], lr [0.0000750], embedding loss [ 0.8188], quantization loss [ 0.0091],  3.63 sec/batch.
2022-10-20 22:20:33,943 step [ 753], lr [0.0000750], embedding loss [ 0.8163], quantization loss [ 0.0094],  3.50 sec/batch.
2022-10-20 22:20:41,442 step [ 754], lr [0.0000750], embedding loss [ 0.8163], quantization loss [ 0.0095],  3.51 sec/batch.
2022-10-20 22:20:46,392 step [ 755], lr [0.0000750], embedding loss [ 0.8187], quantization loss [ 0.0100],  3.45 sec/batch.
2022-10-20 22:20:52,465 step [ 756], lr [0.0000750], embedding loss [ 0.8173], quantization loss [ 0.0105],  3.61 sec/batch.
2022-10-20 22:20:58,342 step [ 757], lr [0.0000750], embedding loss [ 0.8258], quantization loss [ 0.0100],  3.52 sec/batch.
2022-10-20 22:21:04,303 step [ 758], lr [0.0000750], embedding loss [ 0.8211], quantization loss [ 0.0097],  3.55 sec/batch.
2022-10-20 22:21:09,197 step [ 759], lr [0.0000750], embedding loss [ 0.8128], quantization loss [ 0.0093],  3.41 sec/batch.
2022-10-20 22:21:14,916 step [ 760], lr [0.0000750], embedding loss [ 0.8265], quantization loss [ 0.0100],  3.29 sec/batch.
2022-10-20 22:21:20,624 step [ 761], lr [0.0000750], embedding loss [ 0.8207], quantization loss [ 0.0094],  3.33 sec/batch.
2022-10-20 22:21:26,482 step [ 762], lr [0.0000750], embedding loss [ 0.8176], quantization loss [ 0.0096],  3.48 sec/batch.
2022-10-20 22:21:31,351 step [ 763], lr [0.0000750], embedding loss [ 0.8126], quantization loss [ 0.0094],  3.40 sec/batch.
2022-10-20 22:21:37,451 step [ 764], lr [0.0000750], embedding loss [ 0.8192], quantization loss [ 0.0100],  3.63 sec/batch.
2022-10-20 22:21:43,361 step [ 765], lr [0.0000750], embedding loss [ 0.8190], quantization loss [ 0.0097],  3.45 sec/batch.
2022-10-20 22:21:49,465 step [ 766], lr [0.0000750], embedding loss [ 0.8204], quantization loss [ 0.0102],  3.67 sec/batch.
2022-10-20 22:21:55,230 step [ 767], lr [0.0000750], embedding loss [ 0.8160], quantization loss [ 0.0092],  3.29 sec/batch.
2022-10-20 22:21:59,972 step [ 768], lr [0.0000750], embedding loss [ 0.8196], quantization loss [ 0.0096],  3.24 sec/batch.
2022-10-20 22:22:05,860 step [ 769], lr [0.0000750], embedding loss [ 0.8209], quantization loss [ 0.0098],  3.52 sec/batch.
2022-10-20 22:22:11,682 step [ 770], lr [0.0000750], embedding loss [ 0.8182], quantization loss [ 0.0094],  3.40 sec/batch.
2022-10-20 22:22:17,618 step [ 771], lr [0.0000750], embedding loss [ 0.8231], quantization loss [ 0.0095],  3.51 sec/batch.
2022-10-20 22:22:22,418 step [ 772], lr [0.0000750], embedding loss [ 0.8240], quantization loss [ 0.0099],  3.33 sec/batch.
2022-10-20 22:22:28,237 step [ 773], lr [0.0000750], embedding loss [ 0.8244], quantization loss [ 0.0090],  3.43 sec/batch.
2022-10-20 22:22:34,009 step [ 774], lr [0.0000750], embedding loss [ 0.8257], quantization loss [ 0.0096],  3.38 sec/batch.
2022-10-20 22:22:39,883 step [ 775], lr [0.0000750], embedding loss [ 0.8254], quantization loss [ 0.0097],  3.49 sec/batch.
2022-10-20 22:22:44,668 step [ 776], lr [0.0000750], embedding loss [ 0.8246], quantization loss [ 0.0097],  3.29 sec/batch.
2022-10-20 22:22:50,485 step [ 777], lr [0.0000750], embedding loss [ 0.8149], quantization loss [ 0.0094],  3.44 sec/batch.
2022-10-20 22:22:55,825 step [ 778], lr [0.0000750], embedding loss [ 0.8272], quantization loss [ 0.0091],  3.16 sec/batch.
2022-10-20 22:23:01,501 step [ 779], lr [0.0000750], embedding loss [ 0.8188], quantization loss [ 0.0100],  3.34 sec/batch.
2022-10-20 22:23:06,443 step [ 780], lr [0.0000750], embedding loss [ 0.8223], quantization loss [ 0.0090],  3.42 sec/batch.
2022-10-20 22:23:12,066 step [ 781], lr [0.0000750], embedding loss [ 0.8178], quantization loss [ 0.0106],  3.27 sec/batch.
2022-10-20 22:23:19,111 step [ 782], lr [0.0000750], embedding loss [ 0.8234], quantization loss [ 0.0096],  3.30 sec/batch.
2022-10-20 22:23:23,819 step [ 783], lr [0.0000750], embedding loss [ 0.8079], quantization loss [ 0.0092],  3.22 sec/batch.
2022-10-20 22:23:29,578 step [ 784], lr [0.0000750], embedding loss [ 0.8110], quantization loss [ 0.0104],  3.37 sec/batch.
2022-10-20 22:23:35,494 step [ 785], lr [0.0000750], embedding loss [ 0.8211], quantization loss [ 0.0097],  3.53 sec/batch.
2022-10-20 22:23:41,035 step [ 786], lr [0.0000750], embedding loss [ 0.8283], quantization loss [ 0.0095],  3.19 sec/batch.
2022-10-20 22:23:45,669 step [ 787], lr [0.0000750], embedding loss [ 0.8183], quantization loss [ 0.0107],  3.18 sec/batch.
2022-10-20 22:23:51,514 step [ 788], lr [0.0000750], embedding loss [ 0.8159], quantization loss [ 0.0093],  3.44 sec/batch.
2022-10-20 22:23:57,222 step [ 789], lr [0.0000750], embedding loss [ 0.8203], quantization loss [ 0.0098],  3.37 sec/batch.
2022-10-20 22:24:02,819 step [ 790], lr [0.0000750], embedding loss [ 0.8227], quantization loss [ 0.0099],  3.22 sec/batch.
2022-10-20 22:24:07,565 step [ 791], lr [0.0000750], embedding loss [ 0.8145], quantization loss [ 0.0099],  3.22 sec/batch.
2022-10-20 22:24:13,456 step [ 792], lr [0.0000750], embedding loss [ 0.8273], quantization loss [ 0.0112],  3.51 sec/batch.
2022-10-20 22:24:20,440 step [ 793], lr [0.0000750], embedding loss [ 0.8235], quantization loss [ 0.0100],  4.69 sec/batch.
2022-10-20 22:24:26,322 step [ 794], lr [0.0000750], embedding loss [ 0.8149], quantization loss [ 0.0092],  3.50 sec/batch.
2022-10-20 22:24:32,548 step [ 795], lr [0.0000750], embedding loss [ 0.8216], quantization loss [ 0.0100],  4.73 sec/batch.
2022-10-20 22:24:38,100 step [ 796], lr [0.0000750], embedding loss [ 0.8244], quantization loss [ 0.0093],  3.21 sec/batch.
2022-10-20 22:24:43,806 step [ 797], lr [0.0000750], embedding loss [ 0.8178], quantization loss [ 0.0100],  3.29 sec/batch.
2022-10-20 22:24:49,313 step [ 798], lr [0.0000750], embedding loss [ 0.8195], quantization loss [ 0.0094],  3.19 sec/batch.
2022-10-20 22:24:53,994 step [ 799], lr [0.0000750], embedding loss [ 0.8218], quantization loss [ 0.0091],  3.22 sec/batch.
2022-10-20 22:24:59,813 step [ 800], lr [0.0000750], embedding loss [ 0.8220], quantization loss [ 0.0105],  3.36 sec/batch.
2022-10-20 22:24:59,813 finish training iterations and begin saving model.
2022-10-20 22:25:06,560 finish model saving.
2022-10-20 22:25:06,563 finish training, model saved under ./checkpoints/flickr_WSDQH_nbits=32_adaMargin_gamma=1_lambda=0.0001_0004.npy.
2022-10-20 22:25:09,960 prepare dataset.
2022-10-20 22:25:10,610 prepare data loader.
2022-10-20 22:25:10,610 Initializing DataLoader.
2022-10-20 22:25:10,611 DataLoader already.
2022-10-20 22:25:10,611 Initializing DataLoader.
2022-10-20 22:25:10,611 DataLoader already.
2022-10-20 22:25:10,611 prepare model.
2022-10-20 22:25:10,832 Number of semantic embeddings: 1178.
2022-10-20 22:25:27,956 begin validation.
2022-10-20 22:25:50,415 finish query feature extraction, duration: 22.46 sec.
2022-10-20 22:29:20,756 finish database feature extraction, duration: 210.34 sec.
2022-10-20 22:29:20,756 compute quantization codes for query.
2022-10-20 22:29:22,475 number of update_code wrong: 0.
2022-10-20 22:29:22,475 finish query encoding, duration: 1.72 sec.
2022-10-20 22:29:22,475 compute quantization codes for database.
2022-10-20 22:29:31,728 number of update_code wrong: 0.
2022-10-20 22:29:31,728 finish database encoding, duration: 9.25 sec.
2022-10-20 22:29:31,729 save retrieval information: codes, features, reconstructions of queries and database.
2022-10-20 22:29:32,628 begin to calculate MAP@5000.
2022-10-20 22:29:32,629 begin to calculate AQD mAP@5000.
2022-10-20 22:29:36,022 AQD mAP@5000 = [0.7665], duration: 3.39 sec.
2022-10-20 22:29:36,022 begin to calculate SQD mAP@5000.
2022-10-20 22:29:39,285 SQD mAP@5000 = [0.7672], duration: 3.26 sec.
2022-10-20 22:29:39,285 begin to calculate feats mAP@5000.
2022-10-20 22:29:42,591 feats mAP@5000 = [0.7707], duration: 3.31 sec.
2022-10-20 22:29:42,592 finish validation.
