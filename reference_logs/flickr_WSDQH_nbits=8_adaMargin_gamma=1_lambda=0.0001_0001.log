2022-10-20 18:23:50,492 prepare dataset.
2022-10-20 18:24:04,683 prepare data loader.
2022-10-20 18:24:04,684 Initializing DataLoader.
2022-10-20 18:24:04,691 DataLoader already.
2022-10-20 18:24:04,691 prepare model.
2022-10-20 18:24:04,910 Number of semantic embeddings: 1178.
2022-10-20 18:24:12,969 From /data/wangjinpeng/anaconda3/envs/py37torch/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where.
2022-10-20 18:24:25,213 begin training.
2022-10-20 18:24:45,177 step [   1], lr [0.0003000], embedding loss [ 0.8931], quantization loss [ 0.0000], 17.42 sec/batch.
2022-10-20 18:24:48,501 step [   2], lr [0.0003000], embedding loss [ 0.8723], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:24:51,615 step [   3], lr [0.0003000], embedding loss [ 0.8699], quantization loss [ 0.0000],  0.60 sec/batch.
2022-10-20 18:24:54,842 step [   4], lr [0.0003000], embedding loss [ 0.8459], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:24:58,024 step [   5], lr [0.0003000], embedding loss [ 0.8471], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 18:25:01,418 step [   6], lr [0.0003000], embedding loss [ 0.8412], quantization loss [ 0.0000],  0.60 sec/batch.
2022-10-20 18:25:04,696 step [   7], lr [0.0003000], embedding loss [ 0.8475], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:25:07,824 step [   8], lr [0.0003000], embedding loss [ 0.8458], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:25:11,111 step [   9], lr [0.0003000], embedding loss [ 0.8407], quantization loss [ 0.0000],  0.59 sec/batch.
2022-10-20 18:25:14,198 step [  10], lr [0.0003000], embedding loss [ 0.8412], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:25:17,448 step [  11], lr [0.0003000], embedding loss [ 0.8467], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:25:20,731 step [  12], lr [0.0003000], embedding loss [ 0.8400], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:25:24,149 step [  13], lr [0.0003000], embedding loss [ 0.8471], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:25:27,504 step [  14], lr [0.0003000], embedding loss [ 0.8460], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:25:30,832 step [  15], lr [0.0003000], embedding loss [ 0.8447], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:25:34,088 step [  16], lr [0.0003000], embedding loss [ 0.8385], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:25:37,390 step [  17], lr [0.0003000], embedding loss [ 0.8434], quantization loss [ 0.0000],  0.59 sec/batch.
2022-10-20 18:25:40,525 step [  18], lr [0.0003000], embedding loss [ 0.8450], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:25:43,639 step [  19], lr [0.0003000], embedding loss [ 0.8496], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:25:46,926 step [  20], lr [0.0003000], embedding loss [ 0.8388], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:25:50,198 step [  21], lr [0.0003000], embedding loss [ 0.8433], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:25:53,496 step [  22], lr [0.0003000], embedding loss [ 0.8416], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:25:56,751 step [  23], lr [0.0003000], embedding loss [ 0.8389], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:25:59,962 step [  24], lr [0.0003000], embedding loss [ 0.8445], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:26:03,227 step [  25], lr [0.0003000], embedding loss [ 0.8495], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:26:06,677 step [  26], lr [0.0003000], embedding loss [ 0.8416], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:26:09,939 step [  27], lr [0.0003000], embedding loss [ 0.8328], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:26:13,146 step [  28], lr [0.0003000], embedding loss [ 0.8416], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:26:16,517 step [  29], lr [0.0003000], embedding loss [ 0.8340], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:26:19,810 step [  30], lr [0.0003000], embedding loss [ 0.8411], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:26:22,981 step [  31], lr [0.0003000], embedding loss [ 0.8303], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 18:26:26,139 step [  32], lr [0.0003000], embedding loss [ 0.8396], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:26:29,276 step [  33], lr [0.0003000], embedding loss [ 0.8314], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 18:26:32,538 step [  34], lr [0.0003000], embedding loss [ 0.8351], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 18:26:35,838 step [  35], lr [0.0003000], embedding loss [ 0.8425], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 18:26:39,092 step [  36], lr [0.0003000], embedding loss [ 0.8459], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:26:42,339 step [  37], lr [0.0003000], embedding loss [ 0.8356], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:26:45,623 step [  38], lr [0.0003000], embedding loss [ 0.8294], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:26:48,915 step [  39], lr [0.0003000], embedding loss [ 0.8337], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 18:26:52,120 step [  40], lr [0.0003000], embedding loss [ 0.8378], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:26:55,397 step [  41], lr [0.0003000], embedding loss [ 0.8438], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:26:58,722 step [  42], lr [0.0003000], embedding loss [ 0.8324], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 18:27:02,059 step [  43], lr [0.0003000], embedding loss [ 0.8319], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:27:05,426 step [  44], lr [0.0003000], embedding loss [ 0.8350], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:27:08,518 step [  45], lr [0.0003000], embedding loss [ 0.8299], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 18:27:11,726 step [  46], lr [0.0003000], embedding loss [ 0.8409], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:27:15,180 step [  47], lr [0.0003000], embedding loss [ 0.8336], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 18:27:18,461 step [  48], lr [0.0003000], embedding loss [ 0.8357], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:27:21,645 step [  49], lr [0.0003000], embedding loss [ 0.8284], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:27:24,815 step [  50], lr [0.0003000], embedding loss [ 0.8267], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:27:27,986 step [  51], lr [0.0003000], embedding loss [ 0.8339], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 18:27:31,189 step [  52], lr [0.0003000], embedding loss [ 0.8394], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 18:27:34,421 step [  53], lr [0.0003000], embedding loss [ 0.8294], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 18:27:37,726 step [  54], lr [0.0003000], embedding loss [ 0.8333], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:27:40,936 step [  55], lr [0.0003000], embedding loss [ 0.8329], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:27:44,178 step [  56], lr [0.0003000], embedding loss [ 0.8331], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 18:27:47,470 step [  57], lr [0.0003000], embedding loss [ 0.8269], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:27:50,649 step [  58], lr [0.0003000], embedding loss [ 0.8366], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 18:27:53,907 step [  59], lr [0.0003000], embedding loss [ 0.8221], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-20 18:27:57,235 step [  60], lr [0.0003000], embedding loss [ 0.8210], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:28:00,469 step [  61], lr [0.0003000], embedding loss [ 0.8297], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:28:03,808 step [  62], lr [0.0003000], embedding loss [ 0.8311], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:28:07,131 step [  63], lr [0.0003000], embedding loss [ 0.8351], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:28:10,477 step [  64], lr [0.0003000], embedding loss [ 0.8307], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:28:13,791 step [  65], lr [0.0003000], embedding loss [ 0.8339], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:28:24,278 step [  66], lr [0.0003000], embedding loss [ 0.8314], quantization loss [ 0.0000],  7.76 sec/batch.
2022-10-20 18:28:31,708 step [  67], lr [0.0003000], embedding loss [ 0.8349], quantization loss [ 0.0000],  2.95 sec/batch.
2022-10-20 18:28:36,138 step [  68], lr [0.0003000], embedding loss [ 0.8229], quantization loss [ 0.0000],  1.29 sec/batch.
2022-10-20 18:28:42,243 step [  69], lr [0.0003000], embedding loss [ 0.8308], quantization loss [ 0.0000],  2.44 sec/batch.
2022-10-20 18:28:48,353 step [  70], lr [0.0003000], embedding loss [ 0.8275], quantization loss [ 0.0000],  2.23 sec/batch.
2022-10-20 18:28:55,610 step [  71], lr [0.0003000], embedding loss [ 0.8351], quantization loss [ 0.0000],  2.03 sec/batch.
2022-10-20 18:29:02,305 step [  72], lr [0.0003000], embedding loss [ 0.8356], quantization loss [ 0.0000],  2.21 sec/batch.
2022-10-20 18:29:07,364 step [  73], lr [0.0003000], embedding loss [ 0.8278], quantization loss [ 0.0000],  2.22 sec/batch.
2022-10-20 18:29:12,596 step [  74], lr [0.0003000], embedding loss [ 0.8278], quantization loss [ 0.0000],  2.78 sec/batch.
2022-10-20 18:29:20,490 step [  75], lr [0.0003000], embedding loss [ 0.8317], quantization loss [ 0.0000],  5.50 sec/batch.
2022-10-20 18:29:25,584 step [  76], lr [0.0003000], embedding loss [ 0.8296], quantization loss [ 0.0000],  3.37 sec/batch.
2022-10-20 18:29:31,136 step [  77], lr [0.0003000], embedding loss [ 0.8250], quantization loss [ 0.0000],  3.44 sec/batch.
2022-10-20 18:29:45,402 step [  78], lr [0.0003000], embedding loss [ 0.8284], quantization loss [ 0.0000],  6.16 sec/batch.
2022-10-20 18:29:55,182 step [  79], lr [0.0003000], embedding loss [ 0.8312], quantization loss [ 0.0000],  8.12 sec/batch.
2022-10-20 18:30:06,164 step [  80], lr [0.0003000], embedding loss [ 0.8242], quantization loss [ 0.0000],  5.64 sec/batch.
2022-10-20 18:30:13,650 step [  81], lr [0.0003000], embedding loss [ 0.8287], quantization loss [ 0.0000],  4.80 sec/batch.
2022-10-20 18:30:17,921 step [  82], lr [0.0003000], embedding loss [ 0.8340], quantization loss [ 0.0000],  1.50 sec/batch.
2022-10-20 18:30:19,822 step [  83], lr [0.0003000], embedding loss [ 0.8324], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:30:21,795 step [  84], lr [0.0003000], embedding loss [ 0.8342], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:30:23,798 step [  85], lr [0.0003000], embedding loss [ 0.8310], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:30:25,731 step [  86], lr [0.0003000], embedding loss [ 0.8298], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:30:27,603 step [  87], lr [0.0003000], embedding loss [ 0.8298], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:30:29,576 step [  88], lr [0.0003000], embedding loss [ 0.8284], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:30:31,585 step [  89], lr [0.0003000], embedding loss [ 0.8349], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:30:33,535 step [  90], lr [0.0003000], embedding loss [ 0.8292], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:30:35,461 step [  91], lr [0.0003000], embedding loss [ 0.8213], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:30:37,487 step [  92], lr [0.0003000], embedding loss [ 0.8207], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:30:39,440 step [  93], lr [0.0003000], embedding loss [ 0.8166], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:30:41,357 step [  94], lr [0.0003000], embedding loss [ 0.8238], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:30:43,276 step [  95], lr [0.0003000], embedding loss [ 0.8298], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:30:45,216 step [  96], lr [0.0003000], embedding loss [ 0.8268], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:30:47,190 step [  97], lr [0.0003000], embedding loss [ 0.8290], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:30:49,155 step [  98], lr [0.0003000], embedding loss [ 0.8349], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:30:51,094 step [  99], lr [0.0003000], embedding loss [ 0.8271], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:30:53,084 step [ 100], lr [0.0003000], embedding loss [ 0.8260], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:30:55,101 step [ 101], lr [0.0003000], embedding loss [ 0.8340], quantization loss [ 0.0000],  0.59 sec/batch.
2022-10-20 18:30:57,080 step [ 102], lr [0.0003000], embedding loss [ 0.8184], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:30:59,062 step [ 103], lr [0.0003000], embedding loss [ 0.8269], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:31:01,066 step [ 104], lr [0.0003000], embedding loss [ 0.8227], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:31:03,002 step [ 105], lr [0.0003000], embedding loss [ 0.8200], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:31:04,981 step [ 106], lr [0.0003000], embedding loss [ 0.8253], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:31:06,906 step [ 107], lr [0.0003000], embedding loss [ 0.8229], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:31:08,890 step [ 108], lr [0.0003000], embedding loss [ 0.8250], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:31:10,878 step [ 109], lr [0.0003000], embedding loss [ 0.8207], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:31:12,877 step [ 110], lr [0.0003000], embedding loss [ 0.8256], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:31:14,919 step [ 111], lr [0.0003000], embedding loss [ 0.8203], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:31:16,875 step [ 112], lr [0.0003000], embedding loss [ 0.8323], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:31:18,837 step [ 113], lr [0.0003000], embedding loss [ 0.8340], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:31:20,813 step [ 114], lr [0.0003000], embedding loss [ 0.8235], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:31:22,787 step [ 115], lr [0.0003000], embedding loss [ 0.8219], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:31:24,772 step [ 116], lr [0.0003000], embedding loss [ 0.8255], quantization loss [ 0.0000],  0.59 sec/batch.
2022-10-20 18:31:26,744 step [ 117], lr [0.0003000], embedding loss [ 0.8271], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:31:28,688 step [ 118], lr [0.0003000], embedding loss [ 0.8180], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:31:30,644 step [ 119], lr [0.0003000], embedding loss [ 0.8293], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:31:32,598 step [ 120], lr [0.0003000], embedding loss [ 0.8259], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:31:34,610 step [ 121], lr [0.0003000], embedding loss [ 0.8267], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:31:36,575 step [ 122], lr [0.0003000], embedding loss [ 0.8214], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:31:38,542 step [ 123], lr [0.0003000], embedding loss [ 0.8231], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:31:40,571 step [ 124], lr [0.0003000], embedding loss [ 0.8228], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:31:42,562 step [ 125], lr [0.0003000], embedding loss [ 0.8290], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:31:44,576 step [ 126], lr [0.0003000], embedding loss [ 0.8263], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:31:46,533 step [ 127], lr [0.0003000], embedding loss [ 0.8276], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:31:48,497 step [ 128], lr [0.0003000], embedding loss [ 0.8217], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:31:50,426 step [ 129], lr [0.0003000], embedding loss [ 0.8223], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:31:52,414 step [ 130], lr [0.0003000], embedding loss [ 0.8228], quantization loss [ 0.0000],  0.60 sec/batch.
2022-10-20 18:31:54,361 step [ 131], lr [0.0003000], embedding loss [ 0.8287], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:31:56,325 step [ 132], lr [0.0003000], embedding loss [ 0.8287], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:31:58,266 step [ 133], lr [0.0003000], embedding loss [ 0.8227], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:32:00,268 step [ 134], lr [0.0003000], embedding loss [ 0.8219], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:32:02,252 step [ 135], lr [0.0003000], embedding loss [ 0.8227], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:32:04,233 step [ 136], lr [0.0003000], embedding loss [ 0.8269], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:32:06,169 step [ 137], lr [0.0003000], embedding loss [ 0.8376], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:32:08,172 step [ 138], lr [0.0003000], embedding loss [ 0.8261], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:32:10,162 step [ 139], lr [0.0003000], embedding loss [ 0.8192], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:32:12,092 step [ 140], lr [0.0003000], embedding loss [ 0.8250], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-20 18:32:14,035 step [ 141], lr [0.0003000], embedding loss [ 0.8233], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:32:16,091 step [ 142], lr [0.0003000], embedding loss [ 0.8278], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-20 18:32:18,125 step [ 143], lr [0.0003000], embedding loss [ 0.8224], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-20 18:33:59,585 step [ 144], lr [0.0003000], embedding loss [ 0.8232], quantization loss [ 0.0000],  7.61 sec/batch.
2022-10-20 18:34:06,365 step [ 145], lr [0.0003000], embedding loss [ 0.8202], quantization loss [ 0.0000],  5.42 sec/batch.
2022-10-20 18:34:13,502 step [ 146], lr [0.0003000], embedding loss [ 0.8184], quantization loss [ 0.0000],  5.80 sec/batch.
2022-10-20 18:34:20,194 step [ 147], lr [0.0003000], embedding loss [ 0.8103], quantization loss [ 0.0000],  5.39 sec/batch.
2022-10-20 18:34:20,194 initialize centers iter(1/1).
2022-10-20 18:34:25,347 finish center initialization, duration: 5.15 sec.
2022-10-20 18:34:25,347 update codes and centers iter(1/1).
2022-10-20 18:34:27,981 number of update_code wrong: 71.
2022-10-20 18:34:31,515 non zero codewords: 256.
2022-10-20 18:34:31,515 finish center update, duration: 6.17 sec.
2022-10-20 18:34:40,106 step [ 148], lr [0.0003000], embedding loss [ 0.8276], quantization loss [ 0.1757],  7.33 sec/batch.
2022-10-20 18:34:46,862 step [ 149], lr [0.0003000], embedding loss [ 0.8253], quantization loss [ 0.2132],  5.42 sec/batch.
2022-10-20 18:34:53,595 step [ 150], lr [0.0003000], embedding loss [ 0.8280], quantization loss [ 0.3101],  5.41 sec/batch.
2022-10-20 18:35:00,467 step [ 151], lr [0.0003000], embedding loss [ 0.8284], quantization loss [ 0.1767],  5.49 sec/batch.
2022-10-20 18:35:09,721 step [ 152], lr [0.0003000], embedding loss [ 0.8303], quantization loss [ 0.1559],  7.90 sec/batch.
2022-10-20 18:35:17,267 step [ 153], lr [0.0003000], embedding loss [ 0.8262], quantization loss [ 0.1567],  6.18 sec/batch.
2022-10-20 18:35:24,153 step [ 154], lr [0.0003000], embedding loss [ 0.8271], quantization loss [ 0.1433],  5.53 sec/batch.
2022-10-20 18:35:33,593 step [ 155], lr [0.0003000], embedding loss [ 0.8324], quantization loss [ 0.1666],  8.07 sec/batch.
2022-10-20 18:35:40,636 step [ 156], lr [0.0003000], embedding loss [ 0.8272], quantization loss [ 0.1418],  5.71 sec/batch.
2022-10-20 18:35:47,267 step [ 157], lr [0.0003000], embedding loss [ 0.8289], quantization loss [ 0.1604],  5.31 sec/batch.
2022-10-20 18:35:54,282 step [ 158], lr [0.0003000], embedding loss [ 0.8243], quantization loss [ 0.1139],  5.62 sec/batch.
2022-10-20 18:36:03,513 step [ 159], lr [0.0003000], embedding loss [ 0.8282], quantization loss [ 0.1273],  7.90 sec/batch.
2022-10-20 18:36:10,294 step [ 160], lr [0.0003000], embedding loss [ 0.8258], quantization loss [ 0.1113],  5.43 sec/batch.
2022-10-20 18:36:17,454 step [ 161], lr [0.0003000], embedding loss [ 0.8287], quantization loss [ 0.1142],  5.80 sec/batch.
2022-10-20 18:36:26,353 step [ 162], lr [0.0003000], embedding loss [ 0.8321], quantization loss [ 0.1203],  7.53 sec/batch.
2022-10-20 18:36:33,622 step [ 163], lr [0.0003000], embedding loss [ 0.8317], quantization loss [ 0.1096],  5.91 sec/batch.
2022-10-20 18:36:40,330 step [ 164], lr [0.0003000], embedding loss [ 0.8181], quantization loss [ 0.1071],  5.33 sec/batch.
2022-10-20 18:36:47,181 step [ 165], lr [0.0003000], embedding loss [ 0.8294], quantization loss [ 0.1080],  5.47 sec/batch.
2022-10-20 18:36:56,541 step [ 166], lr [0.0003000], embedding loss [ 0.8264], quantization loss [ 0.1013],  7.99 sec/batch.
2022-10-20 18:37:03,287 step [ 167], lr [0.0003000], embedding loss [ 0.8285], quantization loss [ 0.0964],  5.41 sec/batch.
2022-10-20 18:37:10,052 step [ 168], lr [0.0003000], embedding loss [ 0.8231], quantization loss [ 0.1043],  5.44 sec/batch.
2022-10-20 18:37:19,058 step [ 169], lr [0.0003000], embedding loss [ 0.8261], quantization loss [ 0.0957],  7.63 sec/batch.
2022-10-20 18:37:26,114 step [ 170], lr [0.0003000], embedding loss [ 0.8308], quantization loss [ 0.0962],  5.69 sec/batch.
2022-10-20 18:37:33,206 step [ 171], lr [0.0003000], embedding loss [ 0.8231], quantization loss [ 0.0841],  5.73 sec/batch.
2022-10-20 18:37:40,172 step [ 172], lr [0.0003000], embedding loss [ 0.8255], quantization loss [ 0.0898],  5.61 sec/batch.
2022-10-20 18:37:49,110 step [ 173], lr [0.0003000], embedding loss [ 0.8297], quantization loss [ 0.1016],  7.63 sec/batch.
2022-10-20 18:37:55,825 step [ 174], lr [0.0003000], embedding loss [ 0.8238], quantization loss [ 0.1134],  5.41 sec/batch.
2022-10-20 18:38:03,277 step [ 175], lr [0.0003000], embedding loss [ 0.8274], quantization loss [ 0.1023],  6.05 sec/batch.
2022-10-20 18:38:10,266 step [ 176], lr [0.0003000], embedding loss [ 0.8358], quantization loss [ 0.0873],  5.64 sec/batch.
2022-10-20 18:38:19,841 step [ 177], lr [0.0003000], embedding loss [ 0.8233], quantization loss [ 0.0978],  8.22 sec/batch.
2022-10-20 18:38:26,461 step [ 178], lr [0.0003000], embedding loss [ 0.8218], quantization loss [ 0.0848],  5.25 sec/batch.
2022-10-20 18:38:33,279 step [ 179], lr [0.0003000], embedding loss [ 0.8274], quantization loss [ 0.0888],  5.44 sec/batch.
2022-10-20 18:38:42,132 step [ 180], lr [0.0003000], embedding loss [ 0.8306], quantization loss [ 0.1029],  7.56 sec/batch.
2022-10-20 18:38:48,986 step [ 181], lr [0.0003000], embedding loss [ 0.8203], quantization loss [ 0.0811],  5.54 sec/batch.
2022-10-20 18:38:55,889 step [ 182], lr [0.0003000], embedding loss [ 0.8234], quantization loss [ 0.0849],  5.54 sec/batch.
2022-10-20 18:39:02,935 step [ 183], lr [0.0003000], embedding loss [ 0.8173], quantization loss [ 0.0907],  5.68 sec/batch.
2022-10-20 18:39:11,986 step [ 184], lr [0.0003000], embedding loss [ 0.8225], quantization loss [ 0.0876],  7.71 sec/batch.
2022-10-20 18:39:19,121 step [ 185], lr [0.0003000], embedding loss [ 0.8227], quantization loss [ 0.0874],  5.78 sec/batch.
2022-10-20 18:39:25,855 step [ 186], lr [0.0003000], embedding loss [ 0.8274], quantization loss [ 0.0872],  5.41 sec/batch.
2022-10-20 18:39:34,974 step [ 187], lr [0.0003000], embedding loss [ 0.8147], quantization loss [ 0.0834],  7.79 sec/batch.
2022-10-20 18:39:41,758 step [ 188], lr [0.0003000], embedding loss [ 0.8229], quantization loss [ 0.0818],  5.45 sec/batch.
2022-10-20 18:39:49,095 step [ 189], lr [0.0003000], embedding loss [ 0.8284], quantization loss [ 0.0966],  5.94 sec/batch.
2022-10-20 18:39:56,198 step [ 190], lr [0.0003000], embedding loss [ 0.8271], quantization loss [ 0.0821],  5.74 sec/batch.
2022-10-20 18:40:05,105 step [ 191], lr [0.0003000], embedding loss [ 0.8291], quantization loss [ 0.0795],  7.55 sec/batch.
2022-10-20 18:40:11,822 step [ 192], lr [0.0003000], embedding loss [ 0.8280], quantization loss [ 0.0866],  5.34 sec/batch.
2022-10-20 18:40:18,580 step [ 193], lr [0.0003000], embedding loss [ 0.8290], quantization loss [ 0.0889],  5.46 sec/batch.
2022-10-20 18:40:27,979 step [ 194], lr [0.0003000], embedding loss [ 0.8258], quantization loss [ 0.0910],  8.04 sec/batch.
2022-10-20 18:40:34,840 step [ 195], lr [0.0003000], embedding loss [ 0.8210], quantization loss [ 0.0904],  5.48 sec/batch.
2022-10-20 18:40:41,906 step [ 196], lr [0.0003000], embedding loss [ 0.8186], quantization loss [ 0.0997],  5.70 sec/batch.
2022-10-20 18:40:49,010 step [ 197], lr [0.0003000], embedding loss [ 0.8188], quantization loss [ 0.0812],  5.75 sec/batch.
2022-10-20 18:40:58,127 step [ 198], lr [0.0003000], embedding loss [ 0.8261], quantization loss [ 0.0801],  7.77 sec/batch.
2022-10-20 18:41:05,249 step [ 199], lr [0.0003000], embedding loss [ 0.8217], quantization loss [ 0.0840],  5.76 sec/batch.
2022-10-20 18:41:11,878 step [ 200], lr [0.0003000], embedding loss [ 0.8297], quantization loss [ 0.0805],  5.28 sec/batch.
2022-10-20 18:41:21,014 step [ 201], lr [0.0003000], embedding loss [ 0.8245], quantization loss [ 0.0824],  7.78 sec/batch.
2022-10-20 18:41:28,107 step [ 202], lr [0.0003000], embedding loss [ 0.8200], quantization loss [ 0.0821],  5.71 sec/batch.
2022-10-20 18:41:34,672 step [ 203], lr [0.0003000], embedding loss [ 0.8174], quantization loss [ 0.0938],  5.24 sec/batch.
2022-10-20 18:41:41,329 step [ 204], lr [0.0003000], embedding loss [ 0.8304], quantization loss [ 0.0776],  5.34 sec/batch.
2022-10-20 18:41:50,515 step [ 205], lr [0.0003000], embedding loss [ 0.8190], quantization loss [ 0.0884],  7.81 sec/batch.
2022-10-20 18:41:57,234 step [ 206], lr [0.0003000], embedding loss [ 0.8207], quantization loss [ 0.0830],  5.38 sec/batch.
2022-10-20 18:42:04,243 step [ 207], lr [0.0003000], embedding loss [ 0.8278], quantization loss [ 0.0832],  5.66 sec/batch.
2022-10-20 18:42:11,456 step [ 208], lr [0.0003000], embedding loss [ 0.8191], quantization loss [ 0.0864],  5.85 sec/batch.
2022-10-20 18:42:20,860 step [ 209], lr [0.0003000], embedding loss [ 0.8287], quantization loss [ 0.0892],  8.05 sec/batch.
2022-10-20 18:42:27,611 step [ 210], lr [0.0003000], embedding loss [ 0.8251], quantization loss [ 0.0793],  5.38 sec/batch.
2022-10-20 18:42:34,362 step [ 211], lr [0.0003000], embedding loss [ 0.8287], quantization loss [ 0.0794],  5.38 sec/batch.
2022-10-20 18:42:43,771 step [ 212], lr [0.0003000], embedding loss [ 0.8199], quantization loss [ 0.0814],  8.05 sec/batch.
2022-10-20 18:42:51,070 step [ 213], lr [0.0003000], embedding loss [ 0.8288], quantization loss [ 0.0758],  5.94 sec/batch.
2022-10-20 18:42:58,312 step [ 214], lr [0.0003000], embedding loss [ 0.8255], quantization loss [ 0.0792],  5.88 sec/batch.
2022-10-20 18:43:05,043 step [ 215], lr [0.0003000], embedding loss [ 0.8135], quantization loss [ 0.0757],  5.36 sec/batch.
2022-10-20 18:43:14,254 step [ 216], lr [0.0003000], embedding loss [ 0.8268], quantization loss [ 0.0852],  7.84 sec/batch.
2022-10-20 18:43:21,610 step [ 217], lr [0.0003000], embedding loss [ 0.8358], quantization loss [ 0.0815],  5.97 sec/batch.
2022-10-20 18:43:28,955 step [ 218], lr [0.0003000], embedding loss [ 0.8252], quantization loss [ 0.0808],  5.91 sec/batch.
2022-10-20 18:43:38,111 step [ 219], lr [0.0003000], embedding loss [ 0.8221], quantization loss [ 0.0691],  7.80 sec/batch.
2022-10-20 18:43:45,018 step [ 220], lr [0.0003000], embedding loss [ 0.8249], quantization loss [ 0.0732],  5.52 sec/batch.
2022-10-20 18:43:51,839 step [ 221], lr [0.0003000], embedding loss [ 0.8332], quantization loss [ 0.0853],  5.45 sec/batch.
2022-10-20 18:43:58,893 step [ 222], lr [0.0003000], embedding loss [ 0.8221], quantization loss [ 0.0885],  5.69 sec/batch.
2022-10-20 18:44:10,056 step [ 223], lr [0.0003000], embedding loss [ 0.8271], quantization loss [ 0.0719],  9.81 sec/batch.
2022-10-20 18:44:25,619 step [ 224], lr [0.0003000], embedding loss [ 0.8203], quantization loss [ 0.0832], 14.21 sec/batch.
2022-10-20 18:44:32,779 step [ 225], lr [0.0003000], embedding loss [ 0.8240], quantization loss [ 0.0799],  5.79 sec/batch.
2022-10-20 18:44:41,601 step [ 226], lr [0.0003000], embedding loss [ 0.8270], quantization loss [ 0.0761],  7.46 sec/batch.
2022-10-20 18:44:48,683 step [ 227], lr [0.0003000], embedding loss [ 0.8182], quantization loss [ 0.0782],  5.72 sec/batch.
2022-10-20 18:44:58,018 step [ 228], lr [0.0003000], embedding loss [ 0.8164], quantization loss [ 0.0768],  7.96 sec/batch.
2022-10-20 18:45:04,988 step [ 229], lr [0.0003000], embedding loss [ 0.8258], quantization loss [ 0.0732],  5.64 sec/batch.
2022-10-20 18:45:13,814 step [ 230], lr [0.0003000], embedding loss [ 0.8282], quantization loss [ 0.0861],  7.51 sec/batch.
2022-10-20 18:45:20,559 step [ 231], lr [0.0003000], embedding loss [ 0.8259], quantization loss [ 0.0757],  5.42 sec/batch.
2022-10-20 18:45:33,950 step [ 232], lr [0.0003000], embedding loss [ 0.8219], quantization loss [ 0.0770], 12.00 sec/batch.
2022-10-20 18:45:40,913 step [ 233], lr [0.0003000], embedding loss [ 0.8254], quantization loss [ 0.0898],  5.59 sec/batch.
2022-10-20 18:45:54,446 step [ 234], lr [0.0003000], embedding loss [ 0.8230], quantization loss [ 0.0806], 12.17 sec/batch.
2022-10-20 18:46:01,146 step [ 235], lr [0.0003000], embedding loss [ 0.8296], quantization loss [ 0.0869],  5.35 sec/batch.
2022-10-20 18:46:10,160 step [ 236], lr [0.0003000], embedding loss [ 0.8246], quantization loss [ 0.0742],  7.66 sec/batch.
2022-10-20 18:46:17,332 step [ 237], lr [0.0003000], embedding loss [ 0.8177], quantization loss [ 0.0794],  5.80 sec/batch.
2022-10-20 18:46:26,235 step [ 238], lr [0.0003000], embedding loss [ 0.8237], quantization loss [ 0.0804],  7.54 sec/batch.
2022-10-20 18:46:33,029 step [ 239], lr [0.0003000], embedding loss [ 0.8181], quantization loss [ 0.0751],  5.40 sec/batch.
2022-10-20 18:46:39,704 step [ 240], lr [0.0003000], embedding loss [ 0.8331], quantization loss [ 0.0754],  5.33 sec/batch.
2022-10-20 18:46:48,699 step [ 241], lr [0.0003000], embedding loss [ 0.8212], quantization loss [ 0.0789],  7.63 sec/batch.
2022-10-20 18:46:58,545 step [ 242], lr [0.0003000], embedding loss [ 0.8236], quantization loss [ 0.0701],  8.49 sec/batch.
2022-10-20 18:47:11,465 step [ 243], lr [0.0003000], embedding loss [ 0.8317], quantization loss [ 0.0748], 11.53 sec/batch.
2022-10-20 18:47:20,468 step [ 244], lr [0.0003000], embedding loss [ 0.8210], quantization loss [ 0.0851],  7.62 sec/batch.
2022-10-20 18:47:29,329 step [ 245], lr [0.0003000], embedding loss [ 0.8203], quantization loss [ 0.0801],  7.51 sec/batch.
2022-10-20 18:47:35,949 step [ 246], lr [0.0003000], embedding loss [ 0.8306], quantization loss [ 0.0829],  5.27 sec/batch.
2022-10-20 18:47:44,936 step [ 247], lr [0.0003000], embedding loss [ 0.8255], quantization loss [ 0.0786],  7.65 sec/batch.
2022-10-20 18:47:53,788 step [ 248], lr [0.0003000], embedding loss [ 0.8252], quantization loss [ 0.0701],  7.51 sec/batch.
2022-10-20 18:48:00,585 step [ 249], lr [0.0003000], embedding loss [ 0.8199], quantization loss [ 0.0789],  5.42 sec/batch.
2022-10-20 18:48:09,878 step [ 250], lr [0.0003000], embedding loss [ 0.8187], quantization loss [ 0.0700],  7.92 sec/batch.
2022-10-20 18:48:19,183 step [ 251], lr [0.0003000], embedding loss [ 0.8181], quantization loss [ 0.0681],  7.92 sec/batch.
2022-10-20 18:48:28,449 step [ 252], lr [0.0003000], embedding loss [ 0.8296], quantization loss [ 0.0706],  7.88 sec/batch.
2022-10-20 18:48:38,026 step [ 253], lr [0.0003000], embedding loss [ 0.8255], quantization loss [ 0.0964],  8.19 sec/batch.
2022-10-20 18:48:46,942 step [ 254], lr [0.0003000], embedding loss [ 0.8199], quantization loss [ 0.0672],  7.52 sec/batch.
2022-10-20 18:48:56,021 step [ 255], lr [0.0003000], embedding loss [ 0.8294], quantization loss [ 0.0749],  7.72 sec/batch.
2022-10-20 18:49:05,215 step [ 256], lr [0.0003000], embedding loss [ 0.8220], quantization loss [ 0.0718],  7.83 sec/batch.
2022-10-20 18:49:14,121 step [ 257], lr [0.0003000], embedding loss [ 0.8132], quantization loss [ 0.0693],  7.52 sec/batch.
2022-10-20 18:49:24,407 step [ 258], lr [0.0003000], embedding loss [ 0.8304], quantization loss [ 0.0792],  8.90 sec/batch.
2022-10-20 18:49:35,173 step [ 259], lr [0.0003000], embedding loss [ 0.8239], quantization loss [ 0.0832],  7.93 sec/batch.
2022-10-20 18:49:48,514 step [ 260], lr [0.0003000], embedding loss [ 0.8238], quantization loss [ 0.0638], 12.02 sec/batch.
2022-10-20 18:49:57,546 step [ 261], lr [0.0003000], embedding loss [ 0.8182], quantization loss [ 0.0657],  7.68 sec/batch.
2022-10-20 18:50:06,821 step [ 262], lr [0.0003000], embedding loss [ 0.8232], quantization loss [ 0.0692],  7.90 sec/batch.
2022-10-20 18:50:16,146 step [ 263], lr [0.0003000], embedding loss [ 0.8273], quantization loss [ 0.0771],  7.91 sec/batch.
2022-10-20 18:50:25,131 step [ 264], lr [0.0003000], embedding loss [ 0.8164], quantization loss [ 0.0859],  7.60 sec/batch.
2022-10-20 18:50:34,206 step [ 265], lr [0.0003000], embedding loss [ 0.8274], quantization loss [ 0.0734],  7.70 sec/batch.
2022-10-20 18:50:43,890 step [ 266], lr [0.0003000], embedding loss [ 0.8139], quantization loss [ 0.0830],  8.30 sec/batch.
2022-10-20 18:50:57,152 step [ 267], lr [0.0003000], embedding loss [ 0.8259], quantization loss [ 0.0883], 11.89 sec/batch.
2022-10-20 18:51:06,355 step [ 268], lr [0.0003000], embedding loss [ 0.8170], quantization loss [ 0.0785],  7.84 sec/batch.
2022-10-20 18:51:15,392 step [ 269], lr [0.0003000], embedding loss [ 0.8235], quantization loss [ 0.0671],  7.65 sec/batch.
2022-10-20 18:51:24,873 step [ 270], lr [0.0003000], embedding loss [ 0.8189], quantization loss [ 0.0891],  8.10 sec/batch.
2022-10-20 18:51:34,044 step [ 271], lr [0.0003000], embedding loss [ 0.8184], quantization loss [ 0.0835],  7.80 sec/batch.
2022-10-20 18:51:43,565 step [ 272], lr [0.0003000], embedding loss [ 0.8253], quantization loss [ 0.0764],  8.16 sec/batch.
2022-10-20 18:51:52,450 step [ 273], lr [0.0003000], embedding loss [ 0.8230], quantization loss [ 0.0745],  7.50 sec/batch.
2022-10-20 18:52:01,362 step [ 274], lr [0.0003000], embedding loss [ 0.8162], quantization loss [ 0.0722],  7.57 sec/batch.
2022-10-20 18:52:13,332 step [ 275], lr [0.0003000], embedding loss [ 0.8270], quantization loss [ 0.0822], 10.56 sec/batch.
2022-10-20 18:52:23,548 step [ 276], lr [0.0003000], embedding loss [ 0.8231], quantization loss [ 0.0754],  7.74 sec/batch.
2022-10-20 18:52:34,863 step [ 277], lr [0.0003000], embedding loss [ 0.8208], quantization loss [ 0.0734],  9.97 sec/batch.
2022-10-20 18:52:44,262 step [ 278], lr [0.0003000], embedding loss [ 0.8326], quantization loss [ 0.0829],  8.03 sec/batch.
2022-10-20 18:52:53,075 step [ 279], lr [0.0003000], embedding loss [ 0.8222], quantization loss [ 0.0827],  7.47 sec/batch.
2022-10-20 18:53:02,014 step [ 280], lr [0.0003000], embedding loss [ 0.8151], quantization loss [ 0.0722],  7.58 sec/batch.
2022-10-20 18:53:11,277 step [ 281], lr [0.0003000], embedding loss [ 0.8185], quantization loss [ 0.0675],  7.89 sec/batch.
2022-10-20 18:53:20,233 step [ 282], lr [0.0003000], embedding loss [ 0.8274], quantization loss [ 0.0679],  7.60 sec/batch.
2022-10-20 18:53:29,614 step [ 283], lr [0.0003000], embedding loss [ 0.8211], quantization loss [ 0.0747],  8.06 sec/batch.
2022-10-20 18:53:42,957 step [ 284], lr [0.0003000], embedding loss [ 0.8203], quantization loss [ 0.0887], 11.98 sec/batch.
2022-10-20 18:53:55,256 step [ 285], lr [0.0003000], embedding loss [ 0.8299], quantization loss [ 0.0791], 10.87 sec/batch.
2022-10-20 18:54:04,413 step [ 286], lr [0.0003000], embedding loss [ 0.8162], quantization loss [ 0.0875],  7.83 sec/batch.
2022-10-20 18:54:13,933 step [ 287], lr [0.0003000], embedding loss [ 0.8213], quantization loss [ 0.0758],  8.17 sec/batch.
2022-10-20 18:54:22,897 step [ 288], lr [0.0003000], embedding loss [ 0.8253], quantization loss [ 0.0774],  7.63 sec/batch.
2022-10-20 18:54:31,933 step [ 289], lr [0.0003000], embedding loss [ 0.8232], quantization loss [ 0.0691],  7.71 sec/batch.
2022-10-20 18:54:41,747 step [ 290], lr [0.0003000], embedding loss [ 0.8220], quantization loss [ 0.0693],  8.49 sec/batch.
2022-10-20 18:54:51,237 step [ 291], lr [0.0003000], embedding loss [ 0.8236], quantization loss [ 0.0779],  8.15 sec/batch.
2022-10-20 18:55:04,903 step [ 292], lr [0.0003000], embedding loss [ 0.8115], quantization loss [ 0.0699], 12.32 sec/batch.
2022-10-20 18:55:13,817 step [ 293], lr [0.0003000], embedding loss [ 0.8280], quantization loss [ 0.0820],  7.59 sec/batch.
2022-10-20 18:55:13,817 update codes and centers iter(1/1).
2022-10-20 18:55:15,719 number of update_code wrong: 1.
2022-10-20 18:55:18,379 non zero codewords: 256.
2022-10-20 18:55:18,379 finish center update, duration: 4.56 sec.
2022-10-20 18:55:27,258 step [ 294], lr [0.0003000], embedding loss [ 0.8164], quantization loss [ 0.0233],  7.58 sec/batch.
2022-10-20 18:55:36,104 step [ 295], lr [0.0003000], embedding loss [ 0.8222], quantization loss [ 0.0228],  7.53 sec/batch.
2022-10-20 18:55:45,280 step [ 296], lr [0.0003000], embedding loss [ 0.8194], quantization loss [ 0.0211],  7.82 sec/batch.
2022-10-20 18:55:54,889 step [ 297], lr [0.0003000], embedding loss [ 0.8236], quantization loss [ 0.0240],  8.25 sec/batch.
2022-10-20 18:56:04,356 step [ 298], lr [0.0003000], embedding loss [ 0.8194], quantization loss [ 0.0215],  8.09 sec/batch.
2022-10-20 18:56:13,307 step [ 299], lr [0.0003000], embedding loss [ 0.8266], quantization loss [ 0.0207],  7.61 sec/batch.
2022-10-20 18:56:26,526 step [ 300], lr [0.0003000], embedding loss [ 0.8277], quantization loss [ 0.0191], 11.94 sec/batch.
2022-10-20 18:56:37,556 step [ 301], lr [0.0001500], embedding loss [ 0.8281], quantization loss [ 0.0231],  9.70 sec/batch.
2022-10-20 18:56:48,741 step [ 302], lr [0.0001500], embedding loss [ 0.8192], quantization loss [ 0.0201],  8.64 sec/batch.
2022-10-20 18:56:57,911 step [ 303], lr [0.0001500], embedding loss [ 0.8214], quantization loss [ 0.0218],  7.81 sec/batch.
2022-10-20 18:57:07,253 step [ 304], lr [0.0001500], embedding loss [ 0.8177], quantization loss [ 0.0217],  7.99 sec/batch.
2022-10-20 18:57:16,532 step [ 305], lr [0.0001500], embedding loss [ 0.8245], quantization loss [ 0.0199],  7.90 sec/batch.
2022-10-20 18:57:25,572 step [ 306], lr [0.0001500], embedding loss [ 0.8189], quantization loss [ 0.0194],  7.75 sec/batch.
2022-10-20 18:57:34,491 step [ 307], lr [0.0001500], embedding loss [ 0.8245], quantization loss [ 0.0179],  7.61 sec/batch.
2022-10-20 18:57:43,518 step [ 308], lr [0.0001500], embedding loss [ 0.8169], quantization loss [ 0.0203],  7.68 sec/batch.
2022-10-20 18:57:52,625 step [ 309], lr [0.0001500], embedding loss [ 0.8221], quantization loss [ 0.0195],  7.78 sec/batch.
2022-10-20 18:58:01,543 step [ 310], lr [0.0001500], embedding loss [ 0.8194], quantization loss [ 0.0184],  7.58 sec/batch.
2022-10-20 18:58:10,705 step [ 311], lr [0.0001500], embedding loss [ 0.8256], quantization loss [ 0.0198],  7.81 sec/batch.
2022-10-20 18:58:20,347 step [ 312], lr [0.0001500], embedding loss [ 0.8198], quantization loss [ 0.0198],  8.29 sec/batch.
2022-10-20 18:58:29,638 step [ 313], lr [0.0001500], embedding loss [ 0.8219], quantization loss [ 0.0192],  7.91 sec/batch.
2022-10-20 18:58:38,906 step [ 314], lr [0.0001500], embedding loss [ 0.8275], quantization loss [ 0.0184],  7.93 sec/batch.
2022-10-20 18:58:48,312 step [ 315], lr [0.0001500], embedding loss [ 0.8191], quantization loss [ 0.0215],  8.05 sec/batch.
2022-10-20 18:58:59,466 step [ 316], lr [0.0001500], embedding loss [ 0.8178], quantization loss [ 0.0188],  9.79 sec/batch.
2022-10-20 18:59:10,414 step [ 317], lr [0.0001500], embedding loss [ 0.8232], quantization loss [ 0.0198],  8.41 sec/batch.
2022-10-20 18:59:23,416 step [ 318], lr [0.0001500], embedding loss [ 0.8265], quantization loss [ 0.0181], 11.65 sec/batch.
2022-10-20 18:59:33,105 step [ 319], lr [0.0001500], embedding loss [ 0.8110], quantization loss [ 0.0197],  8.34 sec/batch.
2022-10-20 18:59:42,271 step [ 320], lr [0.0001500], embedding loss [ 0.8249], quantization loss [ 0.0227],  7.83 sec/batch.
2022-10-20 18:59:51,207 step [ 321], lr [0.0001500], embedding loss [ 0.8240], quantization loss [ 0.0197],  7.62 sec/batch.
2022-10-20 19:00:00,618 step [ 322], lr [0.0001500], embedding loss [ 0.8153], quantization loss [ 0.0184],  8.08 sec/batch.
2022-10-20 19:00:09,546 step [ 323], lr [0.0001500], embedding loss [ 0.8257], quantization loss [ 0.0183],  7.57 sec/batch.
2022-10-20 19:00:18,759 step [ 324], lr [0.0001500], embedding loss [ 0.8180], quantization loss [ 0.0180],  7.85 sec/batch.
2022-10-20 19:00:32,183 step [ 325], lr [0.0001500], embedding loss [ 0.8214], quantization loss [ 0.0187], 12.07 sec/batch.
2022-10-20 19:00:41,099 step [ 326], lr [0.0001500], embedding loss [ 0.8238], quantization loss [ 0.0194],  7.55 sec/batch.
2022-10-20 19:00:49,921 step [ 327], lr [0.0001500], embedding loss [ 0.8235], quantization loss [ 0.0188],  7.51 sec/batch.
2022-10-20 19:00:59,172 step [ 328], lr [0.0001500], embedding loss [ 0.8273], quantization loss [ 0.0179],  7.85 sec/batch.
2022-10-20 19:01:08,277 step [ 329], lr [0.0001500], embedding loss [ 0.8311], quantization loss [ 0.0187],  7.69 sec/batch.
2022-10-20 19:01:17,446 step [ 330], lr [0.0001500], embedding loss [ 0.8214], quantization loss [ 0.0182],  7.82 sec/batch.
2022-10-20 19:01:26,706 step [ 331], lr [0.0001500], embedding loss [ 0.8231], quantization loss [ 0.0193],  7.91 sec/batch.
2022-10-20 19:01:35,912 step [ 332], lr [0.0001500], embedding loss [ 0.8165], quantization loss [ 0.0193],  7.87 sec/batch.
2022-10-20 19:01:44,866 step [ 333], lr [0.0001500], embedding loss [ 0.8141], quantization loss [ 0.0185],  7.60 sec/batch.
2022-10-20 19:01:53,831 step [ 334], lr [0.0001500], embedding loss [ 0.8276], quantization loss [ 0.0172],  7.62 sec/batch.
2022-10-20 19:02:03,349 step [ 335], lr [0.0001500], embedding loss [ 0.8287], quantization loss [ 0.0217],  8.15 sec/batch.
2022-10-20 19:02:12,326 step [ 336], lr [0.0001500], embedding loss [ 0.8246], quantization loss [ 0.0187],  7.61 sec/batch.
2022-10-20 19:02:21,487 step [ 337], lr [0.0001500], embedding loss [ 0.8182], quantization loss [ 0.0197],  7.82 sec/batch.
2022-10-20 19:02:30,281 step [ 338], lr [0.0001500], embedding loss [ 0.8274], quantization loss [ 0.0196],  7.53 sec/batch.
2022-10-20 19:02:39,389 step [ 339], lr [0.0001500], embedding loss [ 0.8295], quantization loss [ 0.0172],  7.76 sec/batch.
2022-10-20 19:02:48,360 step [ 340], lr [0.0001500], embedding loss [ 0.8232], quantization loss [ 0.0199],  7.68 sec/batch.
2022-10-20 19:02:57,256 step [ 341], lr [0.0001500], embedding loss [ 0.8277], quantization loss [ 0.0188],  7.59 sec/batch.
2022-10-20 19:03:06,632 step [ 342], lr [0.0001500], embedding loss [ 0.8201], quantization loss [ 0.0199],  8.02 sec/batch.
2022-10-20 19:03:15,542 step [ 343], lr [0.0001500], embedding loss [ 0.8224], quantization loss [ 0.0179],  7.62 sec/batch.
2022-10-20 19:03:24,688 step [ 344], lr [0.0001500], embedding loss [ 0.8187], quantization loss [ 0.0189],  7.79 sec/batch.
2022-10-20 19:03:33,730 step [ 345], lr [0.0001500], embedding loss [ 0.8251], quantization loss [ 0.0183],  7.68 sec/batch.
2022-10-20 19:03:43,196 step [ 346], lr [0.0001500], embedding loss [ 0.8187], quantization loss [ 0.0197],  8.08 sec/batch.
2022-10-20 19:03:52,179 step [ 347], lr [0.0001500], embedding loss [ 0.8247], quantization loss [ 0.0254],  7.62 sec/batch.
2022-10-20 19:04:01,122 step [ 348], lr [0.0001500], embedding loss [ 0.8280], quantization loss [ 0.0179],  7.60 sec/batch.
2022-10-20 19:04:10,219 step [ 349], lr [0.0001500], embedding loss [ 0.8244], quantization loss [ 0.0200],  7.76 sec/batch.
2022-10-20 19:04:19,183 step [ 350], lr [0.0001500], embedding loss [ 0.8292], quantization loss [ 0.0180],  7.64 sec/batch.
2022-10-20 19:04:28,130 step [ 351], lr [0.0001500], embedding loss [ 0.8153], quantization loss [ 0.0186],  7.62 sec/batch.
2022-10-20 19:04:37,484 step [ 352], lr [0.0001500], embedding loss [ 0.8294], quantization loss [ 0.0165],  7.96 sec/batch.
2022-10-20 19:04:46,367 step [ 353], lr [0.0001500], embedding loss [ 0.8212], quantization loss [ 0.0189],  7.58 sec/batch.
2022-10-20 19:04:55,477 step [ 354], lr [0.0001500], embedding loss [ 0.8229], quantization loss [ 0.0180],  7.74 sec/batch.
2022-10-20 19:05:04,822 step [ 355], lr [0.0001500], embedding loss [ 0.8153], quantization loss [ 0.0164],  7.97 sec/batch.
2022-10-20 19:05:14,030 step [ 356], lr [0.0001500], embedding loss [ 0.8239], quantization loss [ 0.0178],  7.82 sec/batch.
2022-10-20 19:05:23,303 step [ 357], lr [0.0001500], embedding loss [ 0.8249], quantization loss [ 0.0182],  7.91 sec/batch.
2022-10-20 19:05:32,797 step [ 358], lr [0.0001500], embedding loss [ 0.8247], quantization loss [ 0.0164],  8.15 sec/batch.
2022-10-20 19:05:41,834 step [ 359], lr [0.0001500], embedding loss [ 0.8254], quantization loss [ 0.0192],  7.65 sec/batch.
2022-10-20 19:05:50,726 step [ 360], lr [0.0001500], embedding loss [ 0.8213], quantization loss [ 0.0211],  7.54 sec/batch.
2022-10-20 19:05:59,942 step [ 361], lr [0.0001500], embedding loss [ 0.8295], quantization loss [ 0.0175],  7.90 sec/batch.
2022-10-20 19:06:08,999 step [ 362], lr [0.0001500], embedding loss [ 0.8229], quantization loss [ 0.0176],  7.70 sec/batch.
2022-10-20 19:06:17,871 step [ 363], lr [0.0001500], embedding loss [ 0.8165], quantization loss [ 0.0200],  7.57 sec/batch.
2022-10-20 19:06:26,971 step [ 364], lr [0.0001500], embedding loss [ 0.8177], quantization loss [ 0.0174],  7.76 sec/batch.
2022-10-20 19:06:36,276 step [ 365], lr [0.0001500], embedding loss [ 0.8303], quantization loss [ 0.0164],  7.96 sec/batch.
2022-10-20 19:06:45,142 step [ 366], lr [0.0001500], embedding loss [ 0.8148], quantization loss [ 0.0188],  7.51 sec/batch.
2022-10-20 19:06:53,971 step [ 367], lr [0.0001500], embedding loss [ 0.8218], quantization loss [ 0.0175],  7.45 sec/batch.
2022-10-20 19:07:03,073 step [ 368], lr [0.0001500], embedding loss [ 0.8300], quantization loss [ 0.0170],  7.74 sec/batch.
2022-10-20 19:07:12,157 step [ 369], lr [0.0001500], embedding loss [ 0.8296], quantization loss [ 0.0164],  7.73 sec/batch.
2022-10-20 19:07:21,030 step [ 370], lr [0.0001500], embedding loss [ 0.8145], quantization loss [ 0.0168],  7.53 sec/batch.
2022-10-20 19:07:29,825 step [ 371], lr [0.0001500], embedding loss [ 0.8232], quantization loss [ 0.0170],  7.44 sec/batch.
2022-10-20 19:07:38,971 step [ 372], lr [0.0001500], embedding loss [ 0.8286], quantization loss [ 0.0181],  7.79 sec/batch.
2022-10-20 19:07:48,257 step [ 373], lr [0.0001500], embedding loss [ 0.8107], quantization loss [ 0.0191],  7.93 sec/batch.
2022-10-20 19:07:57,094 step [ 374], lr [0.0001500], embedding loss [ 0.8154], quantization loss [ 0.0165],  7.53 sec/batch.
2022-10-20 19:08:06,276 step [ 375], lr [0.0001500], embedding loss [ 0.8214], quantization loss [ 0.0170],  7.82 sec/batch.
2022-10-20 19:08:15,170 step [ 376], lr [0.0001500], embedding loss [ 0.8220], quantization loss [ 0.0191],  7.53 sec/batch.
2022-10-20 19:08:24,940 step [ 377], lr [0.0001500], embedding loss [ 0.8181], quantization loss [ 0.0177],  8.35 sec/batch.
2022-10-20 19:08:33,834 step [ 378], lr [0.0001500], embedding loss [ 0.8205], quantization loss [ 0.0175],  7.51 sec/batch.
2022-10-20 19:08:43,403 step [ 379], lr [0.0001500], embedding loss [ 0.8205], quantization loss [ 0.0181],  8.18 sec/batch.
2022-10-20 19:08:52,936 step [ 380], lr [0.0001500], embedding loss [ 0.8245], quantization loss [ 0.0207],  8.10 sec/batch.
2022-10-20 19:09:01,970 step [ 381], lr [0.0001500], embedding loss [ 0.8234], quantization loss [ 0.0185],  7.70 sec/batch.
2022-10-20 19:09:11,015 step [ 382], lr [0.0001500], embedding loss [ 0.8321], quantization loss [ 0.0161],  7.65 sec/batch.
2022-10-20 19:09:20,274 step [ 383], lr [0.0001500], embedding loss [ 0.8294], quantization loss [ 0.0176],  7.90 sec/batch.
2022-10-20 19:09:29,172 step [ 384], lr [0.0001500], embedding loss [ 0.8222], quantization loss [ 0.0191],  7.53 sec/batch.
2022-10-20 19:09:38,430 step [ 385], lr [0.0001500], embedding loss [ 0.8229], quantization loss [ 0.0202],  7.91 sec/batch.
2022-10-20 19:09:47,510 step [ 386], lr [0.0001500], embedding loss [ 0.8221], quantization loss [ 0.0161],  7.66 sec/batch.
2022-10-20 19:09:56,567 step [ 387], lr [0.0001500], embedding loss [ 0.8255], quantization loss [ 0.0179],  7.67 sec/batch.
2022-10-20 19:10:05,821 step [ 388], lr [0.0001500], embedding loss [ 0.8235], quantization loss [ 0.0165],  7.87 sec/batch.
2022-10-20 19:10:15,127 step [ 389], lr [0.0001500], embedding loss [ 0.8196], quantization loss [ 0.0162],  7.91 sec/batch.
2022-10-20 19:10:24,548 step [ 390], lr [0.0001500], embedding loss [ 0.8242], quantization loss [ 0.0170],  8.02 sec/batch.
2022-10-20 19:10:34,066 step [ 391], lr [0.0001500], embedding loss [ 0.8250], quantization loss [ 0.0181],  8.18 sec/batch.
2022-10-20 19:10:43,856 step [ 392], lr [0.0001500], embedding loss [ 0.8178], quantization loss [ 0.0176],  8.42 sec/batch.
2022-10-20 19:10:53,520 step [ 393], lr [0.0001500], embedding loss [ 0.8173], quantization loss [ 0.0224],  8.30 sec/batch.
2022-10-20 19:11:02,755 step [ 394], lr [0.0001500], embedding loss [ 0.8195], quantization loss [ 0.0172],  7.90 sec/batch.
2022-10-20 19:11:11,664 step [ 395], lr [0.0001500], embedding loss [ 0.8198], quantization loss [ 0.0179],  7.57 sec/batch.
2022-10-20 19:11:20,892 step [ 396], lr [0.0001500], embedding loss [ 0.8225], quantization loss [ 0.0172],  7.83 sec/batch.
2022-10-20 19:11:30,001 step [ 397], lr [0.0001500], embedding loss [ 0.8218], quantization loss [ 0.0192],  7.75 sec/batch.
2022-10-20 19:11:38,923 step [ 398], lr [0.0001500], embedding loss [ 0.8241], quantization loss [ 0.0179],  7.55 sec/batch.
2022-10-20 19:11:48,047 step [ 399], lr [0.0001500], embedding loss [ 0.8172], quantization loss [ 0.0192],  7.77 sec/batch.
2022-10-20 19:11:57,465 step [ 400], lr [0.0001500], embedding loss [ 0.8193], quantization loss [ 0.0185],  8.04 sec/batch.
2022-10-20 19:12:06,401 step [ 401], lr [0.0001500], embedding loss [ 0.8309], quantization loss [ 0.0198],  7.60 sec/batch.
2022-10-20 19:12:15,836 step [ 402], lr [0.0001500], embedding loss [ 0.8252], quantization loss [ 0.0189],  8.03 sec/batch.
2022-10-20 19:12:24,892 step [ 403], lr [0.0001500], embedding loss [ 0.8239], quantization loss [ 0.0171],  7.67 sec/batch.
2022-10-20 19:12:33,715 step [ 404], lr [0.0001500], embedding loss [ 0.8224], quantization loss [ 0.0169],  7.48 sec/batch.
2022-10-20 19:12:42,724 step [ 405], lr [0.0001500], embedding loss [ 0.8195], quantization loss [ 0.0178],  7.63 sec/batch.
2022-10-20 19:12:51,868 step [ 406], lr [0.0001500], embedding loss [ 0.8323], quantization loss [ 0.0188],  7.75 sec/batch.
2022-10-20 19:13:01,500 step [ 407], lr [0.0001500], embedding loss [ 0.8236], quantization loss [ 0.0154],  8.23 sec/batch.
2022-10-20 19:13:10,545 step [ 408], lr [0.0001500], embedding loss [ 0.8252], quantization loss [ 0.0181],  7.69 sec/batch.
2022-10-20 19:13:19,550 step [ 409], lr [0.0001500], embedding loss [ 0.8198], quantization loss [ 0.0174],  7.66 sec/batch.
2022-10-20 19:13:28,637 step [ 410], lr [0.0001500], embedding loss [ 0.8252], quantization loss [ 0.0178],  7.72 sec/batch.
2022-10-20 19:13:37,573 step [ 411], lr [0.0001500], embedding loss [ 0.8313], quantization loss [ 0.0173],  7.57 sec/batch.
2022-10-20 19:13:46,668 step [ 412], lr [0.0001500], embedding loss [ 0.8187], quantization loss [ 0.0170],  7.75 sec/batch.
2022-10-20 19:13:55,567 step [ 413], lr [0.0001500], embedding loss [ 0.8194], quantization loss [ 0.0193],  7.58 sec/batch.
2022-10-20 19:14:04,821 step [ 414], lr [0.0001500], embedding loss [ 0.8175], quantization loss [ 0.0160],  7.88 sec/batch.
2022-10-20 19:14:14,265 step [ 415], lr [0.0001500], embedding loss [ 0.8255], quantization loss [ 0.0186],  8.06 sec/batch.
2022-10-20 19:14:23,376 step [ 416], lr [0.0001500], embedding loss [ 0.8258], quantization loss [ 0.0176],  7.73 sec/batch.
2022-10-20 19:14:32,419 step [ 417], lr [0.0001500], embedding loss [ 0.8129], quantization loss [ 0.0180],  7.65 sec/batch.
2022-10-20 19:14:41,289 step [ 418], lr [0.0001500], embedding loss [ 0.8221], quantization loss [ 0.0172],  7.59 sec/batch.
2022-10-20 19:14:50,174 step [ 419], lr [0.0001500], embedding loss [ 0.8319], quantization loss [ 0.0167],  7.50 sec/batch.
2022-10-20 19:14:59,458 step [ 420], lr [0.0001500], embedding loss [ 0.8242], quantization loss [ 0.0186],  7.92 sec/batch.
2022-10-20 19:15:03,965 step [ 421], lr [0.0001500], embedding loss [ 0.8240], quantization loss [ 0.0186],  3.12 sec/batch.
2022-10-20 19:15:13,213 step [ 422], lr [0.0001500], embedding loss [ 0.8170], quantization loss [ 0.0177],  7.88 sec/batch.
2022-10-20 19:15:22,093 step [ 423], lr [0.0001500], embedding loss [ 0.8177], quantization loss [ 0.0172],  7.53 sec/batch.
2022-10-20 19:15:31,318 step [ 424], lr [0.0001500], embedding loss [ 0.8210], quantization loss [ 0.0172],  7.79 sec/batch.
2022-10-20 19:15:40,347 step [ 425], lr [0.0001500], embedding loss [ 0.8192], quantization loss [ 0.0175],  7.63 sec/batch.
2022-10-20 19:15:49,416 step [ 426], lr [0.0001500], embedding loss [ 0.8169], quantization loss [ 0.0180],  7.65 sec/batch.
2022-10-20 19:15:58,261 step [ 427], lr [0.0001500], embedding loss [ 0.8205], quantization loss [ 0.0177],  7.48 sec/batch.
2022-10-20 19:16:07,098 step [ 428], lr [0.0001500], embedding loss [ 0.8254], quantization loss [ 0.0183],  7.49 sec/batch.
2022-10-20 19:16:16,069 step [ 429], lr [0.0001500], embedding loss [ 0.8323], quantization loss [ 0.0174],  7.59 sec/batch.
2022-10-20 19:16:25,107 step [ 430], lr [0.0001500], embedding loss [ 0.8186], quantization loss [ 0.0165],  7.63 sec/batch.
2022-10-20 19:16:34,035 step [ 431], lr [0.0001500], embedding loss [ 0.8284], quantization loss [ 0.0179],  7.57 sec/batch.
2022-10-20 19:16:42,967 step [ 432], lr [0.0001500], embedding loss [ 0.8236], quantization loss [ 0.0173],  7.57 sec/batch.
2022-10-20 19:16:51,833 step [ 433], lr [0.0001500], embedding loss [ 0.8206], quantization loss [ 0.0186],  7.47 sec/batch.
2022-10-20 19:17:01,116 step [ 434], lr [0.0001500], embedding loss [ 0.8212], quantization loss [ 0.0184],  7.89 sec/batch.
2022-10-20 19:17:10,131 step [ 435], lr [0.0001500], embedding loss [ 0.8218], quantization loss [ 0.0177],  7.61 sec/batch.
2022-10-20 19:17:19,279 step [ 436], lr [0.0001500], embedding loss [ 0.8225], quantization loss [ 0.0178],  7.74 sec/batch.
2022-10-20 19:17:28,369 step [ 437], lr [0.0001500], embedding loss [ 0.8204], quantization loss [ 0.0186],  7.70 sec/batch.
2022-10-20 19:17:37,296 step [ 438], lr [0.0001500], embedding loss [ 0.8210], quantization loss [ 0.0170],  7.55 sec/batch.
2022-10-20 19:17:46,316 step [ 439], lr [0.0001500], embedding loss [ 0.8217], quantization loss [ 0.0167],  7.68 sec/batch.
2022-10-20 19:17:46,316 update codes and centers iter(1/1).
2022-10-20 19:17:48,221 number of update_code wrong: 0.
2022-10-20 19:17:50,750 non zero codewords: 256.
2022-10-20 19:17:50,750 finish center update, duration: 4.43 sec.
2022-10-20 19:17:59,510 step [ 440], lr [0.0001500], embedding loss [ 0.8256], quantization loss [ 0.0126],  7.48 sec/batch.
2022-10-20 19:18:08,587 step [ 441], lr [0.0001500], embedding loss [ 0.8190], quantization loss [ 0.0119],  7.65 sec/batch.
2022-10-20 19:18:17,815 step [ 442], lr [0.0001500], embedding loss [ 0.8299], quantization loss [ 0.0120],  7.81 sec/batch.
2022-10-20 19:18:27,547 step [ 443], lr [0.0001500], embedding loss [ 0.8178], quantization loss [ 0.0115],  8.34 sec/batch.
2022-10-20 19:18:36,579 step [ 444], lr [0.0001500], embedding loss [ 0.8182], quantization loss [ 0.0130],  7.63 sec/batch.
2022-10-20 19:18:45,516 step [ 445], lr [0.0001500], embedding loss [ 0.8209], quantization loss [ 0.0131],  7.58 sec/batch.
2022-10-20 19:18:54,517 step [ 446], lr [0.0001500], embedding loss [ 0.8140], quantization loss [ 0.0107],  7.61 sec/batch.
2022-10-20 19:19:08,021 step [ 447], lr [0.0001500], embedding loss [ 0.8222], quantization loss [ 0.0133], 12.12 sec/batch.
2022-10-20 19:19:16,967 step [ 448], lr [0.0001500], embedding loss [ 0.8248], quantization loss [ 0.0117],  7.58 sec/batch.
2022-10-20 19:19:26,061 step [ 449], lr [0.0001500], embedding loss [ 0.8297], quantization loss [ 0.0125],  7.72 sec/batch.
2022-10-20 19:19:35,482 step [ 450], lr [0.0001500], embedding loss [ 0.8296], quantization loss [ 0.0128],  8.03 sec/batch.
2022-10-20 19:19:45,027 step [ 451], lr [0.0001500], embedding loss [ 0.8292], quantization loss [ 0.0114],  8.12 sec/batch.
2022-10-20 19:19:54,275 step [ 452], lr [0.0001500], embedding loss [ 0.8208], quantization loss [ 0.0121],  7.84 sec/batch.
2022-10-20 19:20:03,433 step [ 453], lr [0.0001500], embedding loss [ 0.8141], quantization loss [ 0.0118],  7.73 sec/batch.
2022-10-20 19:20:16,904 step [ 454], lr [0.0001500], embedding loss [ 0.8222], quantization loss [ 0.0115], 12.07 sec/batch.
2022-10-20 19:20:25,918 step [ 455], lr [0.0001500], embedding loss [ 0.8251], quantization loss [ 0.0122],  7.63 sec/batch.
2022-10-20 19:20:35,130 step [ 456], lr [0.0001500], embedding loss [ 0.8198], quantization loss [ 0.0125],  7.82 sec/batch.
2022-10-20 19:20:44,190 step [ 457], lr [0.0001500], embedding loss [ 0.8216], quantization loss [ 0.0126],  7.66 sec/batch.
2022-10-20 19:20:53,269 step [ 458], lr [0.0001500], embedding loss [ 0.8205], quantization loss [ 0.0121],  7.69 sec/batch.
2022-10-20 19:21:02,357 step [ 459], lr [0.0001500], embedding loss [ 0.8159], quantization loss [ 0.0114],  7.72 sec/batch.
2022-10-20 19:21:11,252 step [ 460], lr [0.0001500], embedding loss [ 0.8225], quantization loss [ 0.0123],  7.50 sec/batch.
2022-10-20 19:21:20,498 step [ 461], lr [0.0001500], embedding loss [ 0.8194], quantization loss [ 0.0119],  7.83 sec/batch.
2022-10-20 19:21:33,054 step [ 462], lr [0.0001500], embedding loss [ 0.8256], quantization loss [ 0.0121], 11.15 sec/batch.
2022-10-20 19:21:43,066 step [ 463], lr [0.0001500], embedding loss [ 0.8241], quantization loss [ 0.0120],  7.65 sec/batch.
2022-10-20 19:21:56,630 step [ 464], lr [0.0001500], embedding loss [ 0.8174], quantization loss [ 0.0115], 12.16 sec/batch.
2022-10-20 19:22:05,631 step [ 465], lr [0.0001500], embedding loss [ 0.8240], quantization loss [ 0.0112],  7.59 sec/batch.
2022-10-20 19:22:14,839 step [ 466], lr [0.0001500], embedding loss [ 0.8241], quantization loss [ 0.0117],  7.83 sec/batch.
2022-10-20 19:22:24,156 step [ 467], lr [0.0001500], embedding loss [ 0.8244], quantization loss [ 0.0120],  7.93 sec/batch.
2022-10-20 19:22:33,228 step [ 468], lr [0.0001500], embedding loss [ 0.8233], quantization loss [ 0.0115],  7.69 sec/batch.
2022-10-20 19:22:42,221 step [ 469], lr [0.0001500], embedding loss [ 0.8231], quantization loss [ 0.0124],  7.66 sec/batch.
2022-10-20 19:22:51,637 step [ 470], lr [0.0001500], embedding loss [ 0.8196], quantization loss [ 0.0119],  8.02 sec/batch.
2022-10-20 19:23:04,798 step [ 471], lr [0.0001500], embedding loss [ 0.8149], quantization loss [ 0.0121], 11.76 sec/batch.
2022-10-20 19:23:14,636 step [ 472], lr [0.0001500], embedding loss [ 0.8247], quantization loss [ 0.0115],  8.41 sec/batch.
2022-10-20 19:23:27,318 step [ 473], lr [0.0001500], embedding loss [ 0.8174], quantization loss [ 0.0113], 10.12 sec/batch.
2022-10-20 19:23:36,319 step [ 474], lr [0.0001500], embedding loss [ 0.8228], quantization loss [ 0.0120],  7.60 sec/batch.
2022-10-20 19:23:45,672 step [ 475], lr [0.0001500], embedding loss [ 0.8155], quantization loss [ 0.0121],  7.94 sec/batch.
2022-10-20 19:23:54,558 step [ 476], lr [0.0001500], embedding loss [ 0.8206], quantization loss [ 0.0108],  7.50 sec/batch.
2022-10-20 19:24:03,717 step [ 477], lr [0.0001500], embedding loss [ 0.8294], quantization loss [ 0.0124],  7.76 sec/batch.
2022-10-20 19:24:12,652 step [ 478], lr [0.0001500], embedding loss [ 0.8210], quantization loss [ 0.0111],  7.56 sec/batch.
2022-10-20 19:24:21,797 step [ 479], lr [0.0001500], embedding loss [ 0.8279], quantization loss [ 0.0119],  7.76 sec/batch.
2022-10-20 19:24:30,908 step [ 480], lr [0.0001500], embedding loss [ 0.8222], quantization loss [ 0.0106],  7.74 sec/batch.
2022-10-20 19:24:44,455 step [ 481], lr [0.0001500], embedding loss [ 0.8254], quantization loss [ 0.0121], 12.11 sec/batch.
2022-10-20 19:24:53,322 step [ 482], lr [0.0001500], embedding loss [ 0.8297], quantization loss [ 0.0125],  7.46 sec/batch.
2022-10-20 19:25:02,492 step [ 483], lr [0.0001500], embedding loss [ 0.8209], quantization loss [ 0.0113],  7.71 sec/batch.
2022-10-20 19:25:11,817 step [ 484], lr [0.0001500], embedding loss [ 0.8242], quantization loss [ 0.0108],  7.92 sec/batch.
2022-10-20 19:25:21,114 step [ 485], lr [0.0001500], embedding loss [ 0.8182], quantization loss [ 0.0109],  7.90 sec/batch.
2022-10-20 19:25:30,514 step [ 486], lr [0.0001500], embedding loss [ 0.8229], quantization loss [ 0.0112],  8.01 sec/batch.
2022-10-20 19:25:39,834 step [ 487], lr [0.0001500], embedding loss [ 0.8265], quantization loss [ 0.0123],  7.95 sec/batch.
2022-10-20 19:25:53,544 step [ 488], lr [0.0001500], embedding loss [ 0.8214], quantization loss [ 0.0124], 12.32 sec/batch.
2022-10-20 19:26:04,195 step [ 489], lr [0.0001500], embedding loss [ 0.8234], quantization loss [ 0.0133],  9.28 sec/batch.
2022-10-20 19:26:16,356 step [ 490], lr [0.0001500], embedding loss [ 0.8163], quantization loss [ 0.0117],  9.45 sec/batch.
2022-10-20 19:26:25,578 step [ 491], lr [0.0001500], embedding loss [ 0.8205], quantization loss [ 0.0122],  7.82 sec/batch.
2022-10-20 19:26:35,121 step [ 492], lr [0.0001500], embedding loss [ 0.8193], quantization loss [ 0.0122],  8.14 sec/batch.
2022-10-20 19:26:44,340 step [ 493], lr [0.0001500], embedding loss [ 0.8193], quantization loss [ 0.0129],  7.83 sec/batch.
2022-10-20 19:26:53,235 step [ 494], lr [0.0001500], embedding loss [ 0.8110], quantization loss [ 0.0123],  7.53 sec/batch.
2022-10-20 19:27:02,096 step [ 495], lr [0.0001500], embedding loss [ 0.8266], quantization loss [ 0.0121],  7.47 sec/batch.
2022-10-20 19:27:11,060 step [ 496], lr [0.0001500], embedding loss [ 0.8249], quantization loss [ 0.0121],  7.57 sec/batch.
2022-10-20 19:27:20,081 step [ 497], lr [0.0001500], embedding loss [ 0.8330], quantization loss [ 0.0122],  7.64 sec/batch.
2022-10-20 19:27:29,104 step [ 498], lr [0.0001500], embedding loss [ 0.8203], quantization loss [ 0.0123],  7.65 sec/batch.
2022-10-20 19:27:37,956 step [ 499], lr [0.0001500], embedding loss [ 0.8154], quantization loss [ 0.0127],  7.49 sec/batch.
2022-10-20 19:27:47,550 step [ 500], lr [0.0001500], embedding loss [ 0.8246], quantization loss [ 0.0110],  8.20 sec/batch.
2022-10-20 19:27:56,808 step [ 501], lr [0.0001500], embedding loss [ 0.8277], quantization loss [ 0.0118],  7.84 sec/batch.
2022-10-20 19:28:05,830 step [ 502], lr [0.0001500], embedding loss [ 0.8307], quantization loss [ 0.0119],  7.63 sec/batch.
2022-10-20 19:28:07,825 step [ 503], lr [0.0001500], embedding loss [ 0.8277], quantization loss [ 0.0117],  0.58 sec/batch.
2022-10-20 19:28:17,613 step [ 504], lr [0.0001500], embedding loss [ 0.8232], quantization loss [ 0.0119],  8.38 sec/batch.
2022-10-20 19:28:31,610 step [ 505], lr [0.0001500], embedding loss [ 0.8249], quantization loss [ 0.0110], 12.59 sec/batch.
2022-10-20 19:28:40,583 step [ 506], lr [0.0001500], embedding loss [ 0.8180], quantization loss [ 0.0113],  7.58 sec/batch.
2022-10-20 19:28:49,564 step [ 507], lr [0.0001500], embedding loss [ 0.8234], quantization loss [ 0.0118],  7.59 sec/batch.
2022-10-20 19:28:58,745 step [ 508], lr [0.0001500], embedding loss [ 0.8145], quantization loss [ 0.0116],  7.77 sec/batch.
2022-10-20 19:29:07,888 step [ 509], lr [0.0001500], embedding loss [ 0.8255], quantization loss [ 0.0108],  7.76 sec/batch.
2022-10-20 19:29:16,810 step [ 510], lr [0.0001500], embedding loss [ 0.8217], quantization loss [ 0.0119],  7.50 sec/batch.
2022-10-20 19:29:25,728 step [ 511], lr [0.0001500], embedding loss [ 0.8243], quantization loss [ 0.0114],  7.58 sec/batch.
2022-10-20 19:29:35,072 step [ 512], lr [0.0001500], embedding loss [ 0.8245], quantization loss [ 0.0118],  7.92 sec/batch.
2022-10-20 19:29:46,257 step [ 513], lr [0.0001500], embedding loss [ 0.8275], quantization loss [ 0.0106],  9.78 sec/batch.
2022-10-20 19:29:57,355 step [ 514], lr [0.0001500], embedding loss [ 0.8203], quantization loss [ 0.0108],  8.41 sec/batch.
2022-10-20 19:30:10,748 step [ 515], lr [0.0001500], embedding loss [ 0.8200], quantization loss [ 0.0116], 12.03 sec/batch.
2022-10-20 19:30:19,757 step [ 516], lr [0.0001500], embedding loss [ 0.8207], quantization loss [ 0.0111],  7.62 sec/batch.
2022-10-20 19:30:28,657 step [ 517], lr [0.0001500], embedding loss [ 0.8275], quantization loss [ 0.0124],  7.48 sec/batch.
2022-10-20 19:30:37,672 step [ 518], lr [0.0001500], embedding loss [ 0.8246], quantization loss [ 0.0110],  7.62 sec/batch.
2022-10-20 19:30:46,845 step [ 519], lr [0.0001500], embedding loss [ 0.8206], quantization loss [ 0.0118],  7.79 sec/batch.
2022-10-20 19:30:55,953 step [ 520], lr [0.0001500], embedding loss [ 0.8193], quantization loss [ 0.0120],  7.73 sec/batch.
2022-10-20 19:31:05,027 step [ 521], lr [0.0001500], embedding loss [ 0.8247], quantization loss [ 0.0120],  7.68 sec/batch.
2022-10-20 19:31:17,805 step [ 522], lr [0.0001500], embedding loss [ 0.8185], quantization loss [ 0.0109], 11.40 sec/batch.
2022-10-20 19:31:27,361 step [ 523], lr [0.0001500], embedding loss [ 0.8224], quantization loss [ 0.0109],  7.68 sec/batch.
2022-10-20 19:31:36,374 step [ 524], lr [0.0001500], embedding loss [ 0.8198], quantization loss [ 0.0123],  7.63 sec/batch.
2022-10-20 19:31:45,251 step [ 525], lr [0.0001500], embedding loss [ 0.8222], quantization loss [ 0.0117],  7.52 sec/batch.
2022-10-20 19:31:54,138 step [ 526], lr [0.0001500], embedding loss [ 0.8236], quantization loss [ 0.0112],  7.53 sec/batch.
2022-10-20 19:32:03,372 step [ 527], lr [0.0001500], embedding loss [ 0.8192], quantization loss [ 0.0117],  7.83 sec/batch.
2022-10-20 19:32:12,502 step [ 528], lr [0.0001500], embedding loss [ 0.8158], quantization loss [ 0.0118],  7.74 sec/batch.
2022-10-20 19:32:21,579 step [ 529], lr [0.0001500], embedding loss [ 0.8178], quantization loss [ 0.0101],  7.67 sec/batch.
2022-10-20 19:32:30,928 step [ 530], lr [0.0001500], embedding loss [ 0.8278], quantization loss [ 0.0123],  7.91 sec/batch.
2022-10-20 19:32:44,408 step [ 531], lr [0.0001500], embedding loss [ 0.8206], quantization loss [ 0.0117], 12.07 sec/batch.
2022-10-20 19:32:46,423 step [ 532], lr [0.0001500], embedding loss [ 0.8164], quantization loss [ 0.0118],  0.59 sec/batch.
2022-10-20 19:32:48,482 step [ 533], lr [0.0001500], embedding loss [ 0.8284], quantization loss [ 0.0111],  0.61 sec/batch.
2022-10-20 19:32:50,490 step [ 534], lr [0.0001500], embedding loss [ 0.8132], quantization loss [ 0.0113],  0.60 sec/batch.
2022-10-20 19:32:52,516 step [ 535], lr [0.0001500], embedding loss [ 0.8203], quantization loss [ 0.0121],  0.61 sec/batch.
2022-10-20 19:32:54,532 step [ 536], lr [0.0001500], embedding loss [ 0.8177], quantization loss [ 0.0113],  0.61 sec/batch.
2022-10-20 19:32:56,582 step [ 537], lr [0.0001500], embedding loss [ 0.8229], quantization loss [ 0.0116],  0.59 sec/batch.
2022-10-20 19:32:58,569 step [ 538], lr [0.0001500], embedding loss [ 0.8265], quantization loss [ 0.0114],  0.61 sec/batch.
2022-10-20 19:33:00,630 step [ 539], lr [0.0001500], embedding loss [ 0.8264], quantization loss [ 0.0105],  0.60 sec/batch.
2022-10-20 19:33:02,699 step [ 540], lr [0.0001500], embedding loss [ 0.8179], quantization loss [ 0.0109],  0.59 sec/batch.
2022-10-20 19:33:04,739 step [ 541], lr [0.0001500], embedding loss [ 0.8218], quantization loss [ 0.0106],  0.59 sec/batch.
2022-10-20 19:33:09,426 step [ 542], lr [0.0001500], embedding loss [ 0.8228], quantization loss [ 0.0109],  3.29 sec/batch.
2022-10-20 19:33:11,455 step [ 543], lr [0.0001500], embedding loss [ 0.8277], quantization loss [ 0.0112],  0.67 sec/batch.
2022-10-20 19:33:13,505 step [ 544], lr [0.0001500], embedding loss [ 0.8244], quantization loss [ 0.0117],  0.59 sec/batch.
2022-10-20 19:33:18,082 step [ 545], lr [0.0001500], embedding loss [ 0.8217], quantization loss [ 0.0122],  3.21 sec/batch.
2022-10-20 19:33:20,107 step [ 546], lr [0.0001500], embedding loss [ 0.8208], quantization loss [ 0.0116],  0.60 sec/batch.
2022-10-20 19:33:27,479 step [ 547], lr [0.0001500], embedding loss [ 0.8317], quantization loss [ 0.0102],  5.96 sec/batch.
2022-10-20 19:33:32,072 step [ 548], lr [0.0001500], embedding loss [ 0.8237], quantization loss [ 0.0109],  3.17 sec/batch.
2022-10-20 19:33:34,167 step [ 549], lr [0.0001500], embedding loss [ 0.8314], quantization loss [ 0.0113],  0.60 sec/batch.
2022-10-20 19:33:36,153 step [ 550], lr [0.0001500], embedding loss [ 0.8153], quantization loss [ 0.0101],  0.60 sec/batch.
2022-10-20 19:33:41,160 step [ 551], lr [0.0001500], embedding loss [ 0.8218], quantization loss [ 0.0110],  3.61 sec/batch.
2022-10-20 19:33:43,187 step [ 552], lr [0.0001500], embedding loss [ 0.8215], quantization loss [ 0.0098],  0.61 sec/batch.
2022-10-20 19:33:47,897 step [ 553], lr [0.0001500], embedding loss [ 0.8151], quantization loss [ 0.0112],  3.29 sec/batch.
2022-10-20 19:33:49,950 step [ 554], lr [0.0001500], embedding loss [ 0.8308], quantization loss [ 0.0126],  0.61 sec/batch.
2022-10-20 19:33:54,690 step [ 555], lr [0.0001500], embedding loss [ 0.8222], quantization loss [ 0.0106],  3.27 sec/batch.
2022-10-20 19:33:56,863 step [ 556], lr [0.0001500], embedding loss [ 0.8213], quantization loss [ 0.0130],  0.61 sec/batch.
2022-10-20 19:33:58,942 step [ 557], lr [0.0001500], embedding loss [ 0.8110], quantization loss [ 0.0117],  0.60 sec/batch.
2022-10-20 19:34:01,002 step [ 558], lr [0.0001500], embedding loss [ 0.8208], quantization loss [ 0.0103],  0.58 sec/batch.
2022-10-20 19:34:03,043 step [ 559], lr [0.0001500], embedding loss [ 0.8288], quantization loss [ 0.0117],  0.59 sec/batch.
2022-10-20 19:34:05,144 step [ 560], lr [0.0001500], embedding loss [ 0.8198], quantization loss [ 0.0113],  0.61 sec/batch.
2022-10-20 19:34:09,599 step [ 561], lr [0.0001500], embedding loss [ 0.8230], quantization loss [ 0.0121],  3.03 sec/batch.
2022-10-20 19:34:11,625 step [ 562], lr [0.0001500], embedding loss [ 0.8106], quantization loss [ 0.0108],  0.59 sec/batch.
2022-10-20 19:34:13,699 step [ 563], lr [0.0001500], embedding loss [ 0.8193], quantization loss [ 0.0108],  0.59 sec/batch.
2022-10-20 19:34:15,762 step [ 564], lr [0.0001500], embedding loss [ 0.8250], quantization loss [ 0.0117],  0.61 sec/batch.
2022-10-20 19:34:20,337 step [ 565], lr [0.0001500], embedding loss [ 0.8207], quantization loss [ 0.0096],  3.14 sec/batch.
2022-10-20 19:34:22,416 step [ 566], lr [0.0001500], embedding loss [ 0.8242], quantization loss [ 0.0109],  0.59 sec/batch.
2022-10-20 19:34:24,553 step [ 567], lr [0.0001500], embedding loss [ 0.8219], quantization loss [ 0.0104],  0.61 sec/batch.
2022-10-20 19:34:29,567 step [ 568], lr [0.0001500], embedding loss [ 0.8265], quantization loss [ 0.0107],  3.53 sec/batch.
2022-10-20 19:34:31,656 step [ 569], lr [0.0001500], embedding loss [ 0.8114], quantization loss [ 0.0115],  0.61 sec/batch.
2022-10-20 19:34:33,663 step [ 570], lr [0.0001500], embedding loss [ 0.8292], quantization loss [ 0.0118],  0.60 sec/batch.
2022-10-20 19:34:35,681 step [ 571], lr [0.0001500], embedding loss [ 0.8202], quantization loss [ 0.0110],  0.61 sec/batch.
2022-10-20 19:34:37,840 step [ 572], lr [0.0001500], embedding loss [ 0.8229], quantization loss [ 0.0108],  0.63 sec/batch.
2022-10-20 19:34:39,983 step [ 573], lr [0.0001500], embedding loss [ 0.8218], quantization loss [ 0.0116],  0.60 sec/batch.
2022-10-20 19:34:42,001 step [ 574], lr [0.0001500], embedding loss [ 0.8265], quantization loss [ 0.0114],  0.60 sec/batch.
2022-10-20 19:34:44,068 step [ 575], lr [0.0001500], embedding loss [ 0.8304], quantization loss [ 0.0117],  0.63 sec/batch.
2022-10-20 19:34:46,095 step [ 576], lr [0.0001500], embedding loss [ 0.8225], quantization loss [ 0.0102],  0.60 sec/batch.
2022-10-20 19:34:48,138 step [ 577], lr [0.0001500], embedding loss [ 0.8255], quantization loss [ 0.0113],  0.59 sec/batch.
2022-10-20 19:34:50,187 step [ 578], lr [0.0001500], embedding loss [ 0.8242], quantization loss [ 0.0109],  0.60 sec/batch.
2022-10-20 19:34:52,163 step [ 579], lr [0.0001500], embedding loss [ 0.8184], quantization loss [ 0.0113],  0.58 sec/batch.
2022-10-20 19:34:54,137 step [ 580], lr [0.0001500], embedding loss [ 0.8241], quantization loss [ 0.0108],  0.59 sec/batch.
2022-10-20 19:35:01,809 step [ 581], lr [0.0001500], embedding loss [ 0.8246], quantization loss [ 0.0106],  6.27 sec/batch.
2022-10-20 19:35:03,838 step [ 582], lr [0.0001500], embedding loss [ 0.8163], quantization loss [ 0.0115],  0.59 sec/batch.
2022-10-20 19:35:05,836 step [ 583], lr [0.0001500], embedding loss [ 0.8245], quantization loss [ 0.0106],  0.59 sec/batch.
2022-10-20 19:35:07,874 step [ 584], lr [0.0001500], embedding loss [ 0.8188], quantization loss [ 0.0124],  0.61 sec/batch.
2022-10-20 19:35:09,940 step [ 585], lr [0.0001500], embedding loss [ 0.8260], quantization loss [ 0.0112],  0.61 sec/batch.
2022-10-20 19:35:09,940 update codes and centers iter(1/1).
2022-10-20 19:35:11,859 number of update_code wrong: 0.
2022-10-20 19:35:14,556 non zero codewords: 256.
2022-10-20 19:35:14,556 finish center update, duration: 4.62 sec.
2022-10-20 19:35:25,713 step [ 586], lr [0.0001500], embedding loss [ 0.8111], quantization loss [ 0.0106],  9.88 sec/batch.
2022-10-20 19:35:27,827 step [ 587], lr [0.0001500], embedding loss [ 0.8283], quantization loss [ 0.0105],  0.60 sec/batch.
2022-10-20 19:35:32,688 step [ 588], lr [0.0001500], embedding loss [ 0.8269], quantization loss [ 0.0103],  3.39 sec/batch.
2022-10-20 19:35:37,246 step [ 589], lr [0.0001500], embedding loss [ 0.8166], quantization loss [ 0.0097],  3.09 sec/batch.
2022-10-20 19:35:42,094 step [ 590], lr [0.0001500], embedding loss [ 0.8332], quantization loss [ 0.0091],  3.39 sec/batch.
2022-10-20 19:35:49,661 step [ 591], lr [0.0001500], embedding loss [ 0.8152], quantization loss [ 0.0101],  6.11 sec/batch.
2022-10-20 19:35:51,784 step [ 592], lr [0.0001500], embedding loss [ 0.8215], quantization loss [ 0.0093],  0.60 sec/batch.
2022-10-20 19:35:53,864 step [ 593], lr [0.0001500], embedding loss [ 0.8187], quantization loss [ 0.0108],  0.60 sec/batch.
2022-10-20 19:35:55,872 step [ 594], lr [0.0001500], embedding loss [ 0.8220], quantization loss [ 0.0105],  0.59 sec/batch.
2022-10-20 19:35:57,927 step [ 595], lr [0.0001500], embedding loss [ 0.8257], quantization loss [ 0.0099],  0.59 sec/batch.
2022-10-20 19:36:00,070 step [ 596], lr [0.0001500], embedding loss [ 0.8287], quantization loss [ 0.0093],  0.61 sec/batch.
2022-10-20 19:36:02,174 step [ 597], lr [0.0001500], embedding loss [ 0.8198], quantization loss [ 0.0100],  0.61 sec/batch.
2022-10-20 19:36:07,226 step [ 598], lr [0.0001500], embedding loss [ 0.8230], quantization loss [ 0.0097],  3.57 sec/batch.
2022-10-20 19:36:09,321 step [ 599], lr [0.0001500], embedding loss [ 0.8199], quantization loss [ 0.0091],  0.60 sec/batch.
2022-10-20 19:36:14,124 step [ 600], lr [0.0001500], embedding loss [ 0.8226], quantization loss [ 0.0096],  3.36 sec/batch.
2022-10-20 19:36:16,162 step [ 601], lr [0.0000750], embedding loss [ 0.8205], quantization loss [ 0.0088],  0.60 sec/batch.
2022-10-20 19:36:18,161 step [ 602], lr [0.0000750], embedding loss [ 0.8216], quantization loss [ 0.0094],  0.59 sec/batch.
2022-10-20 19:36:22,847 step [ 603], lr [0.0000750], embedding loss [ 0.8205], quantization loss [ 0.0106],  3.29 sec/batch.
2022-10-20 19:36:24,921 step [ 604], lr [0.0000750], embedding loss [ 0.8201], quantization loss [ 0.0100],  0.61 sec/batch.
2022-10-20 19:36:26,960 step [ 605], lr [0.0000750], embedding loss [ 0.8264], quantization loss [ 0.0103],  0.59 sec/batch.
2022-10-20 19:36:29,035 step [ 606], lr [0.0000750], embedding loss [ 0.8152], quantization loss [ 0.0103],  0.59 sec/batch.
2022-10-20 19:36:31,055 step [ 607], lr [0.0000750], embedding loss [ 0.8235], quantization loss [ 0.0111],  0.59 sec/batch.
2022-10-20 19:36:33,071 step [ 608], lr [0.0000750], embedding loss [ 0.8195], quantization loss [ 0.0102],  0.60 sec/batch.
2022-10-20 19:36:35,088 step [ 609], lr [0.0000750], embedding loss [ 0.8143], quantization loss [ 0.0101],  0.59 sec/batch.
2022-10-20 19:36:37,106 step [ 610], lr [0.0000750], embedding loss [ 0.8210], quantization loss [ 0.0102],  0.61 sec/batch.
2022-10-20 19:36:39,159 step [ 611], lr [0.0000750], embedding loss [ 0.8267], quantization loss [ 0.0091],  0.59 sec/batch.
2022-10-20 19:36:41,276 step [ 612], lr [0.0000750], embedding loss [ 0.8205], quantization loss [ 0.0092],  0.60 sec/batch.
2022-10-20 19:36:43,288 step [ 613], lr [0.0000750], embedding loss [ 0.8247], quantization loss [ 0.0100],  0.61 sec/batch.
2022-10-20 19:36:45,357 step [ 614], lr [0.0000750], embedding loss [ 0.8181], quantization loss [ 0.0091],  0.64 sec/batch.
2022-10-20 19:36:47,343 step [ 615], lr [0.0000750], embedding loss [ 0.8279], quantization loss [ 0.0099],  0.59 sec/batch.
2022-10-20 19:36:49,411 step [ 616], lr [0.0000750], embedding loss [ 0.8266], quantization loss [ 0.0094],  0.60 sec/batch.
2022-10-20 19:36:51,455 step [ 617], lr [0.0000750], embedding loss [ 0.8151], quantization loss [ 0.0093],  0.61 sec/batch.
2022-10-20 19:36:56,498 step [ 618], lr [0.0000750], embedding loss [ 0.8167], quantization loss [ 0.0091],  3.53 sec/batch.
2022-10-20 19:36:58,605 step [ 619], lr [0.0000750], embedding loss [ 0.8229], quantization loss [ 0.0100],  0.61 sec/batch.
2022-10-20 19:37:03,709 step [ 620], lr [0.0000750], embedding loss [ 0.8245], quantization loss [ 0.0094],  3.44 sec/batch.
2022-10-20 19:37:08,592 step [ 621], lr [0.0000750], embedding loss [ 0.8212], quantization loss [ 0.0097],  3.38 sec/batch.
2022-10-20 19:37:13,271 step [ 622], lr [0.0000750], embedding loss [ 0.8280], quantization loss [ 0.0092],  3.17 sec/batch.
2022-10-20 19:37:15,353 step [ 623], lr [0.0000750], embedding loss [ 0.8282], quantization loss [ 0.0096],  0.61 sec/batch.
2022-10-20 19:37:22,476 step [ 624], lr [0.0000750], embedding loss [ 0.8214], quantization loss [ 0.0099],  5.64 sec/batch.
2022-10-20 19:37:24,524 step [ 625], lr [0.0000750], embedding loss [ 0.8235], quantization loss [ 0.0110],  0.59 sec/batch.
2022-10-20 19:37:26,596 step [ 626], lr [0.0000750], embedding loss [ 0.8284], quantization loss [ 0.0095],  0.59 sec/batch.
2022-10-20 19:37:28,753 step [ 627], lr [0.0000750], embedding loss [ 0.8183], quantization loss [ 0.0101],  0.61 sec/batch.
2022-10-20 19:37:30,834 step [ 628], lr [0.0000750], embedding loss [ 0.8243], quantization loss [ 0.0097],  0.61 sec/batch.
2022-10-20 19:37:32,916 step [ 629], lr [0.0000750], embedding loss [ 0.8276], quantization loss [ 0.0103],  0.61 sec/batch.
2022-10-20 19:37:34,947 step [ 630], lr [0.0000750], embedding loss [ 0.8213], quantization loss [ 0.0101],  0.59 sec/batch.
2022-10-20 19:37:37,006 step [ 631], lr [0.0000750], embedding loss [ 0.8238], quantization loss [ 0.0094],  0.60 sec/batch.
2022-10-20 19:37:39,040 step [ 632], lr [0.0000750], embedding loss [ 0.8197], quantization loss [ 0.0103],  0.61 sec/batch.
2022-10-20 19:37:41,090 step [ 633], lr [0.0000750], embedding loss [ 0.8205], quantization loss [ 0.0091],  0.58 sec/batch.
2022-10-20 19:37:45,544 step [ 634], lr [0.0000750], embedding loss [ 0.8263], quantization loss [ 0.0097],  3.06 sec/batch.
2022-10-20 19:37:47,589 step [ 635], lr [0.0000750], embedding loss [ 0.8179], quantization loss [ 0.0096],  0.61 sec/batch.
2022-10-20 19:37:49,677 step [ 636], lr [0.0000750], embedding loss [ 0.8171], quantization loss [ 0.0096],  0.60 sec/batch.
2022-10-20 19:37:51,703 step [ 637], lr [0.0000750], embedding loss [ 0.8193], quantization loss [ 0.0098],  0.58 sec/batch.
2022-10-20 19:37:53,741 step [ 638], lr [0.0000750], embedding loss [ 0.8240], quantization loss [ 0.0095],  0.58 sec/batch.
2022-10-20 19:37:55,816 step [ 639], lr [0.0000750], embedding loss [ 0.8228], quantization loss [ 0.0087],  0.60 sec/batch.
2022-10-20 19:37:57,867 step [ 640], lr [0.0000750], embedding loss [ 0.8212], quantization loss [ 0.0096],  0.61 sec/batch.
2022-10-20 19:38:05,025 step [ 641], lr [0.0000750], embedding loss [ 0.8220], quantization loss [ 0.0099],  5.74 sec/batch.
2022-10-20 19:38:09,776 step [ 642], lr [0.0000750], embedding loss [ 0.8217], quantization loss [ 0.0093],  3.34 sec/batch.
2022-10-20 19:38:11,837 step [ 643], lr [0.0000750], embedding loss [ 0.8240], quantization loss [ 0.0089],  0.61 sec/batch.
2022-10-20 19:38:13,914 step [ 644], lr [0.0000750], embedding loss [ 0.8237], quantization loss [ 0.0091],  0.61 sec/batch.
2022-10-20 19:38:15,951 step [ 645], lr [0.0000750], embedding loss [ 0.8203], quantization loss [ 0.0094],  0.61 sec/batch.
2022-10-20 19:38:20,773 step [ 646], lr [0.0000750], embedding loss [ 0.8218], quantization loss [ 0.0097],  3.34 sec/batch.
2022-10-20 19:38:22,778 step [ 647], lr [0.0000750], embedding loss [ 0.8205], quantization loss [ 0.0098],  0.59 sec/batch.
2022-10-20 19:38:24,877 step [ 648], lr [0.0000750], embedding loss [ 0.8224], quantization loss [ 0.0092],  0.61 sec/batch.
2022-10-20 19:38:26,917 step [ 649], lr [0.0000750], embedding loss [ 0.8258], quantization loss [ 0.0089],  0.60 sec/batch.
2022-10-20 19:38:29,007 step [ 650], lr [0.0000750], embedding loss [ 0.8236], quantization loss [ 0.0094],  0.60 sec/batch.
2022-10-20 19:38:31,027 step [ 651], lr [0.0000750], embedding loss [ 0.8303], quantization loss [ 0.0087],  0.59 sec/batch.
2022-10-20 19:38:35,690 step [ 652], lr [0.0000750], embedding loss [ 0.8108], quantization loss [ 0.0089],  3.16 sec/batch.
2022-10-20 19:38:40,606 step [ 653], lr [0.0000750], embedding loss [ 0.8255], quantization loss [ 0.0092],  3.42 sec/batch.
2022-10-20 19:38:42,654 step [ 654], lr [0.0000750], embedding loss [ 0.8212], quantization loss [ 0.0099],  0.59 sec/batch.
2022-10-20 19:38:47,558 step [ 655], lr [0.0000750], embedding loss [ 0.8268], quantization loss [ 0.0093],  3.43 sec/batch.
2022-10-20 19:38:49,604 step [ 656], lr [0.0000750], embedding loss [ 0.8190], quantization loss [ 0.0094],  0.60 sec/batch.
2022-10-20 19:38:51,736 step [ 657], lr [0.0000750], embedding loss [ 0.8245], quantization loss [ 0.0090],  0.62 sec/batch.
2022-10-20 19:38:53,882 step [ 658], lr [0.0000750], embedding loss [ 0.8191], quantization loss [ 0.0082],  0.59 sec/batch.
2022-10-20 19:38:55,969 step [ 659], lr [0.0000750], embedding loss [ 0.8173], quantization loss [ 0.0096],  0.61 sec/batch.
2022-10-20 19:38:58,061 step [ 660], lr [0.0000750], embedding loss [ 0.8228], quantization loss [ 0.0091],  0.62 sec/batch.
2022-10-20 19:39:00,212 step [ 661], lr [0.0000750], embedding loss [ 0.8227], quantization loss [ 0.0089],  0.59 sec/batch.
2022-10-20 19:39:02,322 step [ 662], lr [0.0000750], embedding loss [ 0.8212], quantization loss [ 0.0097],  0.58 sec/batch.
2022-10-20 19:39:04,439 step [ 663], lr [0.0000750], embedding loss [ 0.8136], quantization loss [ 0.0095],  0.61 sec/batch.
2022-10-20 19:39:06,554 step [ 664], lr [0.0000750], embedding loss [ 0.8078], quantization loss [ 0.0100],  0.59 sec/batch.
2022-10-20 19:39:08,726 step [ 665], lr [0.0000750], embedding loss [ 0.8120], quantization loss [ 0.0101],  0.60 sec/batch.
2022-10-20 19:39:10,781 step [ 666], lr [0.0000750], embedding loss [ 0.8210], quantization loss [ 0.0090],  0.61 sec/batch.
2022-10-20 19:39:15,274 step [ 667], lr [0.0000750], embedding loss [ 0.8208], quantization loss [ 0.0090],  3.02 sec/batch.
2022-10-20 19:39:17,370 step [ 668], lr [0.0000750], embedding loss [ 0.8186], quantization loss [ 0.0082],  0.60 sec/batch.
2022-10-20 19:39:21,875 step [ 669], lr [0.0000750], embedding loss [ 0.8249], quantization loss [ 0.0088],  3.14 sec/batch.
2022-10-20 19:39:23,959 step [ 670], lr [0.0000750], embedding loss [ 0.8275], quantization loss [ 0.0092],  0.61 sec/batch.
2022-10-20 19:39:26,063 step [ 671], lr [0.0000750], embedding loss [ 0.8223], quantization loss [ 0.0105],  0.61 sec/batch.
2022-10-20 19:39:28,067 step [ 672], lr [0.0000750], embedding loss [ 0.8184], quantization loss [ 0.0100],  0.59 sec/batch.
2022-10-20 19:39:30,133 step [ 673], lr [0.0000750], embedding loss [ 0.8292], quantization loss [ 0.0088],  0.59 sec/batch.
2022-10-20 19:39:32,250 step [ 674], lr [0.0000750], embedding loss [ 0.8188], quantization loss [ 0.0093],  0.61 sec/batch.
2022-10-20 19:39:34,268 step [ 675], lr [0.0000750], embedding loss [ 0.8181], quantization loss [ 0.0094],  0.60 sec/batch.
2022-10-20 19:39:39,255 step [ 676], lr [0.0000750], embedding loss [ 0.8184], quantization loss [ 0.0097],  3.51 sec/batch.
2022-10-20 19:39:41,384 step [ 677], lr [0.0000750], embedding loss [ 0.8255], quantization loss [ 0.0099],  0.61 sec/batch.
2022-10-20 19:39:43,409 step [ 678], lr [0.0000750], embedding loss [ 0.8227], quantization loss [ 0.0091],  0.61 sec/batch.
2022-10-20 19:39:52,840 step [ 679], lr [0.0000750], embedding loss [ 0.8205], quantization loss [ 0.0094],  7.97 sec/batch.
2022-10-20 19:39:54,848 step [ 680], lr [0.0000750], embedding loss [ 0.8225], quantization loss [ 0.0096],  0.60 sec/batch.
2022-10-20 19:39:56,950 step [ 681], lr [0.0000750], embedding loss [ 0.8149], quantization loss [ 0.0097],  0.61 sec/batch.
2022-10-20 19:39:58,959 step [ 682], lr [0.0000750], embedding loss [ 0.8305], quantization loss [ 0.0098],  0.60 sec/batch.
2022-10-20 19:40:01,265 step [ 683], lr [0.0000750], embedding loss [ 0.8226], quantization loss [ 0.0100],  0.84 sec/batch.
2022-10-20 19:40:03,294 step [ 684], lr [0.0000750], embedding loss [ 0.8266], quantization loss [ 0.0090],  0.59 sec/batch.
2022-10-20 19:40:10,343 step [ 685], lr [0.0000750], embedding loss [ 0.8221], quantization loss [ 0.0102],  5.56 sec/batch.
2022-10-20 19:40:12,445 step [ 686], lr [0.0000750], embedding loss [ 0.8237], quantization loss [ 0.0098],  0.61 sec/batch.
2022-10-20 19:40:21,691 step [ 687], lr [0.0000750], embedding loss [ 0.8259], quantization loss [ 0.0104],  7.77 sec/batch.
2022-10-20 19:40:26,721 step [ 688], lr [0.0000750], embedding loss [ 0.8292], quantization loss [ 0.0103],  3.55 sec/batch.
2022-10-20 19:40:28,802 step [ 689], lr [0.0000750], embedding loss [ 0.8171], quantization loss [ 0.0094],  0.60 sec/batch.
2022-10-20 19:40:30,798 step [ 690], lr [0.0000750], embedding loss [ 0.8239], quantization loss [ 0.0090],  0.61 sec/batch.
2022-10-20 19:40:38,222 step [ 691], lr [0.0000750], embedding loss [ 0.8250], quantization loss [ 0.0092],  5.99 sec/batch.
2022-10-20 19:40:43,054 step [ 692], lr [0.0000750], embedding loss [ 0.8079], quantization loss [ 0.0102],  3.33 sec/batch.
2022-10-20 19:40:45,049 step [ 693], lr [0.0000750], embedding loss [ 0.8139], quantization loss [ 0.0090],  0.59 sec/batch.
2022-10-20 19:40:49,768 step [ 694], lr [0.0000750], embedding loss [ 0.8226], quantization loss [ 0.0096],  3.32 sec/batch.
2022-10-20 19:40:51,785 step [ 695], lr [0.0000750], embedding loss [ 0.8132], quantization loss [ 0.0093],  0.58 sec/batch.
2022-10-20 19:40:56,409 step [ 696], lr [0.0000750], embedding loss [ 0.8221], quantization loss [ 0.0099],  3.23 sec/batch.
2022-10-20 19:40:58,515 step [ 697], lr [0.0000750], embedding loss [ 0.8227], quantization loss [ 0.0094],  0.62 sec/batch.
2022-10-20 19:41:00,606 step [ 698], lr [0.0000750], embedding loss [ 0.8251], quantization loss [ 0.0097],  0.59 sec/batch.
2022-10-20 19:41:02,742 step [ 699], lr [0.0000750], embedding loss [ 0.8242], quantization loss [ 0.0095],  0.61 sec/batch.
2022-10-20 19:41:07,447 step [ 700], lr [0.0000750], embedding loss [ 0.8215], quantization loss [ 0.0086],  3.19 sec/batch.
2022-10-20 19:41:09,608 step [ 701], lr [0.0000750], embedding loss [ 0.8253], quantization loss [ 0.0094],  0.66 sec/batch.
2022-10-20 19:41:11,666 step [ 702], lr [0.0000750], embedding loss [ 0.8213], quantization loss [ 0.0091],  0.60 sec/batch.
2022-10-20 19:41:13,841 step [ 703], lr [0.0000750], embedding loss [ 0.8272], quantization loss [ 0.0090],  0.61 sec/batch.
2022-10-20 19:41:15,919 step [ 704], lr [0.0000750], embedding loss [ 0.8293], quantization loss [ 0.0094],  0.60 sec/batch.
2022-10-20 19:41:27,816 step [ 705], lr [0.0000750], embedding loss [ 0.8268], quantization loss [ 0.0099], 10.39 sec/batch.
2022-10-20 19:41:34,242 step [ 706], lr [0.0000750], embedding loss [ 0.8309], quantization loss [ 0.0099],  3.38 sec/batch.
2022-10-20 19:41:39,256 step [ 707], lr [0.0000750], embedding loss [ 0.8208], quantization loss [ 0.0094],  3.48 sec/batch.
2022-10-20 19:41:43,858 step [ 708], lr [0.0000750], embedding loss [ 0.8254], quantization loss [ 0.0095],  3.14 sec/batch.
2022-10-20 19:41:46,009 step [ 709], lr [0.0000750], embedding loss [ 0.8215], quantization loss [ 0.0086],  0.60 sec/batch.
2022-10-20 19:41:48,139 step [ 710], lr [0.0000750], embedding loss [ 0.8279], quantization loss [ 0.0094],  0.61 sec/batch.
2022-10-20 19:41:50,248 step [ 711], lr [0.0000750], embedding loss [ 0.8238], quantization loss [ 0.0093],  0.62 sec/batch.
2022-10-20 19:41:52,343 step [ 712], lr [0.0000750], embedding loss [ 0.8233], quantization loss [ 0.0098],  0.59 sec/batch.
2022-10-20 19:41:54,415 step [ 713], lr [0.0000750], embedding loss [ 0.8249], quantization loss [ 0.0092],  0.60 sec/batch.
2022-10-20 19:41:56,456 step [ 714], lr [0.0000750], embedding loss [ 0.8178], quantization loss [ 0.0096],  0.62 sec/batch.
2022-10-20 19:42:01,339 step [ 715], lr [0.0000750], embedding loss [ 0.8224], quantization loss [ 0.0090],  3.41 sec/batch.
2022-10-20 19:42:06,252 step [ 716], lr [0.0000750], embedding loss [ 0.8157], quantization loss [ 0.0091],  3.44 sec/batch.
2022-10-20 19:42:08,359 step [ 717], lr [0.0000750], embedding loss [ 0.8265], quantization loss [ 0.0090],  0.61 sec/batch.
2022-10-20 19:42:10,437 step [ 718], lr [0.0000750], embedding loss [ 0.8220], quantization loss [ 0.0088],  0.60 sec/batch.
2022-10-20 19:42:15,416 step [ 719], lr [0.0000750], embedding loss [ 0.8241], quantization loss [ 0.0097],  3.51 sec/batch.
2022-10-20 19:42:17,494 step [ 720], lr [0.0000750], embedding loss [ 0.8277], quantization loss [ 0.0091],  0.59 sec/batch.
2022-10-20 19:42:22,034 step [ 721], lr [0.0000750], embedding loss [ 0.8212], quantization loss [ 0.0091],  3.09 sec/batch.
2022-10-20 19:42:24,166 step [ 722], lr [0.0000750], embedding loss [ 0.8190], quantization loss [ 0.0093],  0.62 sec/batch.
2022-10-20 19:42:26,430 step [ 723], lr [0.0000750], embedding loss [ 0.8196], quantization loss [ 0.0101],  0.83 sec/batch.
2022-10-20 19:42:31,452 step [ 724], lr [0.0000750], embedding loss [ 0.8236], quantization loss [ 0.0093],  3.52 sec/batch.
2022-10-20 19:42:33,595 step [ 725], lr [0.0000750], embedding loss [ 0.8248], quantization loss [ 0.0087],  0.60 sec/batch.
2022-10-20 19:42:35,730 step [ 726], lr [0.0000750], embedding loss [ 0.8163], quantization loss [ 0.0088],  0.62 sec/batch.
2022-10-20 19:42:40,815 step [ 727], lr [0.0000750], embedding loss [ 0.8193], quantization loss [ 0.0097],  3.58 sec/batch.
2022-10-20 19:42:52,319 step [ 728], lr [0.0000750], embedding loss [ 0.8184], quantization loss [ 0.0099], 10.04 sec/batch.
2022-10-20 19:42:54,417 step [ 729], lr [0.0000750], embedding loss [ 0.8240], quantization loss [ 0.0088],  0.59 sec/batch.
2022-10-20 19:42:56,542 step [ 730], lr [0.0000750], embedding loss [ 0.8273], quantization loss [ 0.0101],  0.62 sec/batch.
2022-10-20 19:43:01,698 step [ 731], lr [0.0000750], embedding loss [ 0.8210], quantization loss [ 0.0096],  3.64 sec/batch.
2022-10-20 19:43:01,698 update codes and centers iter(1/1).
2022-10-20 19:43:03,569 number of update_code wrong: 0.
2022-10-20 19:43:06,599 non zero codewords: 256.
2022-10-20 19:43:06,599 finish center update, duration: 4.90 sec.
2022-10-20 19:43:11,048 step [ 732], lr [0.0000750], embedding loss [ 0.8280], quantization loss [ 0.0095],  3.12 sec/batch.
2022-10-20 19:43:18,474 step [ 733], lr [0.0000750], embedding loss [ 0.8258], quantization loss [ 0.0099],  5.93 sec/batch.
2022-10-20 19:43:23,407 step [ 734], lr [0.0000750], embedding loss [ 0.8266], quantization loss [ 0.0085],  3.41 sec/batch.
2022-10-20 19:43:25,511 step [ 735], lr [0.0000750], embedding loss [ 0.8202], quantization loss [ 0.0085],  0.58 sec/batch.
2022-10-20 19:43:27,570 step [ 736], lr [0.0000750], embedding loss [ 0.8257], quantization loss [ 0.0081],  0.62 sec/batch.
2022-10-20 19:43:29,621 step [ 737], lr [0.0000750], embedding loss [ 0.8230], quantization loss [ 0.0094],  0.58 sec/batch.
2022-10-20 19:43:31,685 step [ 738], lr [0.0000750], embedding loss [ 0.8228], quantization loss [ 0.0079],  0.59 sec/batch.
2022-10-20 19:43:33,730 step [ 739], lr [0.0000750], embedding loss [ 0.8224], quantization loss [ 0.0082],  0.59 sec/batch.
2022-10-20 19:43:35,829 step [ 740], lr [0.0000750], embedding loss [ 0.8291], quantization loss [ 0.0095],  0.60 sec/batch.
2022-10-20 19:43:37,885 step [ 741], lr [0.0000750], embedding loss [ 0.8161], quantization loss [ 0.0086],  0.60 sec/batch.
2022-10-20 19:43:39,982 step [ 742], lr [0.0000750], embedding loss [ 0.8203], quantization loss [ 0.0086],  0.59 sec/batch.
2022-10-20 19:43:41,954 step [ 743], lr [0.0000750], embedding loss [ 0.8165], quantization loss [ 0.0088],  0.59 sec/batch.
2022-10-20 19:43:43,991 step [ 744], lr [0.0000750], embedding loss [ 0.8260], quantization loss [ 0.0084],  0.60 sec/batch.
2022-10-20 19:43:46,098 step [ 745], lr [0.0000750], embedding loss [ 0.8145], quantization loss [ 0.0089],  0.61 sec/batch.
2022-10-20 19:43:48,249 step [ 746], lr [0.0000750], embedding loss [ 0.8269], quantization loss [ 0.0087],  0.64 sec/batch.
2022-10-20 19:43:50,253 step [ 747], lr [0.0000750], embedding loss [ 0.8169], quantization loss [ 0.0101],  0.59 sec/batch.
2022-10-20 19:43:52,264 step [ 748], lr [0.0000750], embedding loss [ 0.8178], quantization loss [ 0.0093],  0.59 sec/batch.
2022-10-20 19:43:57,193 step [ 749], lr [0.0000750], embedding loss [ 0.8221], quantization loss [ 0.0081],  3.39 sec/batch.
2022-10-20 19:43:59,241 step [ 750], lr [0.0000750], embedding loss [ 0.8245], quantization loss [ 0.0087],  0.59 sec/batch.
2022-10-20 19:44:01,347 step [ 751], lr [0.0000750], embedding loss [ 0.8203], quantization loss [ 0.0092],  0.59 sec/batch.
2022-10-20 19:44:03,443 step [ 752], lr [0.0000750], embedding loss [ 0.8326], quantization loss [ 0.0085],  0.61 sec/batch.
2022-10-20 19:44:05,507 step [ 753], lr [0.0000750], embedding loss [ 0.8245], quantization loss [ 0.0086],  0.59 sec/batch.
2022-10-20 19:44:07,601 step [ 754], lr [0.0000750], embedding loss [ 0.8318], quantization loss [ 0.0085],  0.60 sec/batch.
2022-10-20 19:44:09,738 step [ 755], lr [0.0000750], embedding loss [ 0.8247], quantization loss [ 0.0089],  0.62 sec/batch.
2022-10-20 19:44:11,845 step [ 756], lr [0.0000750], embedding loss [ 0.8233], quantization loss [ 0.0085],  0.59 sec/batch.
2022-10-20 19:44:13,883 step [ 757], lr [0.0000750], embedding loss [ 0.8205], quantization loss [ 0.0090],  0.59 sec/batch.
2022-10-20 19:44:15,961 step [ 758], lr [0.0000750], embedding loss [ 0.8153], quantization loss [ 0.0086],  0.59 sec/batch.
2022-10-20 19:44:23,232 step [ 759], lr [0.0000750], embedding loss [ 0.8265], quantization loss [ 0.0088],  5.84 sec/batch.
2022-10-20 19:44:25,316 step [ 760], lr [0.0000750], embedding loss [ 0.8205], quantization loss [ 0.0084],  0.62 sec/batch.
2022-10-20 19:44:27,394 step [ 761], lr [0.0000750], embedding loss [ 0.8129], quantization loss [ 0.0095],  0.62 sec/batch.
2022-10-20 19:44:29,440 step [ 762], lr [0.0000750], embedding loss [ 0.8226], quantization loss [ 0.0091],  0.61 sec/batch.
2022-10-20 19:44:31,465 step [ 763], lr [0.0000750], embedding loss [ 0.8261], quantization loss [ 0.0091],  0.60 sec/batch.
2022-10-20 19:44:33,604 step [ 764], lr [0.0000750], embedding loss [ 0.8225], quantization loss [ 0.0091],  0.65 sec/batch.
2022-10-20 19:44:35,642 step [ 765], lr [0.0000750], embedding loss [ 0.8284], quantization loss [ 0.0081],  0.59 sec/batch.
2022-10-20 19:44:37,667 step [ 766], lr [0.0000750], embedding loss [ 0.8199], quantization loss [ 0.0081],  0.59 sec/batch.
2022-10-20 19:44:42,544 step [ 767], lr [0.0000750], embedding loss [ 0.8163], quantization loss [ 0.0081],  3.37 sec/batch.
2022-10-20 19:44:44,615 step [ 768], lr [0.0000750], embedding loss [ 0.8204], quantization loss [ 0.0082],  0.61 sec/batch.
2022-10-20 19:44:46,641 step [ 769], lr [0.0000750], embedding loss [ 0.8177], quantization loss [ 0.0088],  0.58 sec/batch.
2022-10-20 19:44:48,703 step [ 770], lr [0.0000750], embedding loss [ 0.8167], quantization loss [ 0.0093],  0.59 sec/batch.
2022-10-20 19:44:50,741 step [ 771], lr [0.0000750], embedding loss [ 0.8211], quantization loss [ 0.0085],  0.59 sec/batch.
2022-10-20 19:44:55,769 step [ 772], lr [0.0000750], embedding loss [ 0.8182], quantization loss [ 0.0092],  3.60 sec/batch.
2022-10-20 19:45:00,627 step [ 773], lr [0.0000750], embedding loss [ 0.8207], quantization loss [ 0.0090],  3.38 sec/batch.
2022-10-20 19:45:14,515 step [ 774], lr [0.0000750], embedding loss [ 0.8317], quantization loss [ 0.0088], 12.45 sec/batch.
2022-10-20 19:45:16,595 step [ 775], lr [0.0000750], embedding loss [ 0.8203], quantization loss [ 0.0082],  0.59 sec/batch.
2022-10-20 19:45:18,637 step [ 776], lr [0.0000750], embedding loss [ 0.8155], quantization loss [ 0.0088],  0.60 sec/batch.
2022-10-20 19:45:20,709 step [ 777], lr [0.0000750], embedding loss [ 0.8248], quantization loss [ 0.0087],  0.62 sec/batch.
2022-10-20 19:45:22,846 step [ 778], lr [0.0000750], embedding loss [ 0.8230], quantization loss [ 0.0079],  0.59 sec/batch.
2022-10-20 19:45:24,930 step [ 779], lr [0.0000750], embedding loss [ 0.8197], quantization loss [ 0.0089],  0.59 sec/batch.
2022-10-20 19:45:26,991 step [ 780], lr [0.0000750], embedding loss [ 0.8197], quantization loss [ 0.0084],  0.60 sec/batch.
2022-10-20 19:45:29,200 step [ 781], lr [0.0000750], embedding loss [ 0.8220], quantization loss [ 0.0086],  0.77 sec/batch.
2022-10-20 19:45:31,268 step [ 782], lr [0.0000750], embedding loss [ 0.8136], quantization loss [ 0.0089],  0.58 sec/batch.
2022-10-20 19:45:38,560 step [ 783], lr [0.0000750], embedding loss [ 0.8287], quantization loss [ 0.0089],  5.84 sec/batch.
2022-10-20 19:45:43,319 step [ 784], lr [0.0000750], embedding loss [ 0.8205], quantization loss [ 0.0081],  3.31 sec/batch.
2022-10-20 19:45:48,290 step [ 785], lr [0.0000750], embedding loss [ 0.8246], quantization loss [ 0.0081],  3.49 sec/batch.
2022-10-20 19:45:50,400 step [ 786], lr [0.0000750], embedding loss [ 0.8204], quantization loss [ 0.0074],  0.60 sec/batch.
2022-10-20 19:45:52,522 step [ 787], lr [0.0000750], embedding loss [ 0.8126], quantization loss [ 0.0079],  0.60 sec/batch.
2022-10-20 19:45:54,660 step [ 788], lr [0.0000750], embedding loss [ 0.8224], quantization loss [ 0.0085],  0.59 sec/batch.
2022-10-20 19:45:56,751 step [ 789], lr [0.0000750], embedding loss [ 0.8204], quantization loss [ 0.0095],  0.59 sec/batch.
2022-10-20 19:45:58,851 step [ 790], lr [0.0000750], embedding loss [ 0.8148], quantization loss [ 0.0085],  0.61 sec/batch.
2022-10-20 19:46:00,972 step [ 791], lr [0.0000750], embedding loss [ 0.8147], quantization loss [ 0.0076],  0.60 sec/batch.
2022-10-20 19:46:02,999 step [ 792], lr [0.0000750], embedding loss [ 0.8214], quantization loss [ 0.0085],  0.59 sec/batch.
2022-10-20 19:46:10,656 step [ 793], lr [0.0000750], embedding loss [ 0.8256], quantization loss [ 0.0084],  6.14 sec/batch.
2022-10-20 19:46:12,754 step [ 794], lr [0.0000750], embedding loss [ 0.8277], quantization loss [ 0.0085],  0.61 sec/batch.
2022-10-20 19:46:14,825 step [ 795], lr [0.0000750], embedding loss [ 0.8314], quantization loss [ 0.0081],  0.59 sec/batch.
2022-10-20 19:46:16,952 step [ 796], lr [0.0000750], embedding loss [ 0.8207], quantization loss [ 0.0084],  0.59 sec/batch.
2022-10-20 19:46:18,977 step [ 797], lr [0.0000750], embedding loss [ 0.8192], quantization loss [ 0.0088],  0.60 sec/batch.
2022-10-20 19:46:21,125 step [ 798], lr [0.0000750], embedding loss [ 0.8207], quantization loss [ 0.0082],  0.61 sec/batch.
2022-10-20 19:46:23,164 step [ 799], lr [0.0000750], embedding loss [ 0.8300], quantization loss [ 0.0080],  0.60 sec/batch.
2022-10-20 19:46:25,236 step [ 800], lr [0.0000750], embedding loss [ 0.8243], quantization loss [ 0.0081],  0.59 sec/batch.
2022-10-20 19:46:25,236 finish training iterations and begin saving model.
2022-10-20 19:46:32,179 finish model saving.
2022-10-20 19:46:32,180 finish training, model saved under ./checkpoints/flickr_WSDQH_nbits=8_adaMargin_gamma=1_lambda=0.0001_0001.npy.
2022-10-20 19:46:35,843 prepare dataset.
2022-10-20 19:46:36,498 prepare data loader.
2022-10-20 19:46:36,499 Initializing DataLoader.
2022-10-20 19:46:36,499 DataLoader already.
2022-10-20 19:46:36,499 Initializing DataLoader.
2022-10-20 19:46:36,499 DataLoader already.
2022-10-20 19:46:36,500 prepare model.
2022-10-20 19:46:36,723 Number of semantic embeddings: 1178.
2022-10-20 19:46:54,107 begin validation.
2022-10-20 19:47:16,431 finish query feature extraction, duration: 22.32 sec.
2022-10-20 19:50:46,803 finish database feature extraction, duration: 210.37 sec.
2022-10-20 19:50:46,804 compute quantization codes for query.
2022-10-20 19:50:47,880 number of update_code wrong: 0.
2022-10-20 19:50:47,880 finish query encoding, duration: 1.08 sec.
2022-10-20 19:50:47,881 compute quantization codes for database.
2022-10-20 19:50:50,215 number of update_code wrong: 0.
2022-10-20 19:50:50,215 finish database encoding, duration: 2.33 sec.
2022-10-20 19:50:50,216 save retrieval information: codes, features, reconstructions of queries and database.
2022-10-20 19:50:51,145 begin to calculate MAP@5000.
2022-10-20 19:50:51,145 begin to calculate AQD mAP@5000.
2022-10-20 19:50:54,478 AQD mAP@5000 = [0.7661], duration: 3.33 sec.
2022-10-20 19:50:54,479 begin to calculate SQD mAP@5000.
2022-10-20 19:50:57,705 SQD mAP@5000 = [0.7651], duration: 3.23 sec.
2022-10-20 19:50:57,705 begin to calculate feats mAP@5000.
2022-10-20 19:51:01,233 feats mAP@5000 = [0.7666], duration: 3.53 sec.
2022-10-20 19:51:01,234 finish validation.
