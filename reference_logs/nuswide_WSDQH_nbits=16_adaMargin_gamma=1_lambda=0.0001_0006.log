2022-10-19 00:08:58,908 prepare dataset.
2022-10-19 00:09:03,369 prepare data loader.
2022-10-19 00:09:03,370 Initializing DataLoader.
2022-10-19 00:09:03,372 DataLoader already.
2022-10-19 00:09:03,372 prepare model.
2022-10-19 00:09:03,555 Number of semantic embeddings: 928.
2022-10-19 00:09:11,370 From /data/wangjinpeng/anaconda3/envs/py37torch/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where.
2022-10-19 00:09:24,331 begin training.
2022-10-19 00:09:39,033 step [   1], lr [0.0003000], embedding loss [ 0.8909], quantization loss [ 0.0000], 13.05 sec/batch.
2022-10-19 00:09:41,217 step [   2], lr [0.0003000], embedding loss [ 0.8572], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-19 00:09:43,316 step [   3], lr [0.0003000], embedding loss [ 0.8549], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-19 00:09:45,392 step [   4], lr [0.0003000], embedding loss [ 0.8436], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-19 00:09:47,529 step [   5], lr [0.0003000], embedding loss [ 0.8339], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-19 00:09:49,665 step [   6], lr [0.0003000], embedding loss [ 0.8331], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-19 00:09:51,801 step [   7], lr [0.0003000], embedding loss [ 0.8267], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-19 00:09:53,894 step [   8], lr [0.0003000], embedding loss [ 0.8194], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-19 00:09:56,029 step [   9], lr [0.0003000], embedding loss [ 0.8068], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-19 00:09:58,129 step [  10], lr [0.0003000], embedding loss [ 0.8153], quantization loss [ 0.0000],  0.53 sec/batch.
2022-10-19 00:10:00,299 step [  11], lr [0.0003000], embedding loss [ 0.8151], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-19 00:10:02,393 step [  12], lr [0.0003000], embedding loss [ 0.8042], quantization loss [ 0.0000],  0.53 sec/batch.
2022-10-19 00:10:04,534 step [  13], lr [0.0003000], embedding loss [ 0.8058], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-19 00:10:06,695 step [  14], lr [0.0003000], embedding loss [ 0.8056], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-19 00:10:08,857 step [  15], lr [0.0003000], embedding loss [ 0.8034], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-19 00:10:10,967 step [  16], lr [0.0003000], embedding loss [ 0.8005], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-19 00:10:13,187 step [  17], lr [0.0003000], embedding loss [ 0.7960], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-19 00:10:15,367 step [  18], lr [0.0003000], embedding loss [ 0.7955], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-19 00:10:17,615 step [  19], lr [0.0003000], embedding loss [ 0.7862], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-19 00:10:19,810 step [  20], lr [0.0003000], embedding loss [ 0.7941], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-19 00:10:21,976 step [  21], lr [0.0003000], embedding loss [ 0.7943], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-19 00:10:24,137 step [  22], lr [0.0003000], embedding loss [ 0.7970], quantization loss [ 0.0000],  0.56 sec/batch.
2022-10-19 00:10:26,323 step [  23], lr [0.0003000], embedding loss [ 0.7915], quantization loss [ 0.0000],  0.57 sec/batch.
2022-10-19 00:10:28,739 step [  24], lr [0.0003000], embedding loss [ 0.7871], quantization loss [ 0.0000],  0.77 sec/batch.
2022-10-19 00:10:30,864 step [  25], lr [0.0003000], embedding loss [ 0.7832], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 00:10:33,035 step [  26], lr [0.0003000], embedding loss [ 0.7830], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-19 00:10:35,156 step [  27], lr [0.0003000], embedding loss [ 0.7892], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-19 00:10:37,268 step [  28], lr [0.0003000], embedding loss [ 0.7943], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-19 00:10:39,436 step [  29], lr [0.0003000], embedding loss [ 0.7914], quantization loss [ 0.0000],  0.53 sec/batch.
2022-10-19 00:10:41,571 step [  30], lr [0.0003000], embedding loss [ 0.7770], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 00:10:43,669 step [  31], lr [0.0003000], embedding loss [ 0.7733], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:10:45,784 step [  32], lr [0.0003000], embedding loss [ 0.7703], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 00:10:47,942 step [  33], lr [0.0003000], embedding loss [ 0.7734], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-19 00:10:50,015 step [  34], lr [0.0003000], embedding loss [ 0.7720], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 00:10:52,137 step [  35], lr [0.0003000], embedding loss [ 0.7729], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:10:54,239 step [  36], lr [0.0003000], embedding loss [ 0.7717], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:10:56,341 step [  37], lr [0.0003000], embedding loss [ 0.7765], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 00:10:58,468 step [  38], lr [0.0003000], embedding loss [ 0.7726], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:11:00,540 step [  39], lr [0.0003000], embedding loss [ 0.7767], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 00:11:02,271 step [  40], lr [0.0003000], embedding loss [ 0.7635], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 00:11:04,026 step [  41], lr [0.0003000], embedding loss [ 0.7793], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:11:05,799 step [  42], lr [0.0003000], embedding loss [ 0.7622], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:11:07,582 step [  43], lr [0.0003000], embedding loss [ 0.7738], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 00:11:09,317 step [  44], lr [0.0003000], embedding loss [ 0.7615], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 00:11:11,059 step [  45], lr [0.0003000], embedding loss [ 0.7685], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:11:12,862 step [  46], lr [0.0003000], embedding loss [ 0.7635], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-19 00:11:14,671 step [  47], lr [0.0003000], embedding loss [ 0.7673], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:11:16,423 step [  48], lr [0.0003000], embedding loss [ 0.7702], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:11:18,167 step [  49], lr [0.0003000], embedding loss [ 0.7673], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:11:19,901 step [  50], lr [0.0003000], embedding loss [ 0.7671], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:11:21,699 step [  51], lr [0.0003000], embedding loss [ 0.7621], quantization loss [ 0.0000],  0.55 sec/batch.
2022-10-19 00:11:23,468 step [  52], lr [0.0003000], embedding loss [ 0.7636], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:11:25,366 step [  53], lr [0.0003000], embedding loss [ 0.7600], quantization loss [ 0.0000],  0.58 sec/batch.
2022-10-19 00:11:27,351 step [  54], lr [0.0003000], embedding loss [ 0.7639], quantization loss [ 0.0000],  0.60 sec/batch.
2022-10-19 00:11:29,054 step [  55], lr [0.0003000], embedding loss [ 0.7576], quantization loss [ 0.0000],  0.50 sec/batch.
2022-10-19 00:11:30,860 step [  56], lr [0.0003000], embedding loss [ 0.7550], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 00:11:32,710 step [  57], lr [0.0003000], embedding loss [ 0.7647], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-19 00:11:34,449 step [  58], lr [0.0003000], embedding loss [ 0.7564], quantization loss [ 0.0000],  0.50 sec/batch.
2022-10-19 00:11:36,211 step [  59], lr [0.0003000], embedding loss [ 0.7708], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:11:38,089 step [  60], lr [0.0003000], embedding loss [ 0.7567], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 00:11:39,892 step [  61], lr [0.0003000], embedding loss [ 0.7602], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:11:41,751 step [  62], lr [0.0003000], embedding loss [ 0.7517], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:11:43,556 step [  63], lr [0.0003000], embedding loss [ 0.7596], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:11:45,401 step [  64], lr [0.0003000], embedding loss [ 0.7581], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:11:47,290 step [  65], lr [0.0003000], embedding loss [ 0.7573], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-19 00:11:49,105 step [  66], lr [0.0003000], embedding loss [ 0.7557], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:11:50,955 step [  67], lr [0.0003000], embedding loss [ 0.7503], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:11:52,800 step [  68], lr [0.0003000], embedding loss [ 0.7600], quantization loss [ 0.0000],  0.51 sec/batch.
2022-10-19 00:11:54,667 step [  69], lr [0.0003000], embedding loss [ 0.7512], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:11:56,528 step [  70], lr [0.0003000], embedding loss [ 0.7599], quantization loss [ 0.0000],  0.54 sec/batch.
2022-10-19 00:11:58,368 step [  71], lr [0.0003000], embedding loss [ 0.7588], quantization loss [ 0.0000],  0.53 sec/batch.
2022-10-19 00:12:00,122 step [  72], lr [0.0003000], embedding loss [ 0.7525], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:12:01,993 step [  73], lr [0.0003000], embedding loss [ 0.7446], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:12:03,782 step [  74], lr [0.0003000], embedding loss [ 0.7551], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:12:06,518 step [  75], lr [0.0003000], embedding loss [ 0.7433], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:12:08,399 step [  76], lr [0.0003000], embedding loss [ 0.7523], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:12:10,223 step [  77], lr [0.0003000], embedding loss [ 0.7445], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:12:12,103 step [  78], lr [0.0003000], embedding loss [ 0.7580], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:12:13,968 step [  79], lr [0.0003000], embedding loss [ 0.7414], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:12:15,847 step [  80], lr [0.0003000], embedding loss [ 0.7569], quantization loss [ 0.0000],  0.52 sec/batch.
2022-10-19 00:12:17,710 step [  81], lr [0.0003000], embedding loss [ 0.7368], quantization loss [ 0.0000],  0.53 sec/batch.
2022-10-19 00:12:17,710 initialize centers iter(1/1).
2022-10-19 00:12:22,588 finish center initialization, duration: 4.88 sec.
2022-10-19 00:12:22,589 update codes and centers iter(1/1).
2022-10-19 00:12:25,003 number of update_code wrong: 0.
2022-10-19 00:12:29,082 non zero codewords: 512.
2022-10-19 00:12:29,082 finish center update, duration: 6.49 sec.
2022-10-19 00:12:30,833 step [  82], lr [0.0003000], embedding loss [ 0.7417], quantization loss [ 0.4215],  0.52 sec/batch.
2022-10-19 00:12:32,710 step [  83], lr [0.0003000], embedding loss [ 0.7863], quantization loss [ 1.1080],  0.53 sec/batch.
2022-10-19 00:12:34,550 step [  84], lr [0.0003000], embedding loss [ 0.7674], quantization loss [ 0.6227],  0.52 sec/batch.
2022-10-19 00:12:36,476 step [  85], lr [0.0003000], embedding loss [ 0.7862], quantization loss [ 1.0598],  0.54 sec/batch.
2022-10-19 00:12:38,336 step [  86], lr [0.0003000], embedding loss [ 0.7615], quantization loss [ 0.5582],  0.52 sec/batch.
2022-10-19 00:12:40,219 step [  87], lr [0.0003000], embedding loss [ 0.7826], quantization loss [ 0.8999],  0.52 sec/batch.
2022-10-19 00:12:42,034 step [  88], lr [0.0003000], embedding loss [ 0.7696], quantization loss [ 0.6610],  0.52 sec/batch.
2022-10-19 00:12:43,870 step [  89], lr [0.0003000], embedding loss [ 0.7714], quantization loss [ 0.6214],  0.53 sec/batch.
2022-10-19 00:12:45,910 step [  90], lr [0.0003000], embedding loss [ 0.7650], quantization loss [ 0.5449],  0.59 sec/batch.
2022-10-19 00:12:48,110 step [  91], lr [0.0003000], embedding loss [ 0.7591], quantization loss [ 0.4994],  0.56 sec/batch.
2022-10-19 00:12:50,125 step [  92], lr [0.0003000], embedding loss [ 0.7587], quantization loss [ 0.4196],  0.57 sec/batch.
2022-10-19 00:12:52,253 step [  93], lr [0.0003000], embedding loss [ 0.7605], quantization loss [ 0.5028],  0.59 sec/batch.
2022-10-19 00:12:54,427 step [  94], lr [0.0003000], embedding loss [ 0.7735], quantization loss [ 0.4358],  0.59 sec/batch.
2022-10-19 00:12:56,500 step [  95], lr [0.0003000], embedding loss [ 0.7691], quantization loss [ 0.4300],  0.60 sec/batch.
2022-10-19 00:12:58,726 step [  96], lr [0.0003000], embedding loss [ 0.7597], quantization loss [ 0.3879],  0.55 sec/batch.
2022-10-19 00:13:00,778 step [  97], lr [0.0003000], embedding loss [ 0.7622], quantization loss [ 0.3928],  0.55 sec/batch.
2022-10-19 00:13:02,785 step [  98], lr [0.0003000], embedding loss [ 0.7539], quantization loss [ 0.3631],  0.61 sec/batch.
2022-10-19 00:13:04,747 step [  99], lr [0.0003000], embedding loss [ 0.7549], quantization loss [ 0.3685],  0.56 sec/batch.
2022-10-19 00:13:06,680 step [ 100], lr [0.0003000], embedding loss [ 0.7549], quantization loss [ 0.3568],  0.55 sec/batch.
2022-10-19 00:13:08,659 step [ 101], lr [0.0003000], embedding loss [ 0.7561], quantization loss [ 0.4402],  0.56 sec/batch.
2022-10-19 00:13:10,649 step [ 102], lr [0.0003000], embedding loss [ 0.7611], quantization loss [ 0.3394],  0.55 sec/batch.
2022-10-19 00:13:12,603 step [ 103], lr [0.0003000], embedding loss [ 0.7514], quantization loss [ 0.3472],  0.54 sec/batch.
2022-10-19 00:13:14,529 step [ 104], lr [0.0003000], embedding loss [ 0.7675], quantization loss [ 0.3445],  0.55 sec/batch.
2022-10-19 00:13:16,482 step [ 105], lr [0.0003000], embedding loss [ 0.7459], quantization loss [ 0.3307],  0.56 sec/batch.
2022-10-19 00:13:18,415 step [ 106], lr [0.0003000], embedding loss [ 0.7571], quantization loss [ 0.3655],  0.55 sec/batch.
2022-10-19 00:13:20,318 step [ 107], lr [0.0003000], embedding loss [ 0.7693], quantization loss [ 0.4057],  0.55 sec/batch.
2022-10-19 00:13:22,314 step [ 108], lr [0.0003000], embedding loss [ 0.7520], quantization loss [ 0.3505],  0.57 sec/batch.
2022-10-19 00:13:24,282 step [ 109], lr [0.0003000], embedding loss [ 0.7561], quantization loss [ 0.3927],  0.58 sec/batch.
2022-10-19 00:13:26,246 step [ 110], lr [0.0003000], embedding loss [ 0.7603], quantization loss [ 0.3608],  0.57 sec/batch.
2022-10-19 00:13:28,194 step [ 111], lr [0.0003000], embedding loss [ 0.7559], quantization loss [ 0.3465],  0.57 sec/batch.
2022-10-19 00:13:30,125 step [ 112], lr [0.0003000], embedding loss [ 0.7465], quantization loss [ 0.3483],  0.55 sec/batch.
2022-10-19 00:13:32,078 step [ 113], lr [0.0003000], embedding loss [ 0.7594], quantization loss [ 0.2896],  0.56 sec/batch.
2022-10-19 00:13:33,979 step [ 114], lr [0.0003000], embedding loss [ 0.7594], quantization loss [ 0.3285],  0.56 sec/batch.
2022-10-19 00:13:35,853 step [ 115], lr [0.0003000], embedding loss [ 0.7559], quantization loss [ 0.3336],  0.55 sec/batch.
2022-10-19 00:13:37,804 step [ 116], lr [0.0003000], embedding loss [ 0.7638], quantization loss [ 0.3237],  0.57 sec/batch.
2022-10-19 00:13:39,753 step [ 117], lr [0.0003000], embedding loss [ 0.7597], quantization loss [ 0.3365],  0.56 sec/batch.
2022-10-19 00:13:41,683 step [ 118], lr [0.0003000], embedding loss [ 0.7504], quantization loss [ 0.3423],  0.56 sec/batch.
2022-10-19 00:13:43,583 step [ 119], lr [0.0003000], embedding loss [ 0.7620], quantization loss [ 0.3490],  0.55 sec/batch.
2022-10-19 00:13:45,504 step [ 120], lr [0.0003000], embedding loss [ 0.7529], quantization loss [ 0.3356],  0.55 sec/batch.
2022-10-19 00:13:47,341 step [ 121], lr [0.0003000], embedding loss [ 0.7587], quantization loss [ 0.2913],  0.55 sec/batch.
2022-10-19 00:13:49,214 step [ 122], lr [0.0003000], embedding loss [ 0.7567], quantization loss [ 0.3387],  0.55 sec/batch.
2022-10-19 00:13:51,055 step [ 123], lr [0.0003000], embedding loss [ 0.7506], quantization loss [ 0.2900],  0.55 sec/batch.
2022-10-19 00:13:53,005 step [ 124], lr [0.0003000], embedding loss [ 0.7569], quantization loss [ 0.3138],  0.56 sec/batch.
2022-10-19 00:13:54,878 step [ 125], lr [0.0003000], embedding loss [ 0.7605], quantization loss [ 0.2939],  0.55 sec/batch.
2022-10-19 00:13:56,805 step [ 126], lr [0.0003000], embedding loss [ 0.7596], quantization loss [ 0.2840],  0.56 sec/batch.
2022-10-19 00:13:58,675 step [ 127], lr [0.0003000], embedding loss [ 0.7688], quantization loss [ 0.2815],  0.55 sec/batch.
2022-10-19 00:14:00,598 step [ 128], lr [0.0003000], embedding loss [ 0.7493], quantization loss [ 0.2747],  0.56 sec/batch.
2022-10-19 00:14:02,467 step [ 129], lr [0.0003000], embedding loss [ 0.7586], quantization loss [ 0.3416],  0.55 sec/batch.
2022-10-19 00:14:04,383 step [ 130], lr [0.0003000], embedding loss [ 0.7609], quantization loss [ 0.2969],  0.56 sec/batch.
2022-10-19 00:14:06,249 step [ 131], lr [0.0003000], embedding loss [ 0.7561], quantization loss [ 0.2989],  0.55 sec/batch.
2022-10-19 00:14:08,147 step [ 132], lr [0.0003000], embedding loss [ 0.7585], quantization loss [ 0.2777],  0.56 sec/batch.
2022-10-19 00:14:10,044 step [ 133], lr [0.0003000], embedding loss [ 0.7486], quantization loss [ 0.2713],  0.55 sec/batch.
2022-10-19 00:14:11,964 step [ 134], lr [0.0003000], embedding loss [ 0.7596], quantization loss [ 0.2680],  0.56 sec/batch.
2022-10-19 00:14:13,898 step [ 135], lr [0.0003000], embedding loss [ 0.7589], quantization loss [ 0.2401],  0.56 sec/batch.
2022-10-19 00:14:15,812 step [ 136], lr [0.0003000], embedding loss [ 0.7418], quantization loss [ 0.3088],  0.55 sec/batch.
2022-10-19 00:14:17,731 step [ 137], lr [0.0003000], embedding loss [ 0.7434], quantization loss [ 0.2663],  0.55 sec/batch.
2022-10-19 00:14:19,612 step [ 138], lr [0.0003000], embedding loss [ 0.7444], quantization loss [ 0.2727],  0.55 sec/batch.
2022-10-19 00:14:21,546 step [ 139], lr [0.0003000], embedding loss [ 0.7488], quantization loss [ 0.2665],  0.56 sec/batch.
2022-10-19 00:14:23,368 step [ 140], lr [0.0003000], embedding loss [ 0.7429], quantization loss [ 0.2597],  0.55 sec/batch.
2022-10-19 00:14:25,291 step [ 141], lr [0.0003000], embedding loss [ 0.7553], quantization loss [ 0.2973],  0.55 sec/batch.
2022-10-19 00:14:27,186 step [ 142], lr [0.0003000], embedding loss [ 0.7490], quantization loss [ 0.2804],  0.55 sec/batch.
2022-10-19 00:14:29,130 step [ 143], lr [0.0003000], embedding loss [ 0.7402], quantization loss [ 0.2719],  0.56 sec/batch.
2022-10-19 00:14:31,066 step [ 144], lr [0.0003000], embedding loss [ 0.7485], quantization loss [ 0.2516],  0.56 sec/batch.
2022-10-19 00:14:33,018 step [ 145], lr [0.0003000], embedding loss [ 0.7526], quantization loss [ 0.2618],  0.56 sec/batch.
2022-10-19 00:14:34,937 step [ 146], lr [0.0003000], embedding loss [ 0.7496], quantization loss [ 0.2693],  0.55 sec/batch.
2022-10-19 00:14:36,819 step [ 147], lr [0.0003000], embedding loss [ 0.7476], quantization loss [ 0.2779],  0.54 sec/batch.
2022-10-19 00:14:38,718 step [ 148], lr [0.0003000], embedding loss [ 0.7402], quantization loss [ 0.2298],  0.55 sec/batch.
2022-10-19 00:14:40,649 step [ 149], lr [0.0003000], embedding loss [ 0.7501], quantization loss [ 0.2580],  0.56 sec/batch.
2022-10-19 00:14:42,575 step [ 150], lr [0.0003000], embedding loss [ 0.7513], quantization loss [ 0.2818],  0.55 sec/batch.
2022-10-19 00:14:44,447 step [ 151], lr [0.0003000], embedding loss [ 0.7510], quantization loss [ 0.2811],  0.54 sec/batch.
2022-10-19 00:14:46,345 step [ 152], lr [0.0003000], embedding loss [ 0.7510], quantization loss [ 0.3259],  0.54 sec/batch.
2022-10-19 00:14:48,314 step [ 153], lr [0.0003000], embedding loss [ 0.7484], quantization loss [ 0.2337],  0.56 sec/batch.
2022-10-19 00:14:50,232 step [ 154], lr [0.0003000], embedding loss [ 0.7425], quantization loss [ 0.2585],  0.55 sec/batch.
2022-10-19 00:14:52,150 step [ 155], lr [0.0003000], embedding loss [ 0.7466], quantization loss [ 0.2688],  0.56 sec/batch.
2022-10-19 00:14:54,072 step [ 156], lr [0.0003000], embedding loss [ 0.7369], quantization loss [ 0.2638],  0.55 sec/batch.
2022-10-19 00:14:55,958 step [ 157], lr [0.0003000], embedding loss [ 0.7540], quantization loss [ 0.3167],  0.53 sec/batch.
2022-10-19 00:14:57,837 step [ 158], lr [0.0003000], embedding loss [ 0.7408], quantization loss [ 0.2872],  0.55 sec/batch.
2022-10-19 00:14:59,766 step [ 159], lr [0.0003000], embedding loss [ 0.7541], quantization loss [ 0.2802],  0.58 sec/batch.
2022-10-19 00:15:01,695 step [ 160], lr [0.0003000], embedding loss [ 0.7552], quantization loss [ 0.2405],  0.55 sec/batch.
2022-10-19 00:15:03,551 step [ 161], lr [0.0003000], embedding loss [ 0.7539], quantization loss [ 0.2539],  0.53 sec/batch.
2022-10-19 00:15:03,551 update codes and centers iter(1/1).
2022-10-19 00:15:05,274 number of update_code wrong: 0.
2022-10-19 00:15:07,857 non zero codewords: 512.
2022-10-19 00:15:07,857 finish center update, duration: 4.31 sec.
2022-10-19 00:15:09,672 step [ 162], lr [0.0003000], embedding loss [ 0.7499], quantization loss [ 0.1735],  0.56 sec/batch.
2022-10-19 00:15:11,605 step [ 163], lr [0.0003000], embedding loss [ 0.7529], quantization loss [ 0.1771],  0.62 sec/batch.
2022-10-19 00:15:13,653 step [ 164], lr [0.0003000], embedding loss [ 0.7521], quantization loss [ 0.1993],  0.63 sec/batch.
2022-10-19 00:15:15,652 step [ 165], lr [0.0003000], embedding loss [ 0.7483], quantization loss [ 0.1957],  0.62 sec/batch.
2022-10-19 00:15:17,641 step [ 166], lr [0.0003000], embedding loss [ 0.7499], quantization loss [ 0.1709],  0.62 sec/batch.
2022-10-19 00:15:19,687 step [ 167], lr [0.0003000], embedding loss [ 0.7468], quantization loss [ 0.1694],  0.64 sec/batch.
2022-10-19 00:15:21,671 step [ 168], lr [0.0003000], embedding loss [ 0.7639], quantization loss [ 0.2067],  0.62 sec/batch.
2022-10-19 00:15:23,681 step [ 169], lr [0.0003000], embedding loss [ 0.7488], quantization loss [ 0.1693],  0.62 sec/batch.
2022-10-19 00:15:25,682 step [ 170], lr [0.0003000], embedding loss [ 0.7440], quantization loss [ 0.1891],  0.62 sec/batch.
2022-10-19 00:15:27,648 step [ 171], lr [0.0003000], embedding loss [ 0.7510], quantization loss [ 0.1737],  0.60 sec/batch.
2022-10-19 00:15:29,649 step [ 172], lr [0.0003000], embedding loss [ 0.7531], quantization loss [ 0.1703],  0.62 sec/batch.
2022-10-19 00:15:31,717 step [ 173], lr [0.0003000], embedding loss [ 0.7545], quantization loss [ 0.1679],  0.65 sec/batch.
2022-10-19 00:15:33,785 step [ 174], lr [0.0003000], embedding loss [ 0.7585], quantization loss [ 0.1731],  0.62 sec/batch.
2022-10-19 00:15:35,804 step [ 175], lr [0.0003000], embedding loss [ 0.7535], quantization loss [ 0.1724],  0.63 sec/batch.
2022-10-19 00:15:37,822 step [ 176], lr [0.0003000], embedding loss [ 0.7482], quantization loss [ 0.1712],  0.63 sec/batch.
2022-10-19 00:15:39,817 step [ 177], lr [0.0003000], embedding loss [ 0.7554], quantization loss [ 0.1486],  0.61 sec/batch.
2022-10-19 00:15:41,822 step [ 178], lr [0.0003000], embedding loss [ 0.7585], quantization loss [ 0.1615],  0.62 sec/batch.
2022-10-19 00:15:43,822 step [ 179], lr [0.0003000], embedding loss [ 0.7470], quantization loss [ 0.1572],  0.62 sec/batch.
2022-10-19 00:15:45,781 step [ 180], lr [0.0003000], embedding loss [ 0.7481], quantization loss [ 0.1422],  0.61 sec/batch.
2022-10-19 00:15:47,758 step [ 181], lr [0.0003000], embedding loss [ 0.7554], quantization loss [ 0.1380],  0.61 sec/batch.
2022-10-19 00:15:49,781 step [ 182], lr [0.0003000], embedding loss [ 0.7487], quantization loss [ 0.1624],  0.62 sec/batch.
2022-10-19 00:15:51,847 step [ 183], lr [0.0003000], embedding loss [ 0.7541], quantization loss [ 0.1498],  0.62 sec/batch.
2022-10-19 00:15:53,930 step [ 184], lr [0.0003000], embedding loss [ 0.7380], quantization loss [ 0.1428],  0.63 sec/batch.
2022-10-19 00:15:55,959 step [ 185], lr [0.0003000], embedding loss [ 0.7435], quantization loss [ 0.1393],  0.62 sec/batch.
2022-10-19 00:15:57,935 step [ 186], lr [0.0003000], embedding loss [ 0.7439], quantization loss [ 0.1670],  0.61 sec/batch.
2022-10-19 00:15:59,951 step [ 187], lr [0.0003000], embedding loss [ 0.7484], quantization loss [ 0.1442],  0.62 sec/batch.
2022-10-19 00:16:01,977 step [ 188], lr [0.0003000], embedding loss [ 0.7433], quantization loss [ 0.1442],  0.62 sec/batch.
2022-10-19 00:16:04,045 step [ 189], lr [0.0003000], embedding loss [ 0.7462], quantization loss [ 0.1484],  0.63 sec/batch.
2022-10-19 00:16:06,097 step [ 190], lr [0.0003000], embedding loss [ 0.7523], quantization loss [ 0.1702],  0.63 sec/batch.
2022-10-19 00:16:08,133 step [ 191], lr [0.0003000], embedding loss [ 0.7432], quantization loss [ 0.1393],  0.61 sec/batch.
2022-10-19 00:16:10,173 step [ 192], lr [0.0003000], embedding loss [ 0.7365], quantization loss [ 0.1546],  0.63 sec/batch.
2022-10-19 00:16:12,219 step [ 193], lr [0.0003000], embedding loss [ 0.7437], quantization loss [ 0.1407],  0.63 sec/batch.
2022-10-19 00:16:14,252 step [ 194], lr [0.0003000], embedding loss [ 0.7498], quantization loss [ 0.1634],  0.63 sec/batch.
2022-10-19 00:16:16,281 step [ 195], lr [0.0003000], embedding loss [ 0.7403], quantization loss [ 0.1529],  0.63 sec/batch.
2022-10-19 00:16:18,319 step [ 196], lr [0.0003000], embedding loss [ 0.7561], quantization loss [ 0.1583],  0.62 sec/batch.
2022-10-19 00:16:20,402 step [ 197], lr [0.0003000], embedding loss [ 0.7405], quantization loss [ 0.1522],  0.63 sec/batch.
2022-10-19 00:16:22,446 step [ 198], lr [0.0003000], embedding loss [ 0.7510], quantization loss [ 0.1414],  0.62 sec/batch.
2022-10-19 00:16:24,457 step [ 199], lr [0.0003000], embedding loss [ 0.7574], quantization loss [ 0.1576],  0.62 sec/batch.
2022-10-19 00:16:26,466 step [ 200], lr [0.0003000], embedding loss [ 0.7423], quantization loss [ 0.1385],  0.63 sec/batch.
2022-10-19 00:16:28,492 step [ 201], lr [0.0003000], embedding loss [ 0.7612], quantization loss [ 0.1205],  0.62 sec/batch.
2022-10-19 00:16:30,508 step [ 202], lr [0.0003000], embedding loss [ 0.7463], quantization loss [ 0.1459],  0.63 sec/batch.
2022-10-19 00:16:32,568 step [ 203], lr [0.0003000], embedding loss [ 0.7425], quantization loss [ 0.1245],  0.62 sec/batch.
2022-10-19 00:16:34,570 step [ 204], lr [0.0003000], embedding loss [ 0.7393], quantization loss [ 0.1299],  0.62 sec/batch.
2022-10-19 00:16:36,583 step [ 205], lr [0.0003000], embedding loss [ 0.7562], quantization loss [ 0.1305],  0.62 sec/batch.
2022-10-19 00:16:38,594 step [ 206], lr [0.0003000], embedding loss [ 0.7495], quantization loss [ 0.1280],  0.61 sec/batch.
2022-10-19 00:16:40,581 step [ 207], lr [0.0003000], embedding loss [ 0.7376], quantization loss [ 0.1200],  0.61 sec/batch.
2022-10-19 00:16:42,535 step [ 208], lr [0.0003000], embedding loss [ 0.7509], quantization loss [ 0.1640],  0.61 sec/batch.
2022-10-19 00:16:44,565 step [ 209], lr [0.0003000], embedding loss [ 0.7401], quantization loss [ 0.1076],  0.62 sec/batch.
2022-10-19 00:16:46,568 step [ 210], lr [0.0003000], embedding loss [ 0.7508], quantization loss [ 0.1203],  0.62 sec/batch.
2022-10-19 00:16:48,615 step [ 211], lr [0.0003000], embedding loss [ 0.7434], quantization loss [ 0.1335],  0.63 sec/batch.
2022-10-19 00:16:50,665 step [ 212], lr [0.0003000], embedding loss [ 0.7466], quantization loss [ 0.1335],  0.63 sec/batch.
2022-10-19 00:16:52,759 step [ 213], lr [0.0003000], embedding loss [ 0.7501], quantization loss [ 0.1307],  0.62 sec/batch.
2022-10-19 00:16:54,744 step [ 214], lr [0.0003000], embedding loss [ 0.7399], quantization loss [ 0.1350],  0.60 sec/batch.
2022-10-19 00:16:56,715 step [ 215], lr [0.0003000], embedding loss [ 0.7357], quantization loss [ 0.1125],  0.58 sec/batch.
2022-10-19 00:16:58,716 step [ 216], lr [0.0003000], embedding loss [ 0.7494], quantization loss [ 0.1255],  0.60 sec/batch.
2022-10-19 00:17:00,762 step [ 217], lr [0.0003000], embedding loss [ 0.7508], quantization loss [ 0.1250],  0.61 sec/batch.
2022-10-19 00:17:02,774 step [ 218], lr [0.0003000], embedding loss [ 0.7446], quantization loss [ 0.1313],  0.60 sec/batch.
2022-10-19 00:17:04,810 step [ 219], lr [0.0003000], embedding loss [ 0.7581], quantization loss [ 0.1322],  0.58 sec/batch.
2022-10-19 00:17:06,785 step [ 220], lr [0.0003000], embedding loss [ 0.7491], quantization loss [ 0.1290],  0.59 sec/batch.
2022-10-19 00:17:08,770 step [ 221], lr [0.0003000], embedding loss [ 0.7494], quantization loss [ 0.1216],  0.59 sec/batch.
2022-10-19 00:17:10,708 step [ 222], lr [0.0003000], embedding loss [ 0.7559], quantization loss [ 0.1251],  0.58 sec/batch.
2022-10-19 00:17:12,738 step [ 223], lr [0.0003000], embedding loss [ 0.7462], quantization loss [ 0.1264],  0.60 sec/batch.
2022-10-19 00:17:14,717 step [ 224], lr [0.0003000], embedding loss [ 0.7490], quantization loss [ 0.1371],  0.60 sec/batch.
2022-10-19 00:17:16,752 step [ 225], lr [0.0003000], embedding loss [ 0.7473], quantization loss [ 0.1281],  0.60 sec/batch.
2022-10-19 00:17:18,768 step [ 226], lr [0.0003000], embedding loss [ 0.7505], quantization loss [ 0.1333],  0.61 sec/batch.
2022-10-19 00:17:20,797 step [ 227], lr [0.0003000], embedding loss [ 0.7393], quantization loss [ 0.1365],  0.61 sec/batch.
2022-10-19 00:17:22,772 step [ 228], lr [0.0003000], embedding loss [ 0.7427], quantization loss [ 0.1337],  0.59 sec/batch.
2022-10-19 00:17:24,797 step [ 229], lr [0.0003000], embedding loss [ 0.7489], quantization loss [ 0.1292],  0.60 sec/batch.
2022-10-19 00:17:26,796 step [ 230], lr [0.0003000], embedding loss [ 0.7403], quantization loss [ 0.1351],  0.60 sec/batch.
2022-10-19 00:17:28,797 step [ 231], lr [0.0003000], embedding loss [ 0.7427], quantization loss [ 0.1206],  0.60 sec/batch.
2022-10-19 00:17:30,792 step [ 232], lr [0.0003000], embedding loss [ 0.7524], quantization loss [ 0.1167],  0.61 sec/batch.
2022-10-19 00:17:32,789 step [ 233], lr [0.0003000], embedding loss [ 0.7506], quantization loss [ 0.1358],  0.60 sec/batch.
2022-10-19 00:17:34,797 step [ 234], lr [0.0003000], embedding loss [ 0.7392], quantization loss [ 0.1285],  0.61 sec/batch.
2022-10-19 00:17:36,790 step [ 235], lr [0.0003000], embedding loss [ 0.7546], quantization loss [ 0.1427],  0.59 sec/batch.
2022-10-19 00:17:38,803 step [ 236], lr [0.0003000], embedding loss [ 0.7570], quantization loss [ 0.1245],  0.60 sec/batch.
2022-10-19 00:17:40,798 step [ 237], lr [0.0003000], embedding loss [ 0.7438], quantization loss [ 0.1291],  0.59 sec/batch.
2022-10-19 00:17:42,790 step [ 238], lr [0.0003000], embedding loss [ 0.7539], quantization loss [ 0.1314],  0.59 sec/batch.
2022-10-19 00:17:44,783 step [ 239], lr [0.0003000], embedding loss [ 0.7449], quantization loss [ 0.1243],  0.59 sec/batch.
2022-10-19 00:17:46,763 step [ 240], lr [0.0003000], embedding loss [ 0.7366], quantization loss [ 0.1159],  0.59 sec/batch.
2022-10-19 00:17:48,769 step [ 241], lr [0.0003000], embedding loss [ 0.7498], quantization loss [ 0.1101],  0.60 sec/batch.
2022-10-19 00:17:48,769 update codes and centers iter(1/1).
2022-10-19 00:17:50,688 number of update_code wrong: 0.
2022-10-19 00:17:53,652 non zero codewords: 512.
2022-10-19 00:17:53,652 finish center update, duration: 4.88 sec.
2022-10-19 00:17:55,597 step [ 242], lr [0.0003000], embedding loss [ 0.7480], quantization loss [ 0.0937],  0.59 sec/batch.
2022-10-19 00:17:57,581 step [ 243], lr [0.0003000], embedding loss [ 0.7492], quantization loss [ 0.1213],  0.59 sec/batch.
2022-10-19 00:17:59,600 step [ 244], lr [0.0003000], embedding loss [ 0.7383], quantization loss [ 0.1009],  0.62 sec/batch.
2022-10-19 00:18:01,648 step [ 245], lr [0.0003000], embedding loss [ 0.7450], quantization loss [ 0.1041],  0.61 sec/batch.
2022-10-19 00:18:03,703 step [ 246], lr [0.0003000], embedding loss [ 0.7493], quantization loss [ 0.0993],  0.61 sec/batch.
2022-10-19 00:18:05,823 step [ 247], lr [0.0003000], embedding loss [ 0.7549], quantization loss [ 0.1063],  0.62 sec/batch.
2022-10-19 00:18:07,888 step [ 248], lr [0.0003000], embedding loss [ 0.7488], quantization loss [ 0.1011],  0.61 sec/batch.
2022-10-19 00:18:09,916 step [ 249], lr [0.0003000], embedding loss [ 0.7472], quantization loss [ 0.0968],  0.61 sec/batch.
2022-10-19 00:18:11,937 step [ 250], lr [0.0003000], embedding loss [ 0.7403], quantization loss [ 0.0971],  0.60 sec/batch.
2022-10-19 00:18:14,001 step [ 251], lr [0.0003000], embedding loss [ 0.7623], quantization loss [ 0.1142],  0.61 sec/batch.
2022-10-19 00:18:16,059 step [ 252], lr [0.0003000], embedding loss [ 0.7409], quantization loss [ 0.1093],  0.61 sec/batch.
2022-10-19 00:18:18,081 step [ 253], lr [0.0003000], embedding loss [ 0.7606], quantization loss [ 0.1030],  0.60 sec/batch.
2022-10-19 00:18:20,102 step [ 254], lr [0.0003000], embedding loss [ 0.7483], quantization loss [ 0.1034],  0.60 sec/batch.
2022-10-19 00:18:22,117 step [ 255], lr [0.0003000], embedding loss [ 0.7542], quantization loss [ 0.1054],  0.60 sec/batch.
2022-10-19 00:18:24,121 step [ 256], lr [0.0003000], embedding loss [ 0.7424], quantization loss [ 0.1103],  0.60 sec/batch.
2022-10-19 00:18:26,133 step [ 257], lr [0.0003000], embedding loss [ 0.7559], quantization loss [ 0.1088],  0.60 sec/batch.
2022-10-19 00:18:28,011 step [ 258], lr [0.0003000], embedding loss [ 0.7527], quantization loss [ 0.0918],  0.60 sec/batch.
2022-10-19 00:18:30,042 step [ 259], lr [0.0003000], embedding loss [ 0.7514], quantization loss [ 0.1011],  0.60 sec/batch.
2022-10-19 00:18:32,089 step [ 260], lr [0.0003000], embedding loss [ 0.7341], quantization loss [ 0.0984],  0.61 sec/batch.
2022-10-19 00:18:34,097 step [ 261], lr [0.0003000], embedding loss [ 0.7468], quantization loss [ 0.1138],  0.61 sec/batch.
2022-10-19 00:18:36,113 step [ 262], lr [0.0003000], embedding loss [ 0.7289], quantization loss [ 0.0972],  0.61 sec/batch.
2022-10-19 00:18:38,160 step [ 263], lr [0.0003000], embedding loss [ 0.7429], quantization loss [ 0.0945],  0.61 sec/batch.
2022-10-19 00:18:40,143 step [ 264], lr [0.0003000], embedding loss [ 0.7482], quantization loss [ 0.1152],  0.58 sec/batch.
2022-10-19 00:18:42,104 step [ 265], lr [0.0003000], embedding loss [ 0.7390], quantization loss [ 0.1123],  0.58 sec/batch.
2022-10-19 00:18:44,071 step [ 266], lr [0.0003000], embedding loss [ 0.7471], quantization loss [ 0.1110],  0.59 sec/batch.
2022-10-19 00:18:46,031 step [ 267], lr [0.0003000], embedding loss [ 0.7346], quantization loss [ 0.1159],  0.58 sec/batch.
2022-10-19 00:18:48,016 step [ 268], lr [0.0003000], embedding loss [ 0.7474], quantization loss [ 0.0940],  0.59 sec/batch.
2022-10-19 00:18:49,998 step [ 269], lr [0.0003000], embedding loss [ 0.7513], quantization loss [ 0.1057],  0.59 sec/batch.
2022-10-19 00:18:51,967 step [ 270], lr [0.0003000], embedding loss [ 0.7385], quantization loss [ 0.0987],  0.57 sec/batch.
2022-10-19 00:18:54,003 step [ 271], lr [0.0003000], embedding loss [ 0.7367], quantization loss [ 0.0966],  0.58 sec/batch.
2022-10-19 00:18:55,978 step [ 272], lr [0.0003000], embedding loss [ 0.7526], quantization loss [ 0.0995],  0.57 sec/batch.
2022-10-19 00:18:58,010 step [ 273], lr [0.0003000], embedding loss [ 0.7528], quantization loss [ 0.0914],  0.59 sec/batch.
2022-10-19 00:19:00,017 step [ 274], lr [0.0003000], embedding loss [ 0.7443], quantization loss [ 0.0969],  0.60 sec/batch.
2022-10-19 00:19:02,060 step [ 275], lr [0.0003000], embedding loss [ 0.7452], quantization loss [ 0.0888],  0.60 sec/batch.
2022-10-19 00:19:04,086 step [ 276], lr [0.0003000], embedding loss [ 0.7525], quantization loss [ 0.0968],  0.60 sec/batch.
2022-10-19 00:19:06,101 step [ 277], lr [0.0003000], embedding loss [ 0.7312], quantization loss [ 0.0941],  0.60 sec/batch.
2022-10-19 00:19:08,156 step [ 278], lr [0.0003000], embedding loss [ 0.7397], quantization loss [ 0.1007],  0.60 sec/batch.
2022-10-19 00:19:10,162 step [ 279], lr [0.0003000], embedding loss [ 0.7462], quantization loss [ 0.0927],  0.60 sec/batch.
2022-10-19 00:19:12,176 step [ 280], lr [0.0003000], embedding loss [ 0.7430], quantization loss [ 0.1015],  0.61 sec/batch.
2022-10-19 00:19:14,157 step [ 281], lr [0.0003000], embedding loss [ 0.7460], quantization loss [ 0.1123],  0.59 sec/batch.
2022-10-19 00:19:16,164 step [ 282], lr [0.0003000], embedding loss [ 0.7311], quantization loss [ 0.0907],  0.60 sec/batch.
2022-10-19 00:19:18,172 step [ 283], lr [0.0003000], embedding loss [ 0.7463], quantization loss [ 0.0815],  0.60 sec/batch.
2022-10-19 00:19:20,214 step [ 284], lr [0.0003000], embedding loss [ 0.7446], quantization loss [ 0.0896],  0.60 sec/batch.
2022-10-19 00:19:22,203 step [ 285], lr [0.0003000], embedding loss [ 0.7494], quantization loss [ 0.0908],  0.56 sec/batch.
2022-10-19 00:19:24,218 step [ 286], lr [0.0003000], embedding loss [ 0.7435], quantization loss [ 0.0897],  0.58 sec/batch.
2022-10-19 00:19:26,191 step [ 287], lr [0.0003000], embedding loss [ 0.7597], quantization loss [ 0.0962],  0.57 sec/batch.
2022-10-19 00:19:28,191 step [ 288], lr [0.0003000], embedding loss [ 0.7484], quantization loss [ 0.0892],  0.57 sec/batch.
2022-10-19 00:19:30,176 step [ 289], lr [0.0003000], embedding loss [ 0.7518], quantization loss [ 0.0908],  0.58 sec/batch.
2022-10-19 00:19:32,095 step [ 290], lr [0.0003000], embedding loss [ 0.7485], quantization loss [ 0.0920],  0.57 sec/batch.
2022-10-19 00:19:34,067 step [ 291], lr [0.0003000], embedding loss [ 0.7479], quantization loss [ 0.0829],  0.57 sec/batch.
2022-10-19 00:19:36,024 step [ 292], lr [0.0003000], embedding loss [ 0.7458], quantization loss [ 0.0856],  0.58 sec/batch.
2022-10-19 00:19:37,995 step [ 293], lr [0.0003000], embedding loss [ 0.7493], quantization loss [ 0.0844],  0.57 sec/batch.
2022-10-19 00:19:39,975 step [ 294], lr [0.0003000], embedding loss [ 0.7440], quantization loss [ 0.0852],  0.58 sec/batch.
2022-10-19 00:19:41,998 step [ 295], lr [0.0003000], embedding loss [ 0.7400], quantization loss [ 0.0858],  0.60 sec/batch.
2022-10-19 00:19:44,004 step [ 296], lr [0.0003000], embedding loss [ 0.7397], quantization loss [ 0.0793],  0.60 sec/batch.
2022-10-19 00:19:46,000 step [ 297], lr [0.0003000], embedding loss [ 0.7525], quantization loss [ 0.0941],  0.60 sec/batch.
2022-10-19 00:19:47,996 step [ 298], lr [0.0003000], embedding loss [ 0.7505], quantization loss [ 0.0863],  0.61 sec/batch.
2022-10-19 00:19:50,009 step [ 299], lr [0.0003000], embedding loss [ 0.7367], quantization loss [ 0.0843],  0.60 sec/batch.
2022-10-19 00:19:52,014 step [ 300], lr [0.0003000], embedding loss [ 0.7401], quantization loss [ 0.0842],  0.59 sec/batch.
2022-10-19 00:19:54,030 step [ 301], lr [0.0001500], embedding loss [ 0.7324], quantization loss [ 0.0931],  0.60 sec/batch.
2022-10-19 00:19:56,049 step [ 302], lr [0.0001500], embedding loss [ 0.7430], quantization loss [ 0.0840],  0.60 sec/batch.
2022-10-19 00:19:58,039 step [ 303], lr [0.0001500], embedding loss [ 0.7509], quantization loss [ 0.1033],  0.60 sec/batch.
2022-10-19 00:20:00,059 step [ 304], lr [0.0001500], embedding loss [ 0.7386], quantization loss [ 0.0891],  0.60 sec/batch.
2022-10-19 00:20:02,074 step [ 305], lr [0.0001500], embedding loss [ 0.7354], quantization loss [ 0.0853],  0.60 sec/batch.
2022-10-19 00:20:04,096 step [ 306], lr [0.0001500], embedding loss [ 0.7458], quantization loss [ 0.0894],  0.61 sec/batch.
2022-10-19 00:20:06,119 step [ 307], lr [0.0001500], embedding loss [ 0.7425], quantization loss [ 0.0877],  0.60 sec/batch.
2022-10-19 00:20:08,142 step [ 308], lr [0.0001500], embedding loss [ 0.7385], quantization loss [ 0.0767],  0.60 sec/batch.
2022-10-19 00:20:10,166 step [ 309], lr [0.0001500], embedding loss [ 0.7469], quantization loss [ 0.0909],  0.61 sec/batch.
2022-10-19 00:20:12,217 step [ 310], lr [0.0001500], embedding loss [ 0.7486], quantization loss [ 0.0758],  0.60 sec/batch.
2022-10-19 00:20:14,269 step [ 311], lr [0.0001500], embedding loss [ 0.7466], quantization loss [ 0.0795],  0.60 sec/batch.
2022-10-19 00:20:16,286 step [ 312], lr [0.0001500], embedding loss [ 0.7458], quantization loss [ 0.0898],  0.61 sec/batch.
2022-10-19 00:20:18,294 step [ 313], lr [0.0001500], embedding loss [ 0.7434], quantization loss [ 0.0862],  0.60 sec/batch.
2022-10-19 00:20:20,332 step [ 314], lr [0.0001500], embedding loss [ 0.7444], quantization loss [ 0.0844],  0.59 sec/batch.
2022-10-19 00:20:22,343 step [ 315], lr [0.0001500], embedding loss [ 0.7411], quantization loss [ 0.0816],  0.60 sec/batch.
2022-10-19 00:20:24,354 step [ 316], lr [0.0001500], embedding loss [ 0.7547], quantization loss [ 0.0967],  0.60 sec/batch.
2022-10-19 00:20:26,316 step [ 317], lr [0.0001500], embedding loss [ 0.7504], quantization loss [ 0.0812],  0.59 sec/batch.
2022-10-19 00:20:28,329 step [ 318], lr [0.0001500], embedding loss [ 0.7409], quantization loss [ 0.0846],  0.60 sec/batch.
2022-10-19 00:20:30,353 step [ 319], lr [0.0001500], embedding loss [ 0.7464], quantization loss [ 0.0859],  0.61 sec/batch.
2022-10-19 00:20:32,398 step [ 320], lr [0.0001500], embedding loss [ 0.7425], quantization loss [ 0.0796],  0.60 sec/batch.
2022-10-19 00:20:34,421 step [ 321], lr [0.0001500], embedding loss [ 0.7390], quantization loss [ 0.0760],  0.60 sec/batch.
2022-10-19 00:20:34,421 update codes and centers iter(1/1).
2022-10-19 00:20:36,390 number of update_code wrong: 0.
2022-10-19 00:20:39,066 non zero codewords: 512.
2022-10-19 00:20:39,067 finish center update, duration: 4.65 sec.
2022-10-19 00:20:40,961 step [ 322], lr [0.0001500], embedding loss [ 0.7453], quantization loss [ 0.0799],  0.60 sec/batch.
2022-10-19 00:20:42,986 step [ 323], lr [0.0001500], embedding loss [ 0.7355], quantization loss [ 0.0765],  0.60 sec/batch.
2022-10-19 00:20:45,062 step [ 324], lr [0.0001500], embedding loss [ 0.7471], quantization loss [ 0.0737],  0.64 sec/batch.
2022-10-19 00:20:47,141 step [ 325], lr [0.0001500], embedding loss [ 0.7354], quantization loss [ 0.0718],  0.63 sec/batch.
2022-10-19 00:20:49,102 step [ 326], lr [0.0001500], embedding loss [ 0.7436], quantization loss [ 0.0742],  0.62 sec/batch.
2022-10-19 00:20:51,180 step [ 327], lr [0.0001500], embedding loss [ 0.7485], quantization loss [ 0.0818],  0.65 sec/batch.
2022-10-19 00:20:53,238 step [ 328], lr [0.0001500], embedding loss [ 0.7208], quantization loss [ 0.0842],  0.63 sec/batch.
2022-10-19 00:20:55,300 step [ 329], lr [0.0001500], embedding loss [ 0.7459], quantization loss [ 0.0773],  0.63 sec/batch.
2022-10-19 00:20:57,366 step [ 330], lr [0.0001500], embedding loss [ 0.7563], quantization loss [ 0.0797],  0.63 sec/batch.
2022-10-19 00:20:59,404 step [ 331], lr [0.0001500], embedding loss [ 0.7554], quantization loss [ 0.0870],  0.64 sec/batch.
2022-10-19 00:21:01,514 step [ 332], lr [0.0001500], embedding loss [ 0.7412], quantization loss [ 0.0771],  0.69 sec/batch.
2022-10-19 00:21:03,574 step [ 333], lr [0.0001500], embedding loss [ 0.7363], quantization loss [ 0.0739],  0.64 sec/batch.
2022-10-19 00:21:05,734 step [ 334], lr [0.0001500], embedding loss [ 0.7465], quantization loss [ 0.0793],  0.64 sec/batch.
2022-10-19 00:21:07,785 step [ 335], lr [0.0001500], embedding loss [ 0.7352], quantization loss [ 0.0799],  0.63 sec/batch.
2022-10-19 00:21:09,829 step [ 336], lr [0.0001500], embedding loss [ 0.7447], quantization loss [ 0.0737],  0.64 sec/batch.
2022-10-19 00:21:11,865 step [ 337], lr [0.0001500], embedding loss [ 0.7446], quantization loss [ 0.0702],  0.62 sec/batch.
2022-10-19 00:21:13,851 step [ 338], lr [0.0001500], embedding loss [ 0.7553], quantization loss [ 0.0740],  0.56 sec/batch.
2022-10-19 00:21:15,971 step [ 339], lr [0.0001500], embedding loss [ 0.7486], quantization loss [ 0.0766],  0.57 sec/batch.
2022-10-19 00:21:17,955 step [ 340], lr [0.0001500], embedding loss [ 0.7352], quantization loss [ 0.0636],  0.57 sec/batch.
2022-10-19 00:21:19,982 step [ 341], lr [0.0001500], embedding loss [ 0.7382], quantization loss [ 0.0739],  0.56 sec/batch.
2022-10-19 00:21:21,923 step [ 342], lr [0.0001500], embedding loss [ 0.7448], quantization loss [ 0.0677],  0.57 sec/batch.
2022-10-19 00:21:23,806 step [ 343], lr [0.0001500], embedding loss [ 0.7334], quantization loss [ 0.0762],  0.55 sec/batch.
2022-10-19 00:21:25,767 step [ 344], lr [0.0001500], embedding loss [ 0.7449], quantization loss [ 0.0750],  0.57 sec/batch.
2022-10-19 00:21:27,683 step [ 345], lr [0.0001500], embedding loss [ 0.7448], quantization loss [ 0.0927],  0.56 sec/batch.
2022-10-19 00:21:29,576 step [ 346], lr [0.0001500], embedding loss [ 0.7421], quantization loss [ 0.0724],  0.53 sec/batch.
2022-10-19 00:21:31,485 step [ 347], lr [0.0001500], embedding loss [ 0.7454], quantization loss [ 0.0865],  0.53 sec/batch.
2022-10-19 00:21:33,558 step [ 348], lr [0.0001500], embedding loss [ 0.7355], quantization loss [ 0.0712],  0.66 sec/batch.
2022-10-19 00:21:35,601 step [ 349], lr [0.0001500], embedding loss [ 0.7606], quantization loss [ 0.0745],  0.63 sec/batch.
2022-10-19 00:21:37,666 step [ 350], lr [0.0001500], embedding loss [ 0.7457], quantization loss [ 0.0702],  0.63 sec/batch.
2022-10-19 00:21:39,838 step [ 351], lr [0.0001500], embedding loss [ 0.7494], quantization loss [ 0.0630],  0.65 sec/batch.
2022-10-19 00:21:41,916 step [ 352], lr [0.0001500], embedding loss [ 0.7441], quantization loss [ 0.0734],  0.64 sec/batch.
2022-10-19 00:21:43,995 step [ 353], lr [0.0001500], embedding loss [ 0.7424], quantization loss [ 0.0758],  0.63 sec/batch.
2022-10-19 00:21:46,063 step [ 354], lr [0.0001500], embedding loss [ 0.7287], quantization loss [ 0.0711],  0.64 sec/batch.
2022-10-19 00:21:48,159 step [ 355], lr [0.0001500], embedding loss [ 0.7490], quantization loss [ 0.0750],  0.64 sec/batch.
2022-10-19 00:21:50,302 step [ 356], lr [0.0001500], embedding loss [ 0.7442], quantization loss [ 0.0742],  0.64 sec/batch.
2022-10-19 00:21:52,340 step [ 357], lr [0.0001500], embedding loss [ 0.7460], quantization loss [ 0.0682],  0.62 sec/batch.
2022-10-19 00:21:54,417 step [ 358], lr [0.0001500], embedding loss [ 0.7472], quantization loss [ 0.0662],  0.64 sec/batch.
2022-10-19 00:21:56,490 step [ 359], lr [0.0001500], embedding loss [ 0.7370], quantization loss [ 0.0703],  0.63 sec/batch.
2022-10-19 00:21:58,536 step [ 360], lr [0.0001500], embedding loss [ 0.7429], quantization loss [ 0.0686],  0.64 sec/batch.
2022-10-19 00:22:00,575 step [ 361], lr [0.0001500], embedding loss [ 0.7488], quantization loss [ 0.0718],  0.63 sec/batch.
2022-10-19 00:22:02,659 step [ 362], lr [0.0001500], embedding loss [ 0.7389], quantization loss [ 0.0620],  0.64 sec/batch.
2022-10-19 00:22:04,739 step [ 363], lr [0.0001500], embedding loss [ 0.7448], quantization loss [ 0.0599],  0.63 sec/batch.
2022-10-19 00:22:06,834 step [ 364], lr [0.0001500], embedding loss [ 0.7507], quantization loss [ 0.0635],  0.64 sec/batch.
2022-10-19 00:22:08,888 step [ 365], lr [0.0001500], embedding loss [ 0.7608], quantization loss [ 0.0656],  0.61 sec/batch.
2022-10-19 00:22:10,935 step [ 366], lr [0.0001500], embedding loss [ 0.7441], quantization loss [ 0.0732],  0.64 sec/batch.
2022-10-19 00:22:13,087 step [ 367], lr [0.0001500], embedding loss [ 0.7509], quantization loss [ 0.0748],  0.64 sec/batch.
2022-10-19 00:22:15,136 step [ 368], lr [0.0001500], embedding loss [ 0.7521], quantization loss [ 0.0691],  0.64 sec/batch.
2022-10-19 00:22:17,204 step [ 369], lr [0.0001500], embedding loss [ 0.7431], quantization loss [ 0.0652],  0.64 sec/batch.
2022-10-19 00:22:19,193 step [ 370], lr [0.0001500], embedding loss [ 0.7417], quantization loss [ 0.0724],  0.63 sec/batch.
2022-10-19 00:22:21,282 step [ 371], lr [0.0001500], embedding loss [ 0.7456], quantization loss [ 0.0728],  0.64 sec/batch.
2022-10-19 00:22:23,393 step [ 372], lr [0.0001500], embedding loss [ 0.7273], quantization loss [ 0.0667],  0.66 sec/batch.
2022-10-19 00:22:25,505 step [ 373], lr [0.0001500], embedding loss [ 0.7433], quantization loss [ 0.0666],  0.64 sec/batch.
2022-10-19 00:22:27,589 step [ 374], lr [0.0001500], embedding loss [ 0.7352], quantization loss [ 0.0637],  0.64 sec/batch.
2022-10-19 00:22:29,683 step [ 375], lr [0.0001500], embedding loss [ 0.7370], quantization loss [ 0.0670],  0.64 sec/batch.
2022-10-19 00:22:31,691 step [ 376], lr [0.0001500], embedding loss [ 0.7343], quantization loss [ 0.0730],  0.63 sec/batch.
2022-10-19 00:22:33,653 step [ 377], lr [0.0001500], embedding loss [ 0.7388], quantization loss [ 0.0740],  0.56 sec/batch.
2022-10-19 00:22:35,704 step [ 378], lr [0.0001500], embedding loss [ 0.7376], quantization loss [ 0.0663],  0.56 sec/batch.
2022-10-19 00:22:37,689 step [ 379], lr [0.0001500], embedding loss [ 0.7402], quantization loss [ 0.0581],  0.54 sec/batch.
2022-10-19 00:22:39,777 step [ 380], lr [0.0001500], embedding loss [ 0.7478], quantization loss [ 0.0654],  0.56 sec/batch.
2022-10-19 00:22:41,800 step [ 381], lr [0.0001500], embedding loss [ 0.7431], quantization loss [ 0.0662],  0.56 sec/batch.
2022-10-19 00:22:43,759 step [ 382], lr [0.0001500], embedding loss [ 0.7494], quantization loss [ 0.0639],  0.55 sec/batch.
2022-10-19 00:22:45,718 step [ 383], lr [0.0001500], embedding loss [ 0.7476], quantization loss [ 0.0705],  0.55 sec/batch.
2022-10-19 00:22:47,675 step [ 384], lr [0.0001500], embedding loss [ 0.7520], quantization loss [ 0.0635],  0.55 sec/batch.
2022-10-19 00:22:49,641 step [ 385], lr [0.0001500], embedding loss [ 0.7464], quantization loss [ 0.0618],  0.56 sec/batch.
2022-10-19 00:22:51,602 step [ 386], lr [0.0001500], embedding loss [ 0.7465], quantization loss [ 0.0639],  0.55 sec/batch.
2022-10-19 00:22:53,788 step [ 387], lr [0.0001500], embedding loss [ 0.7405], quantization loss [ 0.0678],  0.54 sec/batch.
2022-10-19 00:22:55,747 step [ 388], lr [0.0001500], embedding loss [ 0.7374], quantization loss [ 0.0664],  0.56 sec/batch.
2022-10-19 00:22:57,716 step [ 389], lr [0.0001500], embedding loss [ 0.7396], quantization loss [ 0.0725],  0.56 sec/batch.
2022-10-19 00:22:59,661 step [ 390], lr [0.0001500], embedding loss [ 0.7473], quantization loss [ 0.0661],  0.54 sec/batch.
2022-10-19 00:23:01,652 step [ 391], lr [0.0001500], embedding loss [ 0.7330], quantization loss [ 0.0725],  0.57 sec/batch.
2022-10-19 00:23:03,652 step [ 392], lr [0.0001500], embedding loss [ 0.7446], quantization loss [ 0.0672],  0.56 sec/batch.
2022-10-19 00:23:05,675 step [ 393], lr [0.0001500], embedding loss [ 0.7394], quantization loss [ 0.0629],  0.55 sec/batch.
2022-10-19 00:23:07,716 step [ 394], lr [0.0001500], embedding loss [ 0.7524], quantization loss [ 0.0618],  0.57 sec/batch.
2022-10-19 00:23:09,751 step [ 395], lr [0.0001500], embedding loss [ 0.7308], quantization loss [ 0.0684],  0.56 sec/batch.
2022-10-19 00:23:11,677 step [ 396], lr [0.0001500], embedding loss [ 0.7401], quantization loss [ 0.0660],  0.55 sec/batch.
2022-10-19 00:23:13,671 step [ 397], lr [0.0001500], embedding loss [ 0.7453], quantization loss [ 0.0742],  0.56 sec/batch.
2022-10-19 00:23:15,556 step [ 398], lr [0.0001500], embedding loss [ 0.7329], quantization loss [ 0.0699],  0.55 sec/batch.
2022-10-19 00:23:17,495 step [ 399], lr [0.0001500], embedding loss [ 0.7334], quantization loss [ 0.0715],  0.56 sec/batch.
2022-10-19 00:23:19,394 step [ 400], lr [0.0001500], embedding loss [ 0.7554], quantization loss [ 0.0635],  0.55 sec/batch.
2022-10-19 00:23:21,382 step [ 401], lr [0.0001500], embedding loss [ 0.7426], quantization loss [ 0.0561],  0.56 sec/batch.
2022-10-19 00:23:21,382 update codes and centers iter(1/1).
2022-10-19 00:23:23,150 number of update_code wrong: 0.
2022-10-19 00:23:26,218 non zero codewords: 512.
2022-10-19 00:23:26,219 finish center update, duration: 4.84 sec.
2022-10-19 00:23:28,421 step [ 402], lr [0.0001500], embedding loss [ 0.7399], quantization loss [ 0.0605],  0.78 sec/batch.
2022-10-19 00:23:30,384 step [ 403], lr [0.0001500], embedding loss [ 0.7308], quantization loss [ 0.0548],  0.52 sec/batch.
2022-10-19 00:23:32,372 step [ 404], lr [0.0001500], embedding loss [ 0.7290], quantization loss [ 0.0626],  0.51 sec/batch.
2022-10-19 00:23:34,338 step [ 405], lr [0.0001500], embedding loss [ 0.7503], quantization loss [ 0.0618],  0.53 sec/batch.
2022-10-19 00:23:36,335 step [ 406], lr [0.0001500], embedding loss [ 0.7419], quantization loss [ 0.0591],  0.53 sec/batch.
2022-10-19 00:23:38,309 step [ 407], lr [0.0001500], embedding loss [ 0.7399], quantization loss [ 0.0532],  0.52 sec/batch.
2022-10-19 00:23:40,300 step [ 408], lr [0.0001500], embedding loss [ 0.7339], quantization loss [ 0.0524],  0.53 sec/batch.
2022-10-19 00:23:42,343 step [ 409], lr [0.0001500], embedding loss [ 0.7269], quantization loss [ 0.0558],  0.58 sec/batch.
2022-10-19 00:23:44,351 step [ 410], lr [0.0001500], embedding loss [ 0.7458], quantization loss [ 0.0632],  0.53 sec/batch.
2022-10-19 00:23:46,399 step [ 411], lr [0.0001500], embedding loss [ 0.7471], quantization loss [ 0.0733],  0.53 sec/batch.
2022-10-19 00:23:48,359 step [ 412], lr [0.0001500], embedding loss [ 0.7515], quantization loss [ 0.0630],  0.53 sec/batch.
2022-10-19 00:23:50,321 step [ 413], lr [0.0001500], embedding loss [ 0.7343], quantization loss [ 0.0610],  0.53 sec/batch.
2022-10-19 00:23:52,280 step [ 414], lr [0.0001500], embedding loss [ 0.7517], quantization loss [ 0.0659],  0.53 sec/batch.
2022-10-19 00:23:54,237 step [ 415], lr [0.0001500], embedding loss [ 0.7405], quantization loss [ 0.0676],  0.52 sec/batch.
2022-10-19 00:23:56,165 step [ 416], lr [0.0001500], embedding loss [ 0.7485], quantization loss [ 0.0712],  0.52 sec/batch.
2022-10-19 00:23:58,143 step [ 417], lr [0.0001500], embedding loss [ 0.7444], quantization loss [ 0.0550],  0.53 sec/batch.
2022-10-19 00:24:00,113 step [ 418], lr [0.0001500], embedding loss [ 0.7467], quantization loss [ 0.0588],  0.53 sec/batch.
2022-10-19 00:24:02,127 step [ 419], lr [0.0001500], embedding loss [ 0.7474], quantization loss [ 0.0604],  0.52 sec/batch.
2022-10-19 00:24:04,100 step [ 420], lr [0.0001500], embedding loss [ 0.7397], quantization loss [ 0.0630],  0.53 sec/batch.
2022-10-19 00:24:06,058 step [ 421], lr [0.0001500], embedding loss [ 0.7356], quantization loss [ 0.0595],  0.52 sec/batch.
2022-10-19 00:24:08,017 step [ 422], lr [0.0001500], embedding loss [ 0.7345], quantization loss [ 0.0554],  0.53 sec/batch.
2022-10-19 00:24:09,977 step [ 423], lr [0.0001500], embedding loss [ 0.7501], quantization loss [ 0.0562],  0.53 sec/batch.
2022-10-19 00:24:11,935 step [ 424], lr [0.0001500], embedding loss [ 0.7428], quantization loss [ 0.0617],  0.53 sec/batch.
2022-10-19 00:24:13,897 step [ 425], lr [0.0001500], embedding loss [ 0.7393], quantization loss [ 0.0621],  0.53 sec/batch.
2022-10-19 00:24:15,863 step [ 426], lr [0.0001500], embedding loss [ 0.7378], quantization loss [ 0.0582],  0.53 sec/batch.
2022-10-19 00:24:17,822 step [ 427], lr [0.0001500], embedding loss [ 0.7465], quantization loss [ 0.0637],  0.53 sec/batch.
2022-10-19 00:24:19,761 step [ 428], lr [0.0001500], embedding loss [ 0.7381], quantization loss [ 0.0567],  0.51 sec/batch.
2022-10-19 00:24:21,723 step [ 429], lr [0.0001500], embedding loss [ 0.7453], quantization loss [ 0.0614],  0.53 sec/batch.
2022-10-19 00:24:23,674 step [ 430], lr [0.0001500], embedding loss [ 0.7383], quantization loss [ 0.0629],  0.52 sec/batch.
2022-10-19 00:24:25,608 step [ 431], lr [0.0001500], embedding loss [ 0.7556], quantization loss [ 0.0611],  0.52 sec/batch.
2022-10-19 00:24:27,575 step [ 432], lr [0.0001500], embedding loss [ 0.7432], quantization loss [ 0.0616],  0.53 sec/batch.
2022-10-19 00:24:29,542 step [ 433], lr [0.0001500], embedding loss [ 0.7328], quantization loss [ 0.0638],  0.52 sec/batch.
2022-10-19 00:24:31,497 step [ 434], lr [0.0001500], embedding loss [ 0.7315], quantization loss [ 0.0568],  0.52 sec/batch.
2022-10-19 00:24:33,518 step [ 435], lr [0.0001500], embedding loss [ 0.7339], quantization loss [ 0.0586],  0.52 sec/batch.
2022-10-19 00:24:35,423 step [ 436], lr [0.0001500], embedding loss [ 0.7338], quantization loss [ 0.0672],  0.53 sec/batch.
2022-10-19 00:24:37,382 step [ 437], lr [0.0001500], embedding loss [ 0.7407], quantization loss [ 0.0595],  0.53 sec/batch.
2022-10-19 00:24:39,289 step [ 438], lr [0.0001500], embedding loss [ 0.7458], quantization loss [ 0.0590],  0.52 sec/batch.
2022-10-19 00:24:41,258 step [ 439], lr [0.0001500], embedding loss [ 0.7508], quantization loss [ 0.0629],  0.53 sec/batch.
2022-10-19 00:24:43,218 step [ 440], lr [0.0001500], embedding loss [ 0.7389], quantization loss [ 0.0632],  0.53 sec/batch.
2022-10-19 00:24:45,220 step [ 441], lr [0.0001500], embedding loss [ 0.7431], quantization loss [ 0.0564],  0.53 sec/batch.
2022-10-19 00:24:47,190 step [ 442], lr [0.0001500], embedding loss [ 0.7477], quantization loss [ 0.0562],  0.52 sec/batch.
2022-10-19 00:24:49,122 step [ 443], lr [0.0001500], embedding loss [ 0.7271], quantization loss [ 0.0555],  0.51 sec/batch.
2022-10-19 00:24:51,085 step [ 444], lr [0.0001500], embedding loss [ 0.7390], quantization loss [ 0.0523],  0.52 sec/batch.
2022-10-19 00:24:53,004 step [ 445], lr [0.0001500], embedding loss [ 0.7364], quantization loss [ 0.0607],  0.51 sec/batch.
2022-10-19 00:24:54,950 step [ 446], lr [0.0001500], embedding loss [ 0.7398], quantization loss [ 0.0582],  0.52 sec/batch.
2022-10-19 00:24:56,921 step [ 447], lr [0.0001500], embedding loss [ 0.7448], quantization loss [ 0.0591],  0.53 sec/batch.
2022-10-19 00:24:58,833 step [ 448], lr [0.0001500], embedding loss [ 0.7324], quantization loss [ 0.0608],  0.52 sec/batch.
2022-10-19 00:25:00,777 step [ 449], lr [0.0001500], embedding loss [ 0.7380], quantization loss [ 0.0516],  0.51 sec/batch.
2022-10-19 00:25:02,714 step [ 450], lr [0.0001500], embedding loss [ 0.7372], quantization loss [ 0.0564],  0.52 sec/batch.
2022-10-19 00:25:04,639 step [ 451], lr [0.0001500], embedding loss [ 0.7450], quantization loss [ 0.0537],  0.52 sec/batch.
2022-10-19 00:25:06,603 step [ 452], lr [0.0001500], embedding loss [ 0.7461], quantization loss [ 0.0526],  0.53 sec/batch.
2022-10-19 00:25:08,614 step [ 453], lr [0.0001500], embedding loss [ 0.7452], quantization loss [ 0.0585],  0.53 sec/batch.
2022-10-19 00:25:10,604 step [ 454], lr [0.0001500], embedding loss [ 0.7441], quantization loss [ 0.0537],  0.52 sec/batch.
2022-10-19 00:25:12,569 step [ 455], lr [0.0001500], embedding loss [ 0.7426], quantization loss [ 0.0553],  0.52 sec/batch.
2022-10-19 00:25:14,540 step [ 456], lr [0.0001500], embedding loss [ 0.7384], quantization loss [ 0.0546],  0.52 sec/batch.
2022-10-19 00:25:16,556 step [ 457], lr [0.0001500], embedding loss [ 0.7436], quantization loss [ 0.0514],  0.53 sec/batch.
2022-10-19 00:25:18,536 step [ 458], lr [0.0001500], embedding loss [ 0.7449], quantization loss [ 0.0544],  0.52 sec/batch.
2022-10-19 00:25:20,479 step [ 459], lr [0.0001500], embedding loss [ 0.7386], quantization loss [ 0.0579],  0.52 sec/batch.
2022-10-19 00:25:22,468 step [ 460], lr [0.0001500], embedding loss [ 0.7557], quantization loss [ 0.0591],  0.53 sec/batch.
2022-10-19 00:25:24,488 step [ 461], lr [0.0001500], embedding loss [ 0.7476], quantization loss [ 0.0553],  0.54 sec/batch.
2022-10-19 00:25:26,425 step [ 462], lr [0.0001500], embedding loss [ 0.7509], quantization loss [ 0.0552],  0.51 sec/batch.
2022-10-19 00:25:28,343 step [ 463], lr [0.0001500], embedding loss [ 0.7370], quantization loss [ 0.0570],  0.51 sec/batch.
2022-10-19 00:25:30,332 step [ 464], lr [0.0001500], embedding loss [ 0.7462], quantization loss [ 0.0538],  0.52 sec/batch.
2022-10-19 00:25:32,350 step [ 465], lr [0.0001500], embedding loss [ 0.7395], quantization loss [ 0.0526],  0.53 sec/batch.
2022-10-19 00:25:34,310 step [ 466], lr [0.0001500], embedding loss [ 0.7412], quantization loss [ 0.0506],  0.52 sec/batch.
2022-10-19 00:25:36,342 step [ 467], lr [0.0001500], embedding loss [ 0.7395], quantization loss [ 0.0552],  0.53 sec/batch.
2022-10-19 00:25:38,373 step [ 468], lr [0.0001500], embedding loss [ 0.7364], quantization loss [ 0.0534],  0.55 sec/batch.
2022-10-19 00:25:40,404 step [ 469], lr [0.0001500], embedding loss [ 0.7323], quantization loss [ 0.0581],  0.53 sec/batch.
2022-10-19 00:25:42,407 step [ 470], lr [0.0001500], embedding loss [ 0.7294], quantization loss [ 0.0536],  0.52 sec/batch.
2022-10-19 00:25:44,372 step [ 471], lr [0.0001500], embedding loss [ 0.7369], quantization loss [ 0.0553],  0.53 sec/batch.
2022-10-19 00:25:46,317 step [ 472], lr [0.0001500], embedding loss [ 0.7453], quantization loss [ 0.0614],  0.50 sec/batch.
2022-10-19 00:25:48,290 step [ 473], lr [0.0001500], embedding loss [ 0.7429], quantization loss [ 0.0578],  0.53 sec/batch.
2022-10-19 00:25:50,316 step [ 474], lr [0.0001500], embedding loss [ 0.7336], quantization loss [ 0.0565],  0.52 sec/batch.
2022-10-19 00:25:52,305 step [ 475], lr [0.0001500], embedding loss [ 0.7439], quantization loss [ 0.0634],  0.53 sec/batch.
2022-10-19 00:25:54,294 step [ 476], lr [0.0001500], embedding loss [ 0.7294], quantization loss [ 0.0527],  0.53 sec/batch.
2022-10-19 00:25:56,275 step [ 477], lr [0.0001500], embedding loss [ 0.7371], quantization loss [ 0.0505],  0.53 sec/batch.
2022-10-19 00:25:58,256 step [ 478], lr [0.0001500], embedding loss [ 0.7247], quantization loss [ 0.0573],  0.53 sec/batch.
2022-10-19 00:26:00,218 step [ 479], lr [0.0001500], embedding loss [ 0.7433], quantization loss [ 0.0543],  0.51 sec/batch.
2022-10-19 00:26:02,181 step [ 480], lr [0.0001500], embedding loss [ 0.7490], quantization loss [ 0.0524],  0.53 sec/batch.
2022-10-19 00:26:04,166 step [ 481], lr [0.0001500], embedding loss [ 0.7428], quantization loss [ 0.0560],  0.53 sec/batch.
2022-10-19 00:26:04,166 update codes and centers iter(1/1).
2022-10-19 00:26:05,763 number of update_code wrong: 0.
2022-10-19 00:26:08,815 non zero codewords: 512.
2022-10-19 00:26:08,815 finish center update, duration: 4.65 sec.
2022-10-19 00:26:10,698 step [ 482], lr [0.0001500], embedding loss [ 0.7459], quantization loss [ 0.0544],  0.55 sec/batch.
2022-10-19 00:26:12,682 step [ 483], lr [0.0001500], embedding loss [ 0.7436], quantization loss [ 0.0470],  0.54 sec/batch.
2022-10-19 00:26:14,731 step [ 484], lr [0.0001500], embedding loss [ 0.7472], quantization loss [ 0.0511],  0.56 sec/batch.
2022-10-19 00:26:16,813 step [ 485], lr [0.0001500], embedding loss [ 0.7493], quantization loss [ 0.0643],  0.56 sec/batch.
2022-10-19 00:26:18,823 step [ 486], lr [0.0001500], embedding loss [ 0.7385], quantization loss [ 0.0578],  0.55 sec/batch.
2022-10-19 00:26:20,884 step [ 487], lr [0.0001500], embedding loss [ 0.7440], quantization loss [ 0.0612],  0.55 sec/batch.
2022-10-19 00:26:22,886 step [ 488], lr [0.0001500], embedding loss [ 0.7436], quantization loss [ 0.0609],  0.54 sec/batch.
2022-10-19 00:26:24,926 step [ 489], lr [0.0001500], embedding loss [ 0.7393], quantization loss [ 0.0584],  0.56 sec/batch.
2022-10-19 00:26:27,021 step [ 490], lr [0.0001500], embedding loss [ 0.7406], quantization loss [ 0.0543],  0.56 sec/batch.
2022-10-19 00:26:29,032 step [ 491], lr [0.0001500], embedding loss [ 0.7460], quantization loss [ 0.0564],  0.52 sec/batch.
2022-10-19 00:26:30,911 step [ 492], lr [0.0001500], embedding loss [ 0.7381], quantization loss [ 0.0515],  0.50 sec/batch.
2022-10-19 00:26:32,909 step [ 493], lr [0.0001500], embedding loss [ 0.7316], quantization loss [ 0.0556],  0.50 sec/batch.
2022-10-19 00:26:34,809 step [ 494], lr [0.0001500], embedding loss [ 0.7374], quantization loss [ 0.0540],  0.49 sec/batch.
2022-10-19 00:26:36,772 step [ 495], lr [0.0001500], embedding loss [ 0.7376], quantization loss [ 0.0629],  0.53 sec/batch.
2022-10-19 00:26:38,760 step [ 496], lr [0.0001500], embedding loss [ 0.7352], quantization loss [ 0.0563],  0.52 sec/batch.
2022-10-19 00:26:40,741 step [ 497], lr [0.0001500], embedding loss [ 0.7363], quantization loss [ 0.0560],  0.53 sec/batch.
2022-10-19 00:26:42,742 step [ 498], lr [0.0001500], embedding loss [ 0.7310], quantization loss [ 0.0524],  0.53 sec/batch.
2022-10-19 00:26:44,742 step [ 499], lr [0.0001500], embedding loss [ 0.7433], quantization loss [ 0.0556],  0.52 sec/batch.
2022-10-19 00:26:46,717 step [ 500], lr [0.0001500], embedding loss [ 0.7422], quantization loss [ 0.0542],  0.52 sec/batch.
2022-10-19 00:26:48,712 step [ 501], lr [0.0001500], embedding loss [ 0.7437], quantization loss [ 0.0575],  0.53 sec/batch.
2022-10-19 00:26:50,727 step [ 502], lr [0.0001500], embedding loss [ 0.7337], quantization loss [ 0.0520],  0.52 sec/batch.
2022-10-19 00:26:52,701 step [ 503], lr [0.0001500], embedding loss [ 0.7487], quantization loss [ 0.0498],  0.53 sec/batch.
2022-10-19 00:26:54,656 step [ 504], lr [0.0001500], embedding loss [ 0.7311], quantization loss [ 0.0528],  0.52 sec/batch.
2022-10-19 00:26:56,623 step [ 505], lr [0.0001500], embedding loss [ 0.7389], quantization loss [ 0.0606],  0.53 sec/batch.
2022-10-19 00:26:58,654 step [ 506], lr [0.0001500], embedding loss [ 0.7315], quantization loss [ 0.0567],  0.52 sec/batch.
2022-10-19 00:27:00,740 step [ 507], lr [0.0001500], embedding loss [ 0.7297], quantization loss [ 0.0529],  0.53 sec/batch.
2022-10-19 00:27:02,716 step [ 508], lr [0.0001500], embedding loss [ 0.7398], quantization loss [ 0.0500],  0.52 sec/batch.
2022-10-19 00:27:04,738 step [ 509], lr [0.0001500], embedding loss [ 0.7335], quantization loss [ 0.0597],  0.53 sec/batch.
2022-10-19 00:27:06,703 step [ 510], lr [0.0001500], embedding loss [ 0.7342], quantization loss [ 0.0607],  0.52 sec/batch.
2022-10-19 00:27:08,738 step [ 511], lr [0.0001500], embedding loss [ 0.7429], quantization loss [ 0.0582],  0.53 sec/batch.
2022-10-19 00:27:10,718 step [ 512], lr [0.0001500], embedding loss [ 0.7337], quantization loss [ 0.0503],  0.53 sec/batch.
2022-10-19 00:27:12,719 step [ 513], lr [0.0001500], embedding loss [ 0.7408], quantization loss [ 0.0521],  0.53 sec/batch.
2022-10-19 00:27:14,693 step [ 514], lr [0.0001500], embedding loss [ 0.7317], quantization loss [ 0.0530],  0.52 sec/batch.
2022-10-19 00:27:16,690 step [ 515], lr [0.0001500], embedding loss [ 0.7473], quantization loss [ 0.0594],  0.53 sec/batch.
2022-10-19 00:27:18,699 step [ 516], lr [0.0001500], embedding loss [ 0.7375], quantization loss [ 0.0608],  0.53 sec/batch.
2022-10-19 00:27:20,688 step [ 517], lr [0.0001500], embedding loss [ 0.7338], quantization loss [ 0.0542],  0.53 sec/batch.
2022-10-19 00:27:22,663 step [ 518], lr [0.0001500], embedding loss [ 0.7399], quantization loss [ 0.0557],  0.52 sec/batch.
2022-10-19 00:27:24,720 step [ 519], lr [0.0001500], embedding loss [ 0.7522], quantization loss [ 0.0573],  0.52 sec/batch.
2022-10-19 00:27:26,765 step [ 520], lr [0.0001500], embedding loss [ 0.7422], quantization loss [ 0.0567],  0.52 sec/batch.
2022-10-19 00:27:28,793 step [ 521], lr [0.0001500], embedding loss [ 0.7310], quantization loss [ 0.0538],  0.52 sec/batch.
2022-10-19 00:27:30,840 step [ 522], lr [0.0001500], embedding loss [ 0.7499], quantization loss [ 0.0520],  0.52 sec/batch.
2022-10-19 00:27:32,868 step [ 523], lr [0.0001500], embedding loss [ 0.7471], quantization loss [ 0.0493],  0.53 sec/batch.
2022-10-19 00:27:34,858 step [ 524], lr [0.0001500], embedding loss [ 0.7365], quantization loss [ 0.0519],  0.52 sec/batch.
2022-10-19 00:27:36,836 step [ 525], lr [0.0001500], embedding loss [ 0.7251], quantization loss [ 0.0495],  0.53 sec/batch.
2022-10-19 00:27:38,805 step [ 526], lr [0.0001500], embedding loss [ 0.7432], quantization loss [ 0.0509],  0.52 sec/batch.
2022-10-19 00:27:40,835 step [ 527], lr [0.0001500], embedding loss [ 0.7450], quantization loss [ 0.0525],  0.53 sec/batch.
2022-10-19 00:27:42,831 step [ 528], lr [0.0001500], embedding loss [ 0.7457], quantization loss [ 0.0460],  0.52 sec/batch.
2022-10-19 00:27:44,811 step [ 529], lr [0.0001500], embedding loss [ 0.7402], quantization loss [ 0.0534],  0.53 sec/batch.
2022-10-19 00:27:46,781 step [ 530], lr [0.0001500], embedding loss [ 0.7406], quantization loss [ 0.0470],  0.52 sec/batch.
2022-10-19 00:27:48,858 step [ 531], lr [0.0001500], embedding loss [ 0.7444], quantization loss [ 0.0534],  0.52 sec/batch.
2022-10-19 00:27:50,824 step [ 532], lr [0.0001500], embedding loss [ 0.7288], quantization loss [ 0.0455],  0.53 sec/batch.
2022-10-19 00:27:52,857 step [ 533], lr [0.0001500], embedding loss [ 0.7444], quantization loss [ 0.0572],  0.53 sec/batch.
2022-10-19 00:27:54,841 step [ 534], lr [0.0001500], embedding loss [ 0.7351], quantization loss [ 0.0542],  0.53 sec/batch.
2022-10-19 00:27:56,883 step [ 535], lr [0.0001500], embedding loss [ 0.7336], quantization loss [ 0.0517],  0.53 sec/batch.
2022-10-19 00:27:58,864 step [ 536], lr [0.0001500], embedding loss [ 0.7237], quantization loss [ 0.0521],  0.53 sec/batch.
2022-10-19 00:28:00,912 step [ 537], lr [0.0001500], embedding loss [ 0.7442], quantization loss [ 0.0536],  0.53 sec/batch.
2022-10-19 00:28:02,929 step [ 538], lr [0.0001500], embedding loss [ 0.7395], quantization loss [ 0.0548],  0.53 sec/batch.
2022-10-19 00:28:04,973 step [ 539], lr [0.0001500], embedding loss [ 0.7362], quantization loss [ 0.0515],  0.53 sec/batch.
2022-10-19 00:28:06,988 step [ 540], lr [0.0001500], embedding loss [ 0.7273], quantization loss [ 0.0553],  0.53 sec/batch.
2022-10-19 00:28:09,031 step [ 541], lr [0.0001500], embedding loss [ 0.7518], quantization loss [ 0.0457],  0.53 sec/batch.
2022-10-19 00:28:11,024 step [ 542], lr [0.0001500], embedding loss [ 0.7391], quantization loss [ 0.0458],  0.52 sec/batch.
2022-10-19 00:28:13,057 step [ 543], lr [0.0001500], embedding loss [ 0.7359], quantization loss [ 0.0455],  0.53 sec/batch.
2022-10-19 00:28:15,044 step [ 544], lr [0.0001500], embedding loss [ 0.7408], quantization loss [ 0.0495],  0.53 sec/batch.
2022-10-19 00:28:17,076 step [ 545], lr [0.0001500], embedding loss [ 0.7337], quantization loss [ 0.0529],  0.53 sec/batch.
2022-10-19 00:28:19,111 step [ 546], lr [0.0001500], embedding loss [ 0.7323], quantization loss [ 0.0479],  0.54 sec/batch.
2022-10-19 00:28:21,141 step [ 547], lr [0.0001500], embedding loss [ 0.7413], quantization loss [ 0.0581],  0.53 sec/batch.
2022-10-19 00:28:23,131 step [ 548], lr [0.0001500], embedding loss [ 0.7215], quantization loss [ 0.0533],  0.52 sec/batch.
2022-10-19 00:28:25,165 step [ 549], lr [0.0001500], embedding loss [ 0.7238], quantization loss [ 0.0429],  0.53 sec/batch.
2022-10-19 00:28:27,148 step [ 550], lr [0.0001500], embedding loss [ 0.7421], quantization loss [ 0.0526],  0.53 sec/batch.
2022-10-19 00:28:29,185 step [ 551], lr [0.0001500], embedding loss [ 0.7344], quantization loss [ 0.0523],  0.53 sec/batch.
2022-10-19 00:28:31,172 step [ 552], lr [0.0001500], embedding loss [ 0.7339], quantization loss [ 0.0543],  0.52 sec/batch.
2022-10-19 00:28:33,211 step [ 553], lr [0.0001500], embedding loss [ 0.7424], quantization loss [ 0.0554],  0.53 sec/batch.
2022-10-19 00:28:35,203 step [ 554], lr [0.0001500], embedding loss [ 0.7455], quantization loss [ 0.0491],  0.52 sec/batch.
2022-10-19 00:28:37,154 step [ 555], lr [0.0001500], embedding loss [ 0.7369], quantization loss [ 0.0501],  0.51 sec/batch.
2022-10-19 00:28:39,128 step [ 556], lr [0.0001500], embedding loss [ 0.7492], quantization loss [ 0.0557],  0.52 sec/batch.
2022-10-19 00:28:41,195 step [ 557], lr [0.0001500], embedding loss [ 0.7367], quantization loss [ 0.0505],  0.53 sec/batch.
2022-10-19 00:28:43,186 step [ 558], lr [0.0001500], embedding loss [ 0.7491], quantization loss [ 0.0502],  0.52 sec/batch.
2022-10-19 00:28:45,173 step [ 559], lr [0.0001500], embedding loss [ 0.7433], quantization loss [ 0.0590],  0.53 sec/batch.
2022-10-19 00:28:47,144 step [ 560], lr [0.0001500], embedding loss [ 0.7330], quantization loss [ 0.0456],  0.52 sec/batch.
2022-10-19 00:28:49,107 step [ 561], lr [0.0001500], embedding loss [ 0.7319], quantization loss [ 0.0474],  0.51 sec/batch.
2022-10-19 00:28:49,108 update codes and centers iter(1/1).
2022-10-19 00:28:50,702 number of update_code wrong: 0.
2022-10-19 00:28:53,819 non zero codewords: 512.
2022-10-19 00:28:53,819 finish center update, duration: 4.71 sec.
2022-10-19 00:28:55,696 step [ 562], lr [0.0001500], embedding loss [ 0.7477], quantization loss [ 0.0595],  0.53 sec/batch.
2022-10-19 00:28:57,688 step [ 563], lr [0.0001500], embedding loss [ 0.7469], quantization loss [ 0.0554],  0.54 sec/batch.
2022-10-19 00:28:59,676 step [ 564], lr [0.0001500], embedding loss [ 0.7365], quantization loss [ 0.0516],  0.52 sec/batch.
2022-10-19 00:29:01,742 step [ 565], lr [0.0001500], embedding loss [ 0.7295], quantization loss [ 0.0501],  0.53 sec/batch.
2022-10-19 00:29:03,745 step [ 566], lr [0.0001500], embedding loss [ 0.7430], quantization loss [ 0.0496],  0.51 sec/batch.
2022-10-19 00:29:05,757 step [ 567], lr [0.0001500], embedding loss [ 0.7484], quantization loss [ 0.0498],  0.53 sec/batch.
2022-10-19 00:29:07,768 step [ 568], lr [0.0001500], embedding loss [ 0.7279], quantization loss [ 0.0476],  0.55 sec/batch.
2022-10-19 00:29:09,764 step [ 569], lr [0.0001500], embedding loss [ 0.7265], quantization loss [ 0.0525],  0.52 sec/batch.
2022-10-19 00:29:11,822 step [ 570], lr [0.0001500], embedding loss [ 0.7436], quantization loss [ 0.0673],  0.53 sec/batch.
2022-10-19 00:29:13,879 step [ 571], lr [0.0001500], embedding loss [ 0.7410], quantization loss [ 0.0513],  0.53 sec/batch.
2022-10-19 00:29:15,948 step [ 572], lr [0.0001500], embedding loss [ 0.7326], quantization loss [ 0.0507],  0.53 sec/batch.
2022-10-19 00:29:17,960 step [ 573], lr [0.0001500], embedding loss [ 0.7398], quantization loss [ 0.0508],  0.53 sec/batch.
2022-10-19 00:29:19,996 step [ 574], lr [0.0001500], embedding loss [ 0.7401], quantization loss [ 0.0575],  0.53 sec/batch.
2022-10-19 00:29:22,002 step [ 575], lr [0.0001500], embedding loss [ 0.7423], quantization loss [ 0.0583],  0.53 sec/batch.
2022-10-19 00:29:24,003 step [ 576], lr [0.0001500], embedding loss [ 0.7356], quantization loss [ 0.0523],  0.52 sec/batch.
2022-10-19 00:29:26,034 step [ 577], lr [0.0001500], embedding loss [ 0.7386], quantization loss [ 0.0566],  0.54 sec/batch.
2022-10-19 00:29:28,083 step [ 578], lr [0.0001500], embedding loss [ 0.7346], quantization loss [ 0.0499],  0.54 sec/batch.
2022-10-19 00:29:30,100 step [ 579], lr [0.0001500], embedding loss [ 0.7396], quantization loss [ 0.0487],  0.51 sec/batch.
2022-10-19 00:29:32,097 step [ 580], lr [0.0001500], embedding loss [ 0.7382], quantization loss [ 0.0585],  0.53 sec/batch.
2022-10-19 00:29:34,151 step [ 581], lr [0.0001500], embedding loss [ 0.7313], quantization loss [ 0.0523],  0.53 sec/batch.
2022-10-19 00:29:36,214 step [ 582], lr [0.0001500], embedding loss [ 0.7418], quantization loss [ 0.0474],  0.53 sec/batch.
2022-10-19 00:29:38,212 step [ 583], lr [0.0001500], embedding loss [ 0.7408], quantization loss [ 0.0456],  0.53 sec/batch.
2022-10-19 00:29:40,263 step [ 584], lr [0.0001500], embedding loss [ 0.7389], quantization loss [ 0.0483],  0.53 sec/batch.
2022-10-19 00:29:42,303 step [ 585], lr [0.0001500], embedding loss [ 0.7332], quantization loss [ 0.0460],  0.53 sec/batch.
2022-10-19 00:29:44,338 step [ 586], lr [0.0001500], embedding loss [ 0.7349], quantization loss [ 0.0543],  0.52 sec/batch.
2022-10-19 00:29:46,437 step [ 587], lr [0.0001500], embedding loss [ 0.7361], quantization loss [ 0.0523],  0.53 sec/batch.
2022-10-19 00:29:48,452 step [ 588], lr [0.0001500], embedding loss [ 0.7283], quantization loss [ 0.0526],  0.53 sec/batch.
2022-10-19 00:29:50,462 step [ 589], lr [0.0001500], embedding loss [ 0.7434], quantization loss [ 0.0527],  0.53 sec/batch.
2022-10-19 00:29:52,458 step [ 590], lr [0.0001500], embedding loss [ 0.7405], quantization loss [ 0.0453],  0.51 sec/batch.
2022-10-19 00:29:54,465 step [ 591], lr [0.0001500], embedding loss [ 0.7365], quantization loss [ 0.0482],  0.52 sec/batch.
2022-10-19 00:29:56,483 step [ 592], lr [0.0001500], embedding loss [ 0.7396], quantization loss [ 0.0490],  0.52 sec/batch.
2022-10-19 00:29:58,462 step [ 593], lr [0.0001500], embedding loss [ 0.7257], quantization loss [ 0.0458],  0.51 sec/batch.
2022-10-19 00:30:00,472 step [ 594], lr [0.0001500], embedding loss [ 0.7450], quantization loss [ 0.0531],  0.53 sec/batch.
2022-10-19 00:30:02,496 step [ 595], lr [0.0001500], embedding loss [ 0.7402], quantization loss [ 0.0458],  0.53 sec/batch.
2022-10-19 00:30:04,506 step [ 596], lr [0.0001500], embedding loss [ 0.7272], quantization loss [ 0.0529],  0.53 sec/batch.
2022-10-19 00:30:06,481 step [ 597], lr [0.0001500], embedding loss [ 0.7389], quantization loss [ 0.0474],  0.53 sec/batch.
2022-10-19 00:30:08,478 step [ 598], lr [0.0001500], embedding loss [ 0.7273], quantization loss [ 0.0547],  0.52 sec/batch.
2022-10-19 00:30:10,486 step [ 599], lr [0.0001500], embedding loss [ 0.7261], quantization loss [ 0.0496],  0.52 sec/batch.
2022-10-19 00:30:12,475 step [ 600], lr [0.0001500], embedding loss [ 0.7379], quantization loss [ 0.0511],  0.51 sec/batch.
2022-10-19 00:30:14,450 step [ 601], lr [0.0000750], embedding loss [ 0.7424], quantization loss [ 0.0547],  0.52 sec/batch.
2022-10-19 00:30:16,423 step [ 602], lr [0.0000750], embedding loss [ 0.7460], quantization loss [ 0.0471],  0.51 sec/batch.
2022-10-19 00:30:18,466 step [ 603], lr [0.0000750], embedding loss [ 0.7306], quantization loss [ 0.0486],  0.51 sec/batch.
2022-10-19 00:30:20,421 step [ 604], lr [0.0000750], embedding loss [ 0.7383], quantization loss [ 0.0402],  0.51 sec/batch.
2022-10-19 00:30:22,421 step [ 605], lr [0.0000750], embedding loss [ 0.7310], quantization loss [ 0.0444],  0.52 sec/batch.
2022-10-19 00:30:24,430 step [ 606], lr [0.0000750], embedding loss [ 0.7249], quantization loss [ 0.0471],  0.53 sec/batch.
2022-10-19 00:30:26,479 step [ 607], lr [0.0000750], embedding loss [ 0.7244], quantization loss [ 0.0428],  0.53 sec/batch.
2022-10-19 00:30:28,488 step [ 608], lr [0.0000750], embedding loss [ 0.7388], quantization loss [ 0.0454],  0.53 sec/batch.
2022-10-19 00:30:30,531 step [ 609], lr [0.0000750], embedding loss [ 0.7381], quantization loss [ 0.0455],  0.52 sec/batch.
2022-10-19 00:30:32,529 step [ 610], lr [0.0000750], embedding loss [ 0.7315], quantization loss [ 0.0486],  0.52 sec/batch.
2022-10-19 00:30:34,552 step [ 611], lr [0.0000750], embedding loss [ 0.7263], quantization loss [ 0.0440],  0.53 sec/batch.
2022-10-19 00:30:36,560 step [ 612], lr [0.0000750], embedding loss [ 0.7290], quantization loss [ 0.0427],  0.53 sec/batch.
2022-10-19 00:30:38,555 step [ 613], lr [0.0000750], embedding loss [ 0.7276], quantization loss [ 0.0451],  0.52 sec/batch.
2022-10-19 00:30:40,567 step [ 614], lr [0.0000750], embedding loss [ 0.7368], quantization loss [ 0.0439],  0.53 sec/batch.
2022-10-19 00:30:42,579 step [ 615], lr [0.0000750], embedding loss [ 0.7359], quantization loss [ 0.0456],  0.53 sec/batch.
2022-10-19 00:30:44,616 step [ 616], lr [0.0000750], embedding loss [ 0.7382], quantization loss [ 0.0474],  0.54 sec/batch.
2022-10-19 00:30:46,647 step [ 617], lr [0.0000750], embedding loss [ 0.7536], quantization loss [ 0.0531],  0.53 sec/batch.
2022-10-19 00:30:48,642 step [ 618], lr [0.0000750], embedding loss [ 0.7335], quantization loss [ 0.0483],  0.53 sec/batch.
2022-10-19 00:30:50,674 step [ 619], lr [0.0000750], embedding loss [ 0.7438], quantization loss [ 0.0465],  0.53 sec/batch.
2022-10-19 00:30:52,685 step [ 620], lr [0.0000750], embedding loss [ 0.7285], quantization loss [ 0.0439],  0.51 sec/batch.
2022-10-19 00:30:54,685 step [ 621], lr [0.0000750], embedding loss [ 0.7335], quantization loss [ 0.0470],  0.51 sec/batch.
2022-10-19 00:30:56,712 step [ 622], lr [0.0000750], embedding loss [ 0.7293], quantization loss [ 0.0471],  0.55 sec/batch.
2022-10-19 00:30:58,769 step [ 623], lr [0.0000750], embedding loss [ 0.7394], quantization loss [ 0.0383],  0.53 sec/batch.
2022-10-19 00:31:00,832 step [ 624], lr [0.0000750], embedding loss [ 0.7391], quantization loss [ 0.0470],  0.51 sec/batch.
2022-10-19 00:31:02,833 step [ 625], lr [0.0000750], embedding loss [ 0.7342], quantization loss [ 0.0480],  0.52 sec/batch.
2022-10-19 00:31:04,831 step [ 626], lr [0.0000750], embedding loss [ 0.7345], quantization loss [ 0.0457],  0.53 sec/batch.
2022-10-19 00:31:06,830 step [ 627], lr [0.0000750], embedding loss [ 0.7403], quantization loss [ 0.0450],  0.52 sec/batch.
2022-10-19 00:31:08,925 step [ 628], lr [0.0000750], embedding loss [ 0.7407], quantization loss [ 0.0535],  0.53 sec/batch.
2022-10-19 00:31:10,929 step [ 629], lr [0.0000750], embedding loss [ 0.7331], quantization loss [ 0.0451],  0.52 sec/batch.
2022-10-19 00:31:12,937 step [ 630], lr [0.0000750], embedding loss [ 0.7306], quantization loss [ 0.0480],  0.52 sec/batch.
2022-10-19 00:31:14,935 step [ 631], lr [0.0000750], embedding loss [ 0.7381], quantization loss [ 0.0442],  0.52 sec/batch.
2022-10-19 00:31:16,895 step [ 632], lr [0.0000750], embedding loss [ 0.7451], quantization loss [ 0.0472],  0.52 sec/batch.
2022-10-19 00:31:18,913 step [ 633], lr [0.0000750], embedding loss [ 0.7370], quantization loss [ 0.0421],  0.52 sec/batch.
2022-10-19 00:31:20,982 step [ 634], lr [0.0000750], embedding loss [ 0.7465], quantization loss [ 0.0484],  0.53 sec/batch.
2022-10-19 00:31:22,986 step [ 635], lr [0.0000750], embedding loss [ 0.7395], quantization loss [ 0.0420],  0.52 sec/batch.
2022-10-19 00:31:25,067 step [ 636], lr [0.0000750], embedding loss [ 0.7280], quantization loss [ 0.0481],  0.53 sec/batch.
2022-10-19 00:31:27,059 step [ 637], lr [0.0000750], embedding loss [ 0.7501], quantization loss [ 0.0555],  0.52 sec/batch.
2022-10-19 00:31:29,102 step [ 638], lr [0.0000750], embedding loss [ 0.7393], quantization loss [ 0.0468],  0.53 sec/batch.
2022-10-19 00:31:31,137 step [ 639], lr [0.0000750], embedding loss [ 0.7340], quantization loss [ 0.0425],  0.53 sec/batch.
2022-10-19 00:31:33,213 step [ 640], lr [0.0000750], embedding loss [ 0.7373], quantization loss [ 0.0453],  0.53 sec/batch.
2022-10-19 00:31:35,240 step [ 641], lr [0.0000750], embedding loss [ 0.7391], quantization loss [ 0.0438],  0.53 sec/batch.
2022-10-19 00:31:35,241 update codes and centers iter(1/1).
2022-10-19 00:31:36,864 number of update_code wrong: 0.
2022-10-19 00:31:39,946 non zero codewords: 512.
2022-10-19 00:31:39,946 finish center update, duration: 4.71 sec.
2022-10-19 00:31:41,832 step [ 642], lr [0.0000750], embedding loss [ 0.7301], quantization loss [ 0.0446],  0.53 sec/batch.
2022-10-19 00:31:43,796 step [ 643], lr [0.0000750], embedding loss [ 0.7279], quantization loss [ 0.0423],  0.52 sec/batch.
2022-10-19 00:31:45,859 step [ 644], lr [0.0000750], embedding loss [ 0.7217], quantization loss [ 0.0515],  0.53 sec/batch.
2022-10-19 00:31:47,933 step [ 645], lr [0.0000750], embedding loss [ 0.7530], quantization loss [ 0.0467],  0.53 sec/batch.
2022-10-19 00:31:50,001 step [ 646], lr [0.0000750], embedding loss [ 0.7382], quantization loss [ 0.0398],  0.52 sec/batch.
2022-10-19 00:31:52,082 step [ 647], lr [0.0000750], embedding loss [ 0.7533], quantization loss [ 0.0447],  0.53 sec/batch.
2022-10-19 00:31:54,128 step [ 648], lr [0.0000750], embedding loss [ 0.7404], quantization loss [ 0.0475],  0.52 sec/batch.
2022-10-19 00:31:56,169 step [ 649], lr [0.0000750], embedding loss [ 0.7286], quantization loss [ 0.0433],  0.53 sec/batch.
2022-10-19 00:31:58,215 step [ 650], lr [0.0000750], embedding loss [ 0.7435], quantization loss [ 0.0469],  0.52 sec/batch.
2022-10-19 00:32:00,240 step [ 651], lr [0.0000750], embedding loss [ 0.7334], quantization loss [ 0.0410],  0.53 sec/batch.
2022-10-19 00:32:02,239 step [ 652], lr [0.0000750], embedding loss [ 0.7388], quantization loss [ 0.0459],  0.52 sec/batch.
2022-10-19 00:32:04,295 step [ 653], lr [0.0000750], embedding loss [ 0.7271], quantization loss [ 0.0415],  0.53 sec/batch.
2022-10-19 00:32:06,302 step [ 654], lr [0.0000750], embedding loss [ 0.7318], quantization loss [ 0.0418],  0.52 sec/batch.
2022-10-19 00:32:08,310 step [ 655], lr [0.0000750], embedding loss [ 0.7382], quantization loss [ 0.0448],  0.53 sec/batch.
2022-10-19 00:32:10,326 step [ 656], lr [0.0000750], embedding loss [ 0.7308], quantization loss [ 0.0391],  0.53 sec/batch.
2022-10-19 00:32:12,333 step [ 657], lr [0.0000750], embedding loss [ 0.7291], quantization loss [ 0.0480],  0.53 sec/batch.
2022-10-19 00:32:14,359 step [ 658], lr [0.0000750], embedding loss [ 0.7309], quantization loss [ 0.0443],  0.53 sec/batch.
2022-10-19 00:32:16,389 step [ 659], lr [0.0000750], embedding loss [ 0.7221], quantization loss [ 0.0448],  0.51 sec/batch.
2022-10-19 00:32:18,418 step [ 660], lr [0.0000750], embedding loss [ 0.7298], quantization loss [ 0.0392],  0.52 sec/batch.
2022-10-19 00:32:20,411 step [ 661], lr [0.0000750], embedding loss [ 0.7428], quantization loss [ 0.0392],  0.52 sec/batch.
2022-10-19 00:32:22,394 step [ 662], lr [0.0000750], embedding loss [ 0.7417], quantization loss [ 0.0469],  0.52 sec/batch.
2022-10-19 00:32:24,416 step [ 663], lr [0.0000750], embedding loss [ 0.7313], quantization loss [ 0.0489],  0.53 sec/batch.
2022-10-19 00:32:26,424 step [ 664], lr [0.0000750], embedding loss [ 0.7309], quantization loss [ 0.0407],  0.53 sec/batch.
2022-10-19 00:32:28,443 step [ 665], lr [0.0000750], embedding loss [ 0.7453], quantization loss [ 0.0447],  0.53 sec/batch.
2022-10-19 00:32:30,449 step [ 666], lr [0.0000750], embedding loss [ 0.7281], quantization loss [ 0.0474],  0.52 sec/batch.
2022-10-19 00:32:32,490 step [ 667], lr [0.0000750], embedding loss [ 0.7343], quantization loss [ 0.0495],  0.54 sec/batch.
2022-10-19 00:32:34,467 step [ 668], lr [0.0000750], embedding loss [ 0.7316], quantization loss [ 0.0430],  0.52 sec/batch.
2022-10-19 00:32:36,495 step [ 669], lr [0.0000750], embedding loss [ 0.7329], quantization loss [ 0.0426],  0.53 sec/batch.
2022-10-19 00:32:38,509 step [ 670], lr [0.0000750], embedding loss [ 0.7351], quantization loss [ 0.0441],  0.52 sec/batch.
2022-10-19 00:32:40,533 step [ 671], lr [0.0000750], embedding loss [ 0.7369], quantization loss [ 0.0465],  0.52 sec/batch.
2022-10-19 00:32:42,558 step [ 672], lr [0.0000750], embedding loss [ 0.7381], quantization loss [ 0.0473],  0.53 sec/batch.
2022-10-19 00:32:44,597 step [ 673], lr [0.0000750], embedding loss [ 0.7339], quantization loss [ 0.0428],  0.53 sec/batch.
2022-10-19 00:32:46,646 step [ 674], lr [0.0000750], embedding loss [ 0.7459], quantization loss [ 0.0473],  0.54 sec/batch.
2022-10-19 00:32:48,652 step [ 675], lr [0.0000750], embedding loss [ 0.7384], quantization loss [ 0.0411],  0.53 sec/batch.
2022-10-19 00:32:50,694 step [ 676], lr [0.0000750], embedding loss [ 0.7237], quantization loss [ 0.0433],  0.54 sec/batch.
2022-10-19 00:32:52,709 step [ 677], lr [0.0000750], embedding loss [ 0.7276], quantization loss [ 0.0443],  0.53 sec/batch.
2022-10-19 00:32:54,707 step [ 678], lr [0.0000750], embedding loss [ 0.7378], quantization loss [ 0.0397],  0.52 sec/batch.
2022-10-19 00:32:56,740 step [ 679], lr [0.0000750], embedding loss [ 0.7383], quantization loss [ 0.0447],  0.53 sec/batch.
2022-10-19 00:32:58,801 step [ 680], lr [0.0000750], embedding loss [ 0.7361], quantization loss [ 0.0418],  0.53 sec/batch.
2022-10-19 00:33:00,829 step [ 681], lr [0.0000750], embedding loss [ 0.7283], quantization loss [ 0.0440],  0.53 sec/batch.
2022-10-19 00:33:02,849 step [ 682], lr [0.0000750], embedding loss [ 0.7410], quantization loss [ 0.0425],  0.53 sec/batch.
2022-10-19 00:33:04,871 step [ 683], lr [0.0000750], embedding loss [ 0.7415], quantization loss [ 0.0402],  0.53 sec/batch.
2022-10-19 00:33:06,892 step [ 684], lr [0.0000750], embedding loss [ 0.7156], quantization loss [ 0.0507],  0.52 sec/batch.
2022-10-19 00:33:08,911 step [ 685], lr [0.0000750], embedding loss [ 0.7283], quantization loss [ 0.0460],  0.53 sec/batch.
2022-10-19 00:33:10,931 step [ 686], lr [0.0000750], embedding loss [ 0.7408], quantization loss [ 0.0397],  0.52 sec/batch.
2022-10-19 00:33:12,952 step [ 687], lr [0.0000750], embedding loss [ 0.7285], quantization loss [ 0.0465],  0.52 sec/batch.
2022-10-19 00:33:14,977 step [ 688], lr [0.0000750], embedding loss [ 0.7421], quantization loss [ 0.0424],  0.53 sec/batch.
2022-10-19 00:33:17,014 step [ 689], lr [0.0000750], embedding loss [ 0.7348], quantization loss [ 0.0422],  0.53 sec/batch.
2022-10-19 00:33:19,021 step [ 690], lr [0.0000750], embedding loss [ 0.7236], quantization loss [ 0.0392],  0.53 sec/batch.
2022-10-19 00:33:21,044 step [ 691], lr [0.0000750], embedding loss [ 0.7382], quantization loss [ 0.0419],  0.52 sec/batch.
2022-10-19 00:33:23,065 step [ 692], lr [0.0000750], embedding loss [ 0.7461], quantization loss [ 0.0419],  0.51 sec/batch.
2022-10-19 00:33:25,195 step [ 693], lr [0.0000750], embedding loss [ 0.7427], quantization loss [ 0.0441],  0.56 sec/batch.
2022-10-19 00:33:27,278 step [ 694], lr [0.0000750], embedding loss [ 0.7324], quantization loss [ 0.0416],  0.53 sec/batch.
2022-10-19 00:33:29,350 step [ 695], lr [0.0000750], embedding loss [ 0.7278], quantization loss [ 0.0425],  0.53 sec/batch.
2022-10-19 00:33:31,381 step [ 696], lr [0.0000750], embedding loss [ 0.7346], quantization loss [ 0.0425],  0.53 sec/batch.
2022-10-19 00:33:33,468 step [ 697], lr [0.0000750], embedding loss [ 0.7297], quantization loss [ 0.0410],  0.53 sec/batch.
2022-10-19 00:33:35,469 step [ 698], lr [0.0000750], embedding loss [ 0.7432], quantization loss [ 0.0419],  0.52 sec/batch.
2022-10-19 00:33:37,571 step [ 699], lr [0.0000750], embedding loss [ 0.7283], quantization loss [ 0.0413],  0.52 sec/batch.
2022-10-19 00:33:39,642 step [ 700], lr [0.0000750], embedding loss [ 0.7350], quantization loss [ 0.0431],  0.53 sec/batch.
2022-10-19 00:33:41,753 step [ 701], lr [0.0000750], embedding loss [ 0.7348], quantization loss [ 0.0407],  0.54 sec/batch.
2022-10-19 00:33:43,767 step [ 702], lr [0.0000750], embedding loss [ 0.7389], quantization loss [ 0.0386],  0.53 sec/batch.
2022-10-19 00:33:45,837 step [ 703], lr [0.0000750], embedding loss [ 0.7430], quantization loss [ 0.0414],  0.53 sec/batch.
2022-10-19 00:33:47,893 step [ 704], lr [0.0000750], embedding loss [ 0.7369], quantization loss [ 0.0378],  0.53 sec/batch.
2022-10-19 00:33:49,940 step [ 705], lr [0.0000750], embedding loss [ 0.7245], quantization loss [ 0.0408],  0.52 sec/batch.
2022-10-19 00:33:51,995 step [ 706], lr [0.0000750], embedding loss [ 0.7272], quantization loss [ 0.0407],  0.53 sec/batch.
2022-10-19 00:33:54,034 step [ 707], lr [0.0000750], embedding loss [ 0.7342], quantization loss [ 0.0449],  0.53 sec/batch.
2022-10-19 00:33:56,071 step [ 708], lr [0.0000750], embedding loss [ 0.7349], quantization loss [ 0.0467],  0.52 sec/batch.
2022-10-19 00:33:58,124 step [ 709], lr [0.0000750], embedding loss [ 0.7366], quantization loss [ 0.0430],  0.52 sec/batch.
2022-10-19 00:34:00,188 step [ 710], lr [0.0000750], embedding loss [ 0.7400], quantization loss [ 0.0459],  0.53 sec/batch.
2022-10-19 00:34:02,185 step [ 711], lr [0.0000750], embedding loss [ 0.7384], quantization loss [ 0.0377],  0.52 sec/batch.
2022-10-19 00:34:04,193 step [ 712], lr [0.0000750], embedding loss [ 0.7377], quantization loss [ 0.0418],  0.52 sec/batch.
2022-10-19 00:34:06,197 step [ 713], lr [0.0000750], embedding loss [ 0.7329], quantization loss [ 0.0382],  0.53 sec/batch.
2022-10-19 00:34:08,302 step [ 714], lr [0.0000750], embedding loss [ 0.7361], quantization loss [ 0.0468],  0.53 sec/batch.
2022-10-19 00:34:10,378 step [ 715], lr [0.0000750], embedding loss [ 0.7286], quantization loss [ 0.0410],  0.53 sec/batch.
2022-10-19 00:34:12,466 step [ 716], lr [0.0000750], embedding loss [ 0.7344], quantization loss [ 0.0442],  0.53 sec/batch.
2022-10-19 00:34:14,535 step [ 717], lr [0.0000750], embedding loss [ 0.7272], quantization loss [ 0.0445],  0.53 sec/batch.
2022-10-19 00:34:16,638 step [ 718], lr [0.0000750], embedding loss [ 0.7355], quantization loss [ 0.0444],  0.54 sec/batch.
2022-10-19 00:34:18,724 step [ 719], lr [0.0000750], embedding loss [ 0.7372], quantization loss [ 0.0376],  0.53 sec/batch.
2022-10-19 00:34:20,847 step [ 720], lr [0.0000750], embedding loss [ 0.7276], quantization loss [ 0.0379],  0.53 sec/batch.
2022-10-19 00:34:22,935 step [ 721], lr [0.0000750], embedding loss [ 0.7241], quantization loss [ 0.0415],  0.53 sec/batch.
2022-10-19 00:34:22,935 update codes and centers iter(1/1).
2022-10-19 00:34:24,542 number of update_code wrong: 0.
2022-10-19 00:34:27,547 non zero codewords: 512.
2022-10-19 00:34:27,547 finish center update, duration: 4.61 sec.
2022-10-19 00:34:29,451 step [ 722], lr [0.0000750], embedding loss [ 0.7266], quantization loss [ 0.0402],  0.53 sec/batch.
2022-10-19 00:34:31,471 step [ 723], lr [0.0000750], embedding loss [ 0.7233], quantization loss [ 0.0401],  0.52 sec/batch.
2022-10-19 00:34:33,588 step [ 724], lr [0.0000750], embedding loss [ 0.7321], quantization loss [ 0.0435],  0.53 sec/batch.
2022-10-19 00:34:35,677 step [ 725], lr [0.0000750], embedding loss [ 0.7397], quantization loss [ 0.0422],  0.53 sec/batch.
2022-10-19 00:34:37,786 step [ 726], lr [0.0000750], embedding loss [ 0.7443], quantization loss [ 0.0424],  0.53 sec/batch.
2022-10-19 00:34:39,879 step [ 727], lr [0.0000750], embedding loss [ 0.7266], quantization loss [ 0.0383],  0.53 sec/batch.
2022-10-19 00:34:41,914 step [ 728], lr [0.0000750], embedding loss [ 0.7251], quantization loss [ 0.0434],  0.53 sec/batch.
2022-10-19 00:34:44,002 step [ 729], lr [0.0000750], embedding loss [ 0.7317], quantization loss [ 0.0445],  0.54 sec/batch.
2022-10-19 00:34:46,065 step [ 730], lr [0.0000750], embedding loss [ 0.7332], quantization loss [ 0.0435],  0.53 sec/batch.
2022-10-19 00:34:48,115 step [ 731], lr [0.0000750], embedding loss [ 0.7330], quantization loss [ 0.0410],  0.53 sec/batch.
2022-10-19 00:34:50,139 step [ 732], lr [0.0000750], embedding loss [ 0.7336], quantization loss [ 0.0438],  0.53 sec/batch.
2022-10-19 00:34:52,212 step [ 733], lr [0.0000750], embedding loss [ 0.7337], quantization loss [ 0.0484],  0.53 sec/batch.
2022-10-19 00:34:54,249 step [ 734], lr [0.0000750], embedding loss [ 0.7327], quantization loss [ 0.0454],  0.54 sec/batch.
2022-10-19 00:34:56,250 step [ 735], lr [0.0000750], embedding loss [ 0.7359], quantization loss [ 0.0422],  0.51 sec/batch.
2022-10-19 00:34:58,274 step [ 736], lr [0.0000750], embedding loss [ 0.7279], quantization loss [ 0.0395],  0.53 sec/batch.
2022-10-19 00:35:00,358 step [ 737], lr [0.0000750], embedding loss [ 0.7280], quantization loss [ 0.0390],  0.52 sec/batch.
2022-10-19 00:35:02,409 step [ 738], lr [0.0000750], embedding loss [ 0.7409], quantization loss [ 0.0403],  0.53 sec/batch.
2022-10-19 00:35:04,460 step [ 739], lr [0.0000750], embedding loss [ 0.7386], quantization loss [ 0.0375],  0.52 sec/batch.
2022-10-19 00:35:06,504 step [ 740], lr [0.0000750], embedding loss [ 0.7213], quantization loss [ 0.0425],  0.52 sec/batch.
2022-10-19 00:35:08,552 step [ 741], lr [0.0000750], embedding loss [ 0.7257], quantization loss [ 0.0422],  0.51 sec/batch.
2022-10-19 00:35:10,628 step [ 742], lr [0.0000750], embedding loss [ 0.7305], quantization loss [ 0.0440],  0.53 sec/batch.
2022-10-19 00:35:12,656 step [ 743], lr [0.0000750], embedding loss [ 0.7307], quantization loss [ 0.0430],  0.53 sec/batch.
2022-10-19 00:35:14,647 step [ 744], lr [0.0000750], embedding loss [ 0.7358], quantization loss [ 0.0449],  0.53 sec/batch.
2022-10-19 00:35:16,732 step [ 745], lr [0.0000750], embedding loss [ 0.7182], quantization loss [ 0.0440],  0.53 sec/batch.
2022-10-19 00:35:18,759 step [ 746], lr [0.0000750], embedding loss [ 0.7450], quantization loss [ 0.0446],  0.52 sec/batch.
2022-10-19 00:35:20,870 step [ 747], lr [0.0000750], embedding loss [ 0.7291], quantization loss [ 0.0418],  0.53 sec/batch.
2022-10-19 00:35:22,884 step [ 748], lr [0.0000750], embedding loss [ 0.7231], quantization loss [ 0.0422],  0.52 sec/batch.
2022-10-19 00:35:24,899 step [ 749], lr [0.0000750], embedding loss [ 0.7387], quantization loss [ 0.0432],  0.52 sec/batch.
2022-10-19 00:35:26,883 step [ 750], lr [0.0000750], embedding loss [ 0.7432], quantization loss [ 0.0383],  0.52 sec/batch.
2022-10-19 00:35:28,927 step [ 751], lr [0.0000750], embedding loss [ 0.7356], quantization loss [ 0.0408],  0.53 sec/batch.
2022-10-19 00:35:30,975 step [ 752], lr [0.0000750], embedding loss [ 0.7398], quantization loss [ 0.0469],  0.53 sec/batch.
2022-10-19 00:35:33,051 step [ 753], lr [0.0000750], embedding loss [ 0.7450], quantization loss [ 0.0373],  0.53 sec/batch.
2022-10-19 00:35:35,056 step [ 754], lr [0.0000750], embedding loss [ 0.7268], quantization loss [ 0.0394],  0.51 sec/batch.
2022-10-19 00:35:37,077 step [ 755], lr [0.0000750], embedding loss [ 0.7426], quantization loss [ 0.0409],  0.52 sec/batch.
2022-10-19 00:35:39,080 step [ 756], lr [0.0000750], embedding loss [ 0.7441], quantization loss [ 0.0433],  0.52 sec/batch.
2022-10-19 00:35:41,152 step [ 757], lr [0.0000750], embedding loss [ 0.7295], quantization loss [ 0.0425],  0.52 sec/batch.
2022-10-19 00:35:43,123 step [ 758], lr [0.0000750], embedding loss [ 0.7329], quantization loss [ 0.0430],  0.51 sec/batch.
2022-10-19 00:35:45,190 step [ 759], lr [0.0000750], embedding loss [ 0.7408], quantization loss [ 0.0414],  0.53 sec/batch.
2022-10-19 00:35:47,222 step [ 760], lr [0.0000750], embedding loss [ 0.7389], quantization loss [ 0.0396],  0.52 sec/batch.
2022-10-19 00:35:49,219 step [ 761], lr [0.0000750], embedding loss [ 0.7374], quantization loss [ 0.0408],  0.52 sec/batch.
2022-10-19 00:35:51,213 step [ 762], lr [0.0000750], embedding loss [ 0.7407], quantization loss [ 0.0402],  0.52 sec/batch.
2022-10-19 00:35:53,199 step [ 763], lr [0.0000750], embedding loss [ 0.7378], quantization loss [ 0.0468],  0.52 sec/batch.
2022-10-19 00:35:55,237 step [ 764], lr [0.0000750], embedding loss [ 0.7310], quantization loss [ 0.0341],  0.53 sec/batch.
2022-10-19 00:35:57,262 step [ 765], lr [0.0000750], embedding loss [ 0.7215], quantization loss [ 0.0400],  0.53 sec/batch.
2022-10-19 00:35:59,269 step [ 766], lr [0.0000750], embedding loss [ 0.7258], quantization loss [ 0.0419],  0.53 sec/batch.
2022-10-19 00:36:01,311 step [ 767], lr [0.0000750], embedding loss [ 0.7359], quantization loss [ 0.0399],  0.53 sec/batch.
2022-10-19 00:36:03,343 step [ 768], lr [0.0000750], embedding loss [ 0.7331], quantization loss [ 0.0360],  0.52 sec/batch.
2022-10-19 00:36:05,436 step [ 769], lr [0.0000750], embedding loss [ 0.7433], quantization loss [ 0.0435],  0.51 sec/batch.
2022-10-19 00:36:07,466 step [ 770], lr [0.0000750], embedding loss [ 0.7360], quantization loss [ 0.0366],  0.52 sec/batch.
2022-10-19 00:36:09,504 step [ 771], lr [0.0000750], embedding loss [ 0.7347], quantization loss [ 0.0365],  0.53 sec/batch.
2022-10-19 00:36:11,535 step [ 772], lr [0.0000750], embedding loss [ 0.7280], quantization loss [ 0.0400],  0.52 sec/batch.
2022-10-19 00:36:13,570 step [ 773], lr [0.0000750], embedding loss [ 0.7305], quantization loss [ 0.0435],  0.52 sec/batch.
2022-10-19 00:36:15,613 step [ 774], lr [0.0000750], embedding loss [ 0.7360], quantization loss [ 0.0415],  0.53 sec/batch.
2022-10-19 00:36:17,719 step [ 775], lr [0.0000750], embedding loss [ 0.7391], quantization loss [ 0.0397],  0.53 sec/batch.
2022-10-19 00:36:19,796 step [ 776], lr [0.0000750], embedding loss [ 0.7304], quantization loss [ 0.0401],  0.52 sec/batch.
2022-10-19 00:36:21,913 step [ 777], lr [0.0000750], embedding loss [ 0.7256], quantization loss [ 0.0374],  0.53 sec/batch.
2022-10-19 00:36:23,978 step [ 778], lr [0.0000750], embedding loss [ 0.7447], quantization loss [ 0.0450],  0.52 sec/batch.
2022-10-19 00:36:26,024 step [ 779], lr [0.0000750], embedding loss [ 0.7384], quantization loss [ 0.0403],  0.53 sec/batch.
2022-10-19 00:36:28,147 step [ 780], lr [0.0000750], embedding loss [ 0.7369], quantization loss [ 0.0429],  0.53 sec/batch.
2022-10-19 00:36:30,207 step [ 781], lr [0.0000750], embedding loss [ 0.7296], quantization loss [ 0.0374],  0.53 sec/batch.
2022-10-19 00:36:32,218 step [ 782], lr [0.0000750], embedding loss [ 0.7565], quantization loss [ 0.0385],  0.51 sec/batch.
2022-10-19 00:36:34,258 step [ 783], lr [0.0000750], embedding loss [ 0.7266], quantization loss [ 0.0382],  0.52 sec/batch.
2022-10-19 00:36:36,364 step [ 784], lr [0.0000750], embedding loss [ 0.7202], quantization loss [ 0.0335],  0.53 sec/batch.
2022-10-19 00:36:38,431 step [ 785], lr [0.0000750], embedding loss [ 0.7425], quantization loss [ 0.0391],  0.53 sec/batch.
2022-10-19 00:36:40,468 step [ 786], lr [0.0000750], embedding loss [ 0.7319], quantization loss [ 0.0375],  0.51 sec/batch.
2022-10-19 00:36:42,508 step [ 787], lr [0.0000750], embedding loss [ 0.7469], quantization loss [ 0.0402],  0.53 sec/batch.
2022-10-19 00:36:44,535 step [ 788], lr [0.0000750], embedding loss [ 0.7304], quantization loss [ 0.0360],  0.53 sec/batch.
2022-10-19 00:36:46,539 step [ 789], lr [0.0000750], embedding loss [ 0.7301], quantization loss [ 0.0420],  0.52 sec/batch.
2022-10-19 00:36:48,558 step [ 790], lr [0.0000750], embedding loss [ 0.7346], quantization loss [ 0.0352],  0.52 sec/batch.
2022-10-19 00:36:50,575 step [ 791], lr [0.0000750], embedding loss [ 0.7241], quantization loss [ 0.0372],  0.51 sec/batch.
2022-10-19 00:36:52,611 step [ 792], lr [0.0000750], embedding loss [ 0.7210], quantization loss [ 0.0367],  0.53 sec/batch.
2022-10-19 00:36:54,663 step [ 793], lr [0.0000750], embedding loss [ 0.7259], quantization loss [ 0.0388],  0.52 sec/batch.
2022-10-19 00:36:56,766 step [ 794], lr [0.0000750], embedding loss [ 0.7316], quantization loss [ 0.0464],  0.53 sec/batch.
2022-10-19 00:36:58,803 step [ 795], lr [0.0000750], embedding loss [ 0.7176], quantization loss [ 0.0359],  0.50 sec/batch.
2022-10-19 00:37:00,842 step [ 796], lr [0.0000750], embedding loss [ 0.7222], quantization loss [ 0.0370],  0.53 sec/batch.
2022-10-19 00:37:02,877 step [ 797], lr [0.0000750], embedding loss [ 0.7308], quantization loss [ 0.0379],  0.52 sec/batch.
2022-10-19 00:37:04,925 step [ 798], lr [0.0000750], embedding loss [ 0.7287], quantization loss [ 0.0380],  0.52 sec/batch.
2022-10-19 00:37:06,958 step [ 799], lr [0.0000750], embedding loss [ 0.7260], quantization loss [ 0.0424],  0.53 sec/batch.
2022-10-19 00:37:09,017 step [ 800], lr [0.0000750], embedding loss [ 0.7268], quantization loss [ 0.0336],  0.53 sec/batch.
2022-10-19 00:37:11,067 step [ 801], lr [0.0000750], embedding loss [ 0.7275], quantization loss [ 0.0411],  0.52 sec/batch.
2022-10-19 00:37:11,067 update codes and centers iter(1/1).
2022-10-19 00:37:12,666 number of update_code wrong: 0.
2022-10-19 00:37:15,692 non zero codewords: 512.
2022-10-19 00:37:15,692 finish center update, duration: 4.63 sec.
2022-10-19 00:37:17,709 step [ 802], lr [0.0000750], embedding loss [ 0.7423], quantization loss [ 0.0432],  0.53 sec/batch.
2022-10-19 00:37:19,825 step [ 803], lr [0.0000750], embedding loss [ 0.7396], quantization loss [ 0.0384],  0.53 sec/batch.
2022-10-19 00:37:21,900 step [ 804], lr [0.0000750], embedding loss [ 0.7240], quantization loss [ 0.0396],  0.53 sec/batch.
2022-10-19 00:37:24,052 step [ 805], lr [0.0000750], embedding loss [ 0.7306], quantization loss [ 0.0390],  0.52 sec/batch.
2022-10-19 00:37:26,148 step [ 806], lr [0.0000750], embedding loss [ 0.7374], quantization loss [ 0.0418],  0.52 sec/batch.
2022-10-19 00:37:28,268 step [ 807], lr [0.0000750], embedding loss [ 0.7289], quantization loss [ 0.0439],  0.53 sec/batch.
2022-10-19 00:37:30,303 step [ 808], lr [0.0000750], embedding loss [ 0.7365], quantization loss [ 0.0404],  0.52 sec/batch.
2022-10-19 00:37:32,350 step [ 809], lr [0.0000750], embedding loss [ 0.7278], quantization loss [ 0.0435],  0.53 sec/batch.
2022-10-19 00:37:34,411 step [ 810], lr [0.0000750], embedding loss [ 0.7364], quantization loss [ 0.0421],  0.53 sec/batch.
2022-10-19 00:37:36,482 step [ 811], lr [0.0000750], embedding loss [ 0.7377], quantization loss [ 0.0366],  0.53 sec/batch.
2022-10-19 00:37:38,567 step [ 812], lr [0.0000750], embedding loss [ 0.7305], quantization loss [ 0.0384],  0.52 sec/batch.
2022-10-19 00:37:40,576 step [ 813], lr [0.0000750], embedding loss [ 0.7205], quantization loss [ 0.0406],  0.52 sec/batch.
2022-10-19 00:37:42,593 step [ 814], lr [0.0000750], embedding loss [ 0.7368], quantization loss [ 0.0391],  0.52 sec/batch.
2022-10-19 00:37:44,692 step [ 815], lr [0.0000750], embedding loss [ 0.7277], quantization loss [ 0.0437],  0.53 sec/batch.
2022-10-19 00:37:46,759 step [ 816], lr [0.0000750], embedding loss [ 0.7270], quantization loss [ 0.0357],  0.52 sec/batch.
2022-10-19 00:37:48,858 step [ 817], lr [0.0000750], embedding loss [ 0.7291], quantization loss [ 0.0403],  0.52 sec/batch.
2022-10-19 00:37:50,903 step [ 818], lr [0.0000750], embedding loss [ 0.7336], quantization loss [ 0.0384],  0.52 sec/batch.
2022-10-19 00:37:52,953 step [ 819], lr [0.0000750], embedding loss [ 0.7249], quantization loss [ 0.0389],  0.52 sec/batch.
2022-10-19 00:37:55,075 step [ 820], lr [0.0000750], embedding loss [ 0.7445], quantization loss [ 0.0387],  0.54 sec/batch.
2022-10-19 00:37:57,147 step [ 821], lr [0.0000750], embedding loss [ 0.7352], quantization loss [ 0.0383],  0.53 sec/batch.
2022-10-19 00:37:59,196 step [ 822], lr [0.0000750], embedding loss [ 0.7497], quantization loss [ 0.0350],  0.53 sec/batch.
2022-10-19 00:38:01,295 step [ 823], lr [0.0000750], embedding loss [ 0.7230], quantization loss [ 0.0408],  0.53 sec/batch.
2022-10-19 00:38:03,412 step [ 824], lr [0.0000750], embedding loss [ 0.7313], quantization loss [ 0.0428],  0.53 sec/batch.
2022-10-19 00:38:05,434 step [ 825], lr [0.0000750], embedding loss [ 0.7153], quantization loss [ 0.0418],  0.51 sec/batch.
2022-10-19 00:38:07,438 step [ 826], lr [0.0000750], embedding loss [ 0.7263], quantization loss [ 0.0370],  0.52 sec/batch.
2022-10-19 00:38:09,482 step [ 827], lr [0.0000750], embedding loss [ 0.7210], quantization loss [ 0.0423],  0.52 sec/batch.
2022-10-19 00:38:11,542 step [ 828], lr [0.0000750], embedding loss [ 0.7396], quantization loss [ 0.0390],  0.53 sec/batch.
2022-10-19 00:38:13,641 step [ 829], lr [0.0000750], embedding loss [ 0.7309], quantization loss [ 0.0426],  0.53 sec/batch.
2022-10-19 00:38:15,665 step [ 830], lr [0.0000750], embedding loss [ 0.7263], quantization loss [ 0.0408],  0.52 sec/batch.
2022-10-19 00:38:17,699 step [ 831], lr [0.0000750], embedding loss [ 0.7282], quantization loss [ 0.0401],  0.52 sec/batch.
2022-10-19 00:38:19,751 step [ 832], lr [0.0000750], embedding loss [ 0.7210], quantization loss [ 0.0423],  0.53 sec/batch.
2022-10-19 00:38:21,814 step [ 833], lr [0.0000750], embedding loss [ 0.7240], quantization loss [ 0.0425],  0.56 sec/batch.
2022-10-19 00:38:23,940 step [ 834], lr [0.0000750], embedding loss [ 0.7420], quantization loss [ 0.0393],  0.53 sec/batch.
2022-10-19 00:38:25,978 step [ 835], lr [0.0000750], embedding loss [ 0.7413], quantization loss [ 0.0468],  0.56 sec/batch.
2022-10-19 00:38:28,025 step [ 836], lr [0.0000750], embedding loss [ 0.7383], quantization loss [ 0.0422],  0.53 sec/batch.
2022-10-19 00:38:30,216 step [ 837], lr [0.0000750], embedding loss [ 0.7405], quantization loss [ 0.0454],  0.59 sec/batch.
2022-10-19 00:38:32,446 step [ 838], lr [0.0000750], embedding loss [ 0.7206], quantization loss [ 0.0415],  0.57 sec/batch.
2022-10-19 00:38:34,598 step [ 839], lr [0.0000750], embedding loss [ 0.7362], quantization loss [ 0.0356],  0.59 sec/batch.
2022-10-19 00:38:36,698 step [ 840], lr [0.0000750], embedding loss [ 0.7342], quantization loss [ 0.0411],  0.53 sec/batch.
2022-10-19 00:38:38,821 step [ 841], lr [0.0000750], embedding loss [ 0.7420], quantization loss [ 0.0371],  0.54 sec/batch.
2022-10-19 00:38:40,921 step [ 842], lr [0.0000750], embedding loss [ 0.7277], quantization loss [ 0.0380],  0.56 sec/batch.
2022-10-19 00:38:42,946 step [ 843], lr [0.0000750], embedding loss [ 0.7258], quantization loss [ 0.0402],  0.52 sec/batch.
2022-10-19 00:38:45,118 step [ 844], lr [0.0000750], embedding loss [ 0.7353], quantization loss [ 0.0416],  0.56 sec/batch.
2022-10-19 00:38:47,176 step [ 845], lr [0.0000750], embedding loss [ 0.7391], quantization loss [ 0.0403],  0.58 sec/batch.
2022-10-19 00:38:49,414 step [ 846], lr [0.0000750], embedding loss [ 0.7384], quantization loss [ 0.0363],  0.56 sec/batch.
2022-10-19 00:38:51,515 step [ 847], lr [0.0000750], embedding loss [ 0.7188], quantization loss [ 0.0419],  0.57 sec/batch.
2022-10-19 00:38:53,583 step [ 848], lr [0.0000750], embedding loss [ 0.7233], quantization loss [ 0.0376],  0.54 sec/batch.
2022-10-19 00:38:55,764 step [ 849], lr [0.0000750], embedding loss [ 0.7346], quantization loss [ 0.0448],  0.51 sec/batch.
2022-10-19 00:38:57,868 step [ 850], lr [0.0000750], embedding loss [ 0.7299], quantization loss [ 0.0399],  0.53 sec/batch.
2022-10-19 00:38:59,918 step [ 851], lr [0.0000750], embedding loss [ 0.7182], quantization loss [ 0.0400],  0.53 sec/batch.
2022-10-19 00:39:01,987 step [ 852], lr [0.0000750], embedding loss [ 0.7239], quantization loss [ 0.0376],  0.54 sec/batch.
2022-10-19 00:39:04,167 step [ 853], lr [0.0000750], embedding loss [ 0.7330], quantization loss [ 0.0381],  0.51 sec/batch.
2022-10-19 00:39:06,218 step [ 854], lr [0.0000750], embedding loss [ 0.7315], quantization loss [ 0.0436],  0.52 sec/batch.
2022-10-19 00:39:08,341 step [ 855], lr [0.0000750], embedding loss [ 0.7197], quantization loss [ 0.0375],  0.52 sec/batch.
2022-10-19 00:39:10,421 step [ 856], lr [0.0000750], embedding loss [ 0.7337], quantization loss [ 0.0377],  0.53 sec/batch.
2022-10-19 00:39:12,503 step [ 857], lr [0.0000750], embedding loss [ 0.7268], quantization loss [ 0.0365],  0.51 sec/batch.
2022-10-19 00:39:14,512 step [ 858], lr [0.0000750], embedding loss [ 0.7374], quantization loss [ 0.0377],  0.52 sec/batch.
2022-10-19 00:39:16,568 step [ 859], lr [0.0000750], embedding loss [ 0.7383], quantization loss [ 0.0362],  0.53 sec/batch.
2022-10-19 00:39:18,698 step [ 860], lr [0.0000750], embedding loss [ 0.7354], quantization loss [ 0.0381],  0.54 sec/batch.
2022-10-19 00:39:20,882 step [ 861], lr [0.0000750], embedding loss [ 0.7374], quantization loss [ 0.0360],  0.55 sec/batch.
2022-10-19 00:39:23,005 step [ 862], lr [0.0000750], embedding loss [ 0.7313], quantization loss [ 0.0412],  0.56 sec/batch.
2022-10-19 00:39:25,184 step [ 863], lr [0.0000750], embedding loss [ 0.7379], quantization loss [ 0.0415],  0.58 sec/batch.
2022-10-19 00:39:27,234 step [ 864], lr [0.0000750], embedding loss [ 0.7324], quantization loss [ 0.0449],  0.57 sec/batch.
2022-10-19 00:39:29,452 step [ 865], lr [0.0000750], embedding loss [ 0.7221], quantization loss [ 0.0375],  0.56 sec/batch.
2022-10-19 00:39:31,640 step [ 866], lr [0.0000750], embedding loss [ 0.7223], quantization loss [ 0.0381],  0.58 sec/batch.
2022-10-19 00:39:33,825 step [ 867], lr [0.0000750], embedding loss [ 0.7393], quantization loss [ 0.0386],  0.61 sec/batch.
2022-10-19 00:39:35,853 step [ 868], lr [0.0000750], embedding loss [ 0.7251], quantization loss [ 0.0373],  0.51 sec/batch.
2022-10-19 00:39:37,905 step [ 869], lr [0.0000750], embedding loss [ 0.7220], quantization loss [ 0.0445],  0.53 sec/batch.
2022-10-19 00:39:39,925 step [ 870], lr [0.0000750], embedding loss [ 0.7316], quantization loss [ 0.0408],  0.53 sec/batch.
2022-10-19 00:39:42,036 step [ 871], lr [0.0000750], embedding loss [ 0.7321], quantization loss [ 0.0385],  0.55 sec/batch.
2022-10-19 00:39:44,160 step [ 872], lr [0.0000750], embedding loss [ 0.7364], quantization loss [ 0.0368],  0.54 sec/batch.
2022-10-19 00:39:46,381 step [ 873], lr [0.0000750], embedding loss [ 0.7445], quantization loss [ 0.0391],  0.58 sec/batch.
2022-10-19 00:39:48,555 step [ 874], lr [0.0000750], embedding loss [ 0.7317], quantization loss [ 0.0395],  0.52 sec/batch.
2022-10-19 00:39:50,644 step [ 875], lr [0.0000750], embedding loss [ 0.7278], quantization loss [ 0.0369],  0.51 sec/batch.
2022-10-19 00:39:52,684 step [ 876], lr [0.0000750], embedding loss [ 0.7320], quantization loss [ 0.0358],  0.51 sec/batch.
2022-10-19 00:39:54,677 step [ 877], lr [0.0000750], embedding loss [ 0.7402], quantization loss [ 0.0367],  0.51 sec/batch.
2022-10-19 00:39:56,690 step [ 878], lr [0.0000750], embedding loss [ 0.7177], quantization loss [ 0.0386],  0.51 sec/batch.
2022-10-19 00:39:58,719 step [ 879], lr [0.0000750], embedding loss [ 0.7283], quantization loss [ 0.0361],  0.52 sec/batch.
2022-10-19 00:40:00,738 step [ 880], lr [0.0000750], embedding loss [ 0.7299], quantization loss [ 0.0323],  0.52 sec/batch.
2022-10-19 00:40:02,759 step [ 881], lr [0.0000750], embedding loss [ 0.7283], quantization loss [ 0.0456],  0.51 sec/batch.
2022-10-19 00:40:02,760 update codes and centers iter(1/1).
2022-10-19 00:40:04,341 number of update_code wrong: 0.
2022-10-19 00:40:07,455 non zero codewords: 512.
2022-10-19 00:40:07,455 finish center update, duration: 4.70 sec.
2022-10-19 00:40:09,492 step [ 882], lr [0.0000750], embedding loss [ 0.7270], quantization loss [ 0.0387],  0.55 sec/batch.
2022-10-19 00:40:11,597 step [ 883], lr [0.0000750], embedding loss [ 0.7404], quantization loss [ 0.0400],  0.53 sec/batch.
2022-10-19 00:40:13,675 step [ 884], lr [0.0000750], embedding loss [ 0.7303], quantization loss [ 0.0459],  0.58 sec/batch.
2022-10-19 00:40:15,798 step [ 885], lr [0.0000750], embedding loss [ 0.7278], quantization loss [ 0.0390],  0.53 sec/batch.
2022-10-19 00:40:17,933 step [ 886], lr [0.0000750], embedding loss [ 0.7282], quantization loss [ 0.0370],  0.54 sec/batch.
2022-10-19 00:40:20,010 step [ 887], lr [0.0000750], embedding loss [ 0.7212], quantization loss [ 0.0398],  0.53 sec/batch.
2022-10-19 00:40:22,067 step [ 888], lr [0.0000750], embedding loss [ 0.7261], quantization loss [ 0.0365],  0.50 sec/batch.
2022-10-19 00:40:24,184 step [ 889], lr [0.0000750], embedding loss [ 0.7229], quantization loss [ 0.0457],  0.52 sec/batch.
2022-10-19 00:40:26,276 step [ 890], lr [0.0000750], embedding loss [ 0.7363], quantization loss [ 0.0394],  0.54 sec/batch.
2022-10-19 00:40:28,379 step [ 891], lr [0.0000750], embedding loss [ 0.7452], quantization loss [ 0.0404],  0.53 sec/batch.
2022-10-19 00:40:30,518 step [ 892], lr [0.0000750], embedding loss [ 0.7326], quantization loss [ 0.0458],  0.53 sec/batch.
2022-10-19 00:40:32,698 step [ 893], lr [0.0000750], embedding loss [ 0.7350], quantization loss [ 0.0406],  0.53 sec/batch.
2022-10-19 00:40:34,815 step [ 894], lr [0.0000750], embedding loss [ 0.7410], quantization loss [ 0.0425],  0.53 sec/batch.
2022-10-19 00:40:36,909 step [ 895], lr [0.0000750], embedding loss [ 0.7292], quantization loss [ 0.0335],  0.53 sec/batch.
2022-10-19 00:40:39,010 step [ 896], lr [0.0000750], embedding loss [ 0.7223], quantization loss [ 0.0365],  0.53 sec/batch.
2022-10-19 00:40:41,099 step [ 897], lr [0.0000750], embedding loss [ 0.7194], quantization loss [ 0.0401],  0.53 sec/batch.
2022-10-19 00:40:43,209 step [ 898], lr [0.0000750], embedding loss [ 0.7355], quantization loss [ 0.0386],  0.52 sec/batch.
2022-10-19 00:40:45,313 step [ 899], lr [0.0000750], embedding loss [ 0.7157], quantization loss [ 0.0380],  0.53 sec/batch.
2022-10-19 00:40:47,418 step [ 900], lr [0.0000750], embedding loss [ 0.7224], quantization loss [ 0.0360],  0.53 sec/batch.
2022-10-19 00:40:49,572 step [ 901], lr [0.0000375], embedding loss [ 0.7244], quantization loss [ 0.0387],  0.53 sec/batch.
2022-10-19 00:40:51,627 step [ 902], lr [0.0000375], embedding loss [ 0.7281], quantization loss [ 0.0390],  0.50 sec/batch.
2022-10-19 00:40:53,696 step [ 903], lr [0.0000375], embedding loss [ 0.7348], quantization loss [ 0.0435],  0.53 sec/batch.
2022-10-19 00:40:55,840 step [ 904], lr [0.0000375], embedding loss [ 0.7340], quantization loss [ 0.0392],  0.53 sec/batch.
2022-10-19 00:40:57,984 step [ 905], lr [0.0000375], embedding loss [ 0.7369], quantization loss [ 0.0426],  0.53 sec/batch.
2022-10-19 00:41:00,131 step [ 906], lr [0.0000375], embedding loss [ 0.7179], quantization loss [ 0.0393],  0.53 sec/batch.
2022-10-19 00:41:02,225 step [ 907], lr [0.0000375], embedding loss [ 0.7266], quantization loss [ 0.0372],  0.53 sec/batch.
2022-10-19 00:41:04,401 step [ 908], lr [0.0000375], embedding loss [ 0.7267], quantization loss [ 0.0356],  0.53 sec/batch.
2022-10-19 00:41:06,468 step [ 909], lr [0.0000375], embedding loss [ 0.7272], quantization loss [ 0.0431],  0.52 sec/batch.
2022-10-19 00:41:08,544 step [ 910], lr [0.0000375], embedding loss [ 0.7357], quantization loss [ 0.0373],  0.51 sec/batch.
2022-10-19 00:41:10,584 step [ 911], lr [0.0000375], embedding loss [ 0.7332], quantization loss [ 0.0395],  0.51 sec/batch.
2022-10-19 00:41:12,683 step [ 912], lr [0.0000375], embedding loss [ 0.7226], quantization loss [ 0.0380],  0.52 sec/batch.
2022-10-19 00:41:14,781 step [ 913], lr [0.0000375], embedding loss [ 0.7376], quantization loss [ 0.0363],  0.51 sec/batch.
2022-10-19 00:41:16,843 step [ 914], lr [0.0000375], embedding loss [ 0.7311], quantization loss [ 0.0355],  0.52 sec/batch.
2022-10-19 00:41:18,874 step [ 915], lr [0.0000375], embedding loss [ 0.7368], quantization loss [ 0.0401],  0.52 sec/batch.
2022-10-19 00:41:20,995 step [ 916], lr [0.0000375], embedding loss [ 0.7398], quantization loss [ 0.0388],  0.52 sec/batch.
2022-10-19 00:41:23,143 step [ 917], lr [0.0000375], embedding loss [ 0.7288], quantization loss [ 0.0384],  0.53 sec/batch.
2022-10-19 00:41:25,225 step [ 918], lr [0.0000375], embedding loss [ 0.7205], quantization loss [ 0.0427],  0.53 sec/batch.
2022-10-19 00:41:27,280 step [ 919], lr [0.0000375], embedding loss [ 0.7303], quantization loss [ 0.0378],  0.52 sec/batch.
2022-10-19 00:41:29,366 step [ 920], lr [0.0000375], embedding loss [ 0.7303], quantization loss [ 0.0360],  0.53 sec/batch.
2022-10-19 00:41:31,416 step [ 921], lr [0.0000375], embedding loss [ 0.7169], quantization loss [ 0.0389],  0.50 sec/batch.
2022-10-19 00:41:33,483 step [ 922], lr [0.0000375], embedding loss [ 0.7192], quantization loss [ 0.0349],  0.52 sec/batch.
2022-10-19 00:41:35,558 step [ 923], lr [0.0000375], embedding loss [ 0.7273], quantization loss [ 0.0359],  0.52 sec/batch.
2022-10-19 00:41:37,651 step [ 924], lr [0.0000375], embedding loss [ 0.7300], quantization loss [ 0.0327],  0.53 sec/batch.
2022-10-19 00:41:39,716 step [ 925], lr [0.0000375], embedding loss [ 0.7436], quantization loss [ 0.0334],  0.53 sec/batch.
2022-10-19 00:41:41,817 step [ 926], lr [0.0000375], embedding loss [ 0.7316], quantization loss [ 0.0386],  0.53 sec/batch.
2022-10-19 00:41:43,879 step [ 927], lr [0.0000375], embedding loss [ 0.7256], quantization loss [ 0.0360],  0.52 sec/batch.
2022-10-19 00:41:45,942 step [ 928], lr [0.0000375], embedding loss [ 0.7276], quantization loss [ 0.0390],  0.53 sec/batch.
2022-10-19 00:41:47,992 step [ 929], lr [0.0000375], embedding loss [ 0.7318], quantization loss [ 0.0398],  0.51 sec/batch.
2022-10-19 00:41:50,024 step [ 930], lr [0.0000375], embedding loss [ 0.7432], quantization loss [ 0.0407],  0.51 sec/batch.
2022-10-19 00:41:52,090 step [ 931], lr [0.0000375], embedding loss [ 0.7323], quantization loss [ 0.0359],  0.53 sec/batch.
2022-10-19 00:41:54,159 step [ 932], lr [0.0000375], embedding loss [ 0.7199], quantization loss [ 0.0402],  0.52 sec/batch.
2022-10-19 00:41:56,225 step [ 933], lr [0.0000375], embedding loss [ 0.7354], quantization loss [ 0.0447],  0.52 sec/batch.
2022-10-19 00:41:58,251 step [ 934], lr [0.0000375], embedding loss [ 0.7259], quantization loss [ 0.0397],  0.51 sec/batch.
2022-10-19 00:42:00,319 step [ 935], lr [0.0000375], embedding loss [ 0.7241], quantization loss [ 0.0376],  0.53 sec/batch.
2022-10-19 00:42:02,411 step [ 936], lr [0.0000375], embedding loss [ 0.7371], quantization loss [ 0.0423],  0.53 sec/batch.
2022-10-19 00:42:04,508 step [ 937], lr [0.0000375], embedding loss [ 0.7258], quantization loss [ 0.0399],  0.53 sec/batch.
2022-10-19 00:42:06,581 step [ 938], lr [0.0000375], embedding loss [ 0.7350], quantization loss [ 0.0394],  0.53 sec/batch.
2022-10-19 00:42:08,651 step [ 939], lr [0.0000375], embedding loss [ 0.7345], quantization loss [ 0.0329],  0.52 sec/batch.
2022-10-19 00:42:10,685 step [ 940], lr [0.0000375], embedding loss [ 0.7407], quantization loss [ 0.0384],  0.52 sec/batch.
2022-10-19 00:42:12,749 step [ 941], lr [0.0000375], embedding loss [ 0.7382], quantization loss [ 0.0339],  0.52 sec/batch.
2022-10-19 00:42:14,748 step [ 942], lr [0.0000375], embedding loss [ 0.7314], quantization loss [ 0.0411],  0.52 sec/batch.
2022-10-19 00:42:16,914 step [ 943], lr [0.0000375], embedding loss [ 0.7019], quantization loss [ 0.0345],  0.53 sec/batch.
2022-10-19 00:42:18,998 step [ 944], lr [0.0000375], embedding loss [ 0.7297], quantization loss [ 0.0344],  0.52 sec/batch.
2022-10-19 00:42:21,102 step [ 945], lr [0.0000375], embedding loss [ 0.7267], quantization loss [ 0.0394],  0.52 sec/batch.
2022-10-19 00:42:23,131 step [ 946], lr [0.0000375], embedding loss [ 0.7339], quantization loss [ 0.0370],  0.52 sec/batch.
2022-10-19 00:42:25,214 step [ 947], lr [0.0000375], embedding loss [ 0.7260], quantization loss [ 0.0387],  0.51 sec/batch.
2022-10-19 00:42:27,278 step [ 948], lr [0.0000375], embedding loss [ 0.7188], quantization loss [ 0.0365],  0.52 sec/batch.
2022-10-19 00:42:29,341 step [ 949], lr [0.0000375], embedding loss [ 0.7234], quantization loss [ 0.0382],  0.52 sec/batch.
2022-10-19 00:42:31,413 step [ 950], lr [0.0000375], embedding loss [ 0.7301], quantization loss [ 0.0395],  0.52 sec/batch.
2022-10-19 00:42:33,485 step [ 951], lr [0.0000375], embedding loss [ 0.7288], quantization loss [ 0.0372],  0.52 sec/batch.
2022-10-19 00:42:35,546 step [ 952], lr [0.0000375], embedding loss [ 0.7248], quantization loss [ 0.0397],  0.52 sec/batch.
2022-10-19 00:42:37,633 step [ 953], lr [0.0000375], embedding loss [ 0.7261], quantization loss [ 0.0347],  0.53 sec/batch.
2022-10-19 00:42:39,827 step [ 954], lr [0.0000375], embedding loss [ 0.7217], quantization loss [ 0.0409],  0.53 sec/batch.
2022-10-19 00:42:41,955 step [ 955], lr [0.0000375], embedding loss [ 0.7426], quantization loss [ 0.0389],  0.52 sec/batch.
2022-10-19 00:42:44,021 step [ 956], lr [0.0000375], embedding loss [ 0.7171], quantization loss [ 0.0329],  0.52 sec/batch.
2022-10-19 00:42:46,061 step [ 957], lr [0.0000375], embedding loss [ 0.7354], quantization loss [ 0.0375],  0.51 sec/batch.
2022-10-19 00:42:48,115 step [ 958], lr [0.0000375], embedding loss [ 0.7288], quantization loss [ 0.0342],  0.53 sec/batch.
2022-10-19 00:42:50,174 step [ 959], lr [0.0000375], embedding loss [ 0.7330], quantization loss [ 0.0408],  0.52 sec/batch.
2022-10-19 00:42:52,306 step [ 960], lr [0.0000375], embedding loss [ 0.7302], quantization loss [ 0.0371],  0.53 sec/batch.
2022-10-19 00:42:54,418 step [ 961], lr [0.0000375], embedding loss [ 0.7196], quantization loss [ 0.0376],  0.52 sec/batch.
2022-10-19 00:42:54,418 update codes and centers iter(1/1).
2022-10-19 00:42:56,028 number of update_code wrong: 0.
2022-10-19 00:42:59,182 non zero codewords: 512.
2022-10-19 00:42:59,182 finish center update, duration: 4.76 sec.
2022-10-19 00:43:01,240 step [ 962], lr [0.0000375], embedding loss [ 0.7338], quantization loss [ 0.0375],  0.54 sec/batch.
2022-10-19 00:43:03,392 step [ 963], lr [0.0000375], embedding loss [ 0.7324], quantization loss [ 0.0362],  0.54 sec/batch.
2022-10-19 00:43:05,476 step [ 964], lr [0.0000375], embedding loss [ 0.7236], quantization loss [ 0.0367],  0.53 sec/batch.
2022-10-19 00:43:07,573 step [ 965], lr [0.0000375], embedding loss [ 0.7272], quantization loss [ 0.0352],  0.53 sec/batch.
2022-10-19 00:43:09,730 step [ 966], lr [0.0000375], embedding loss [ 0.7397], quantization loss [ 0.0339],  0.52 sec/batch.
2022-10-19 00:43:11,831 step [ 967], lr [0.0000375], embedding loss [ 0.7310], quantization loss [ 0.0333],  0.55 sec/batch.
2022-10-19 00:43:13,955 step [ 968], lr [0.0000375], embedding loss [ 0.7320], quantization loss [ 0.0384],  0.53 sec/batch.
2022-10-19 00:43:16,085 step [ 969], lr [0.0000375], embedding loss [ 0.7272], quantization loss [ 0.0378],  0.52 sec/batch.
2022-10-19 00:43:18,195 step [ 970], lr [0.0000375], embedding loss [ 0.7266], quantization loss [ 0.0327],  0.53 sec/batch.
2022-10-19 00:43:20,303 step [ 971], lr [0.0000375], embedding loss [ 0.7301], quantization loss [ 0.0401],  0.51 sec/batch.
2022-10-19 00:43:22,435 step [ 972], lr [0.0000375], embedding loss [ 0.7328], quantization loss [ 0.0334],  0.53 sec/batch.
2022-10-19 00:43:24,491 step [ 973], lr [0.0000375], embedding loss [ 0.7359], quantization loss [ 0.0370],  0.51 sec/batch.
2022-10-19 00:43:26,548 step [ 974], lr [0.0000375], embedding loss [ 0.7398], quantization loss [ 0.0382],  0.52 sec/batch.
2022-10-19 00:43:28,739 step [ 975], lr [0.0000375], embedding loss [ 0.7289], quantization loss [ 0.0349],  0.53 sec/batch.
2022-10-19 00:43:30,836 step [ 976], lr [0.0000375], embedding loss [ 0.7315], quantization loss [ 0.0364],  0.52 sec/batch.
2022-10-19 00:43:32,926 step [ 977], lr [0.0000375], embedding loss [ 0.7237], quantization loss [ 0.0390],  0.53 sec/batch.
2022-10-19 00:43:35,035 step [ 978], lr [0.0000375], embedding loss [ 0.7311], quantization loss [ 0.0418],  0.53 sec/batch.
2022-10-19 00:43:37,191 step [ 979], lr [0.0000375], embedding loss [ 0.7170], quantization loss [ 0.0319],  0.53 sec/batch.
2022-10-19 00:43:39,299 step [ 980], lr [0.0000375], embedding loss [ 0.7283], quantization loss [ 0.0344],  0.52 sec/batch.
2022-10-19 00:43:41,400 step [ 981], lr [0.0000375], embedding loss [ 0.7297], quantization loss [ 0.0365],  0.53 sec/batch.
2022-10-19 00:43:43,477 step [ 982], lr [0.0000375], embedding loss [ 0.7176], quantization loss [ 0.0391],  0.52 sec/batch.
2022-10-19 00:43:45,550 step [ 983], lr [0.0000375], embedding loss [ 0.7362], quantization loss [ 0.0375],  0.53 sec/batch.
2022-10-19 00:43:47,623 step [ 984], lr [0.0000375], embedding loss [ 0.7278], quantization loss [ 0.0326],  0.53 sec/batch.
2022-10-19 00:43:49,758 step [ 985], lr [0.0000375], embedding loss [ 0.7241], quantization loss [ 0.0374],  0.53 sec/batch.
2022-10-19 00:43:51,892 step [ 986], lr [0.0000375], embedding loss [ 0.7160], quantization loss [ 0.0400],  0.53 sec/batch.
2022-10-19 00:43:54,062 step [ 987], lr [0.0000375], embedding loss [ 0.7129], quantization loss [ 0.0313],  0.52 sec/batch.
2022-10-19 00:43:56,143 step [ 988], lr [0.0000375], embedding loss [ 0.7298], quantization loss [ 0.0340],  0.52 sec/batch.
2022-10-19 00:43:58,267 step [ 989], lr [0.0000375], embedding loss [ 0.7216], quantization loss [ 0.0369],  0.53 sec/batch.
2022-10-19 00:44:00,405 step [ 990], lr [0.0000375], embedding loss [ 0.7374], quantization loss [ 0.0359],  0.53 sec/batch.
2022-10-19 00:44:02,495 step [ 991], lr [0.0000375], embedding loss [ 0.7268], quantization loss [ 0.0363],  0.52 sec/batch.
2022-10-19 00:44:04,641 step [ 992], lr [0.0000375], embedding loss [ 0.7260], quantization loss [ 0.0375],  0.52 sec/batch.
2022-10-19 00:44:06,736 step [ 993], lr [0.0000375], embedding loss [ 0.7200], quantization loss [ 0.0396],  0.52 sec/batch.
2022-10-19 00:44:08,819 step [ 994], lr [0.0000375], embedding loss [ 0.7261], quantization loss [ 0.0350],  0.51 sec/batch.
2022-10-19 00:44:10,915 step [ 995], lr [0.0000375], embedding loss [ 0.7312], quantization loss [ 0.0357],  0.52 sec/batch.
2022-10-19 00:44:13,066 step [ 996], lr [0.0000375], embedding loss [ 0.7241], quantization loss [ 0.0390],  0.53 sec/batch.
2022-10-19 00:44:15,162 step [ 997], lr [0.0000375], embedding loss [ 0.7338], quantization loss [ 0.0371],  0.52 sec/batch.
2022-10-19 00:44:17,240 step [ 998], lr [0.0000375], embedding loss [ 0.7241], quantization loss [ 0.0322],  0.52 sec/batch.
2022-10-19 00:44:19,299 step [ 999], lr [0.0000375], embedding loss [ 0.7401], quantization loss [ 0.0362],  0.50 sec/batch.
2022-10-19 00:44:21,414 step [1000], lr [0.0000375], embedding loss [ 0.7198], quantization loss [ 0.0419],  0.52 sec/batch.
2022-10-19 00:44:23,495 step [1001], lr [0.0000375], embedding loss [ 0.7304], quantization loss [ 0.0349],  0.53 sec/batch.
2022-10-19 00:44:25,641 step [1002], lr [0.0000375], embedding loss [ 0.7230], quantization loss [ 0.0357],  0.53 sec/batch.
2022-10-19 00:44:27,710 step [1003], lr [0.0000375], embedding loss [ 0.7206], quantization loss [ 0.0294],  0.52 sec/batch.
2022-10-19 00:44:29,837 step [1004], lr [0.0000375], embedding loss [ 0.7252], quantization loss [ 0.0330],  0.52 sec/batch.
2022-10-19 00:44:31,903 step [1005], lr [0.0000375], embedding loss [ 0.7315], quantization loss [ 0.0332],  0.53 sec/batch.
2022-10-19 00:44:34,005 step [1006], lr [0.0000375], embedding loss [ 0.7352], quantization loss [ 0.0391],  0.52 sec/batch.
2022-10-19 00:44:36,049 step [1007], lr [0.0000375], embedding loss [ 0.7169], quantization loss [ 0.0344],  0.52 sec/batch.
2022-10-19 00:44:38,163 step [1008], lr [0.0000375], embedding loss [ 0.7306], quantization loss [ 0.0321],  0.52 sec/batch.
2022-10-19 00:44:40,231 step [1009], lr [0.0000375], embedding loss [ 0.7153], quantization loss [ 0.0372],  0.52 sec/batch.
2022-10-19 00:44:42,355 step [1010], lr [0.0000375], embedding loss [ 0.7314], quantization loss [ 0.0399],  0.52 sec/batch.
2022-10-19 00:44:44,493 step [1011], lr [0.0000375], embedding loss [ 0.7252], quantization loss [ 0.0386],  0.53 sec/batch.
2022-10-19 00:44:46,611 step [1012], lr [0.0000375], embedding loss [ 0.7345], quantization loss [ 0.0356],  0.53 sec/batch.
2022-10-19 00:44:48,745 step [1013], lr [0.0000375], embedding loss [ 0.7356], quantization loss [ 0.0378],  0.53 sec/batch.
2022-10-19 00:44:50,829 step [1014], lr [0.0000375], embedding loss [ 0.7285], quantization loss [ 0.0355],  0.53 sec/batch.
2022-10-19 00:44:52,884 step [1015], lr [0.0000375], embedding loss [ 0.7325], quantization loss [ 0.0370],  0.51 sec/batch.
2022-10-19 00:44:54,961 step [1016], lr [0.0000375], embedding loss [ 0.7260], quantization loss [ 0.0335],  0.52 sec/batch.
2022-10-19 00:44:57,102 step [1017], lr [0.0000375], embedding loss [ 0.7262], quantization loss [ 0.0354],  0.53 sec/batch.
2022-10-19 00:44:59,203 step [1018], lr [0.0000375], embedding loss [ 0.7243], quantization loss [ 0.0347],  0.53 sec/batch.
2022-10-19 00:45:01,351 step [1019], lr [0.0000375], embedding loss [ 0.7301], quantization loss [ 0.0328],  0.54 sec/batch.
2022-10-19 00:45:03,446 step [1020], lr [0.0000375], embedding loss [ 0.7370], quantization loss [ 0.0324],  0.52 sec/batch.
2022-10-19 00:45:05,552 step [1021], lr [0.0000375], embedding loss [ 0.7263], quantization loss [ 0.0323],  0.53 sec/batch.
2022-10-19 00:45:07,631 step [1022], lr [0.0000375], embedding loss [ 0.7262], quantization loss [ 0.0356],  0.51 sec/batch.
2022-10-19 00:45:09,708 step [1023], lr [0.0000375], embedding loss [ 0.7260], quantization loss [ 0.0343],  0.52 sec/batch.
2022-10-19 00:45:11,766 step [1024], lr [0.0000375], embedding loss [ 0.7253], quantization loss [ 0.0361],  0.51 sec/batch.
2022-10-19 00:45:13,851 step [1025], lr [0.0000375], embedding loss [ 0.7210], quantization loss [ 0.0395],  0.53 sec/batch.
2022-10-19 00:45:15,951 step [1026], lr [0.0000375], embedding loss [ 0.7382], quantization loss [ 0.0378],  0.53 sec/batch.
2022-10-19 00:45:18,039 step [1027], lr [0.0000375], embedding loss [ 0.7283], quantization loss [ 0.0368],  0.52 sec/batch.
2022-10-19 00:45:20,115 step [1028], lr [0.0000375], embedding loss [ 0.7228], quantization loss [ 0.0332],  0.52 sec/batch.
2022-10-19 00:45:22,164 step [1029], lr [0.0000375], embedding loss [ 0.7194], quantization loss [ 0.0324],  0.51 sec/batch.
2022-10-19 00:45:24,238 step [1030], lr [0.0000375], embedding loss [ 0.7361], quantization loss [ 0.0384],  0.51 sec/batch.
2022-10-19 00:45:26,253 step [1031], lr [0.0000375], embedding loss [ 0.7099], quantization loss [ 0.0334],  0.51 sec/batch.
2022-10-19 00:45:28,339 step [1032], lr [0.0000375], embedding loss [ 0.7283], quantization loss [ 0.0346],  0.53 sec/batch.
2022-10-19 00:45:30,422 step [1033], lr [0.0000375], embedding loss [ 0.7373], quantization loss [ 0.0341],  0.52 sec/batch.
2022-10-19 00:45:32,496 step [1034], lr [0.0000375], embedding loss [ 0.7181], quantization loss [ 0.0344],  0.52 sec/batch.
2022-10-19 00:45:34,573 step [1035], lr [0.0000375], embedding loss [ 0.7348], quantization loss [ 0.0318],  0.53 sec/batch.
2022-10-19 00:45:36,667 step [1036], lr [0.0000375], embedding loss [ 0.7251], quantization loss [ 0.0364],  0.52 sec/batch.
2022-10-19 00:45:38,709 step [1037], lr [0.0000375], embedding loss [ 0.7306], quantization loss [ 0.0352],  0.52 sec/batch.
2022-10-19 00:45:40,757 step [1038], lr [0.0000375], embedding loss [ 0.7259], quantization loss [ 0.0392],  0.51 sec/batch.
2022-10-19 00:45:42,777 step [1039], lr [0.0000375], embedding loss [ 0.7387], quantization loss [ 0.0348],  0.51 sec/batch.
2022-10-19 00:45:44,819 step [1040], lr [0.0000375], embedding loss [ 0.7252], quantization loss [ 0.0323],  0.52 sec/batch.
2022-10-19 00:45:46,909 step [1041], lr [0.0000375], embedding loss [ 0.7168], quantization loss [ 0.0350],  0.52 sec/batch.
2022-10-19 00:45:46,909 update codes and centers iter(1/1).
2022-10-19 00:45:48,485 number of update_code wrong: 0.
2022-10-19 00:45:51,528 non zero codewords: 512.
2022-10-19 00:45:51,528 finish center update, duration: 4.62 sec.
2022-10-19 00:45:53,485 step [1042], lr [0.0000375], embedding loss [ 0.7176], quantization loss [ 0.0335],  0.53 sec/batch.
2022-10-19 00:45:55,575 step [1043], lr [0.0000375], embedding loss [ 0.7117], quantization loss [ 0.0365],  0.53 sec/batch.
2022-10-19 00:45:57,829 step [1044], lr [0.0000375], embedding loss [ 0.7317], quantization loss [ 0.0361],  0.54 sec/batch.
2022-10-19 00:46:00,001 step [1045], lr [0.0000375], embedding loss [ 0.7181], quantization loss [ 0.0331],  0.53 sec/batch.
2022-10-19 00:46:02,111 step [1046], lr [0.0000375], embedding loss [ 0.7247], quantization loss [ 0.0327],  0.53 sec/batch.
2022-10-19 00:46:04,230 step [1047], lr [0.0000375], embedding loss [ 0.7339], quantization loss [ 0.0369],  0.52 sec/batch.
2022-10-19 00:46:06,410 step [1048], lr [0.0000375], embedding loss [ 0.7309], quantization loss [ 0.0346],  0.53 sec/batch.
2022-10-19 00:46:08,592 step [1049], lr [0.0000375], embedding loss [ 0.7266], quantization loss [ 0.0413],  0.53 sec/batch.
2022-10-19 00:46:10,785 step [1050], lr [0.0000375], embedding loss [ 0.7249], quantization loss [ 0.0358],  0.57 sec/batch.
2022-10-19 00:46:12,996 step [1051], lr [0.0000375], embedding loss [ 0.7313], quantization loss [ 0.0378],  0.53 sec/batch.
2022-10-19 00:46:15,103 step [1052], lr [0.0000375], embedding loss [ 0.7152], quantization loss [ 0.0362],  0.53 sec/batch.
2022-10-19 00:46:17,238 step [1053], lr [0.0000375], embedding loss [ 0.7316], quantization loss [ 0.0390],  0.53 sec/batch.
2022-10-19 00:46:19,338 step [1054], lr [0.0000375], embedding loss [ 0.7312], quantization loss [ 0.0349],  0.52 sec/batch.
2022-10-19 00:46:21,410 step [1055], lr [0.0000375], embedding loss [ 0.7260], quantization loss [ 0.0346],  0.53 sec/batch.
2022-10-19 00:46:23,519 step [1056], lr [0.0000375], embedding loss [ 0.7303], quantization loss [ 0.0328],  0.52 sec/batch.
2022-10-19 00:46:25,675 step [1057], lr [0.0000375], embedding loss [ 0.7207], quantization loss [ 0.0333],  0.53 sec/batch.
2022-10-19 00:46:27,805 step [1058], lr [0.0000375], embedding loss [ 0.7237], quantization loss [ 0.0324],  0.53 sec/batch.
2022-10-19 00:46:29,900 step [1059], lr [0.0000375], embedding loss [ 0.7339], quantization loss [ 0.0368],  0.52 sec/batch.
2022-10-19 00:46:32,059 step [1060], lr [0.0000375], embedding loss [ 0.7223], quantization loss [ 0.0331],  0.53 sec/batch.
2022-10-19 00:46:34,204 step [1061], lr [0.0000375], embedding loss [ 0.7367], quantization loss [ 0.0415],  0.53 sec/batch.
2022-10-19 00:46:36,349 step [1062], lr [0.0000375], embedding loss [ 0.7328], quantization loss [ 0.0346],  0.53 sec/batch.
2022-10-19 00:46:38,473 step [1063], lr [0.0000375], embedding loss [ 0.7142], quantization loss [ 0.0392],  0.52 sec/batch.
2022-10-19 00:46:40,617 step [1064], lr [0.0000375], embedding loss [ 0.7369], quantization loss [ 0.0337],  0.53 sec/batch.
2022-10-19 00:46:42,751 step [1065], lr [0.0000375], embedding loss [ 0.7205], quantization loss [ 0.0311],  0.53 sec/batch.
2022-10-19 00:46:44,902 step [1066], lr [0.0000375], embedding loss [ 0.7226], quantization loss [ 0.0384],  0.53 sec/batch.
2022-10-19 00:46:46,993 step [1067], lr [0.0000375], embedding loss [ 0.7283], quantization loss [ 0.0361],  0.53 sec/batch.
2022-10-19 00:46:49,077 step [1068], lr [0.0000375], embedding loss [ 0.7270], quantization loss [ 0.0366],  0.52 sec/batch.
2022-10-19 00:46:51,159 step [1069], lr [0.0000375], embedding loss [ 0.7403], quantization loss [ 0.0328],  0.53 sec/batch.
2022-10-19 00:46:53,312 step [1070], lr [0.0000375], embedding loss [ 0.7252], quantization loss [ 0.0348],  0.53 sec/batch.
2022-10-19 00:46:55,374 step [1071], lr [0.0000375], embedding loss [ 0.7443], quantization loss [ 0.0348],  0.51 sec/batch.
2022-10-19 00:46:57,443 step [1072], lr [0.0000375], embedding loss [ 0.7370], quantization loss [ 0.0331],  0.52 sec/batch.
2022-10-19 00:46:59,553 step [1073], lr [0.0000375], embedding loss [ 0.7312], quantization loss [ 0.0335],  0.53 sec/batch.
2022-10-19 00:47:01,721 step [1074], lr [0.0000375], embedding loss [ 0.7217], quantization loss [ 0.0347],  0.53 sec/batch.
2022-10-19 00:47:03,896 step [1075], lr [0.0000375], embedding loss [ 0.7366], quantization loss [ 0.0325],  0.53 sec/batch.
2022-10-19 00:47:05,987 step [1076], lr [0.0000375], embedding loss [ 0.7210], quantization loss [ 0.0361],  0.52 sec/batch.
2022-10-19 00:47:08,089 step [1077], lr [0.0000375], embedding loss [ 0.7277], quantization loss [ 0.0348],  0.53 sec/batch.
2022-10-19 00:47:10,231 step [1078], lr [0.0000375], embedding loss [ 0.7264], quantization loss [ 0.0333],  0.53 sec/batch.
2022-10-19 00:47:12,343 step [1079], lr [0.0000375], embedding loss [ 0.7235], quantization loss [ 0.0348],  0.51 sec/batch.
2022-10-19 00:47:14,459 step [1080], lr [0.0000375], embedding loss [ 0.7194], quantization loss [ 0.0326],  0.53 sec/batch.
2022-10-19 00:47:16,568 step [1081], lr [0.0000375], embedding loss [ 0.7275], quantization loss [ 0.0355],  0.53 sec/batch.
2022-10-19 00:47:18,699 step [1082], lr [0.0000375], embedding loss [ 0.7316], quantization loss [ 0.0342],  0.53 sec/batch.
2022-10-19 00:47:20,846 step [1083], lr [0.0000375], embedding loss [ 0.7292], quantization loss [ 0.0350],  0.53 sec/batch.
2022-10-19 00:47:22,972 step [1084], lr [0.0000375], embedding loss [ 0.7127], quantization loss [ 0.0355],  0.52 sec/batch.
2022-10-19 00:47:25,057 step [1085], lr [0.0000375], embedding loss [ 0.7156], quantization loss [ 0.0340],  0.53 sec/batch.
2022-10-19 00:47:27,160 step [1086], lr [0.0000375], embedding loss [ 0.7230], quantization loss [ 0.0346],  0.53 sec/batch.
2022-10-19 00:47:29,288 step [1087], lr [0.0000375], embedding loss [ 0.7283], quantization loss [ 0.0317],  0.51 sec/batch.
2022-10-19 00:47:31,386 step [1088], lr [0.0000375], embedding loss [ 0.7176], quantization loss [ 0.0327],  0.53 sec/batch.
2022-10-19 00:47:33,477 step [1089], lr [0.0000375], embedding loss [ 0.7315], quantization loss [ 0.0365],  0.53 sec/batch.
2022-10-19 00:47:35,572 step [1090], lr [0.0000375], embedding loss [ 0.7276], quantization loss [ 0.0343],  0.53 sec/batch.
2022-10-19 00:47:37,707 step [1091], lr [0.0000375], embedding loss [ 0.7339], quantization loss [ 0.0333],  0.53 sec/batch.
2022-10-19 00:47:39,799 step [1092], lr [0.0000375], embedding loss [ 0.7218], quantization loss [ 0.0383],  0.52 sec/batch.
2022-10-19 00:47:41,887 step [1093], lr [0.0000375], embedding loss [ 0.7294], quantization loss [ 0.0355],  0.52 sec/batch.
2022-10-19 00:47:43,957 step [1094], lr [0.0000375], embedding loss [ 0.7194], quantization loss [ 0.0363],  0.52 sec/batch.
2022-10-19 00:47:46,033 step [1095], lr [0.0000375], embedding loss [ 0.7265], quantization loss [ 0.0358],  0.52 sec/batch.
2022-10-19 00:47:48,176 step [1096], lr [0.0000375], embedding loss [ 0.7249], quantization loss [ 0.0358],  0.53 sec/batch.
2022-10-19 00:47:50,265 step [1097], lr [0.0000375], embedding loss [ 0.7287], quantization loss [ 0.0312],  0.53 sec/batch.
2022-10-19 00:47:52,420 step [1098], lr [0.0000375], embedding loss [ 0.7170], quantization loss [ 0.0337],  0.53 sec/batch.
2022-10-19 00:47:54,550 step [1099], lr [0.0000375], embedding loss [ 0.7353], quantization loss [ 0.0328],  0.53 sec/batch.
2022-10-19 00:47:56,700 step [1100], lr [0.0000375], embedding loss [ 0.7339], quantization loss [ 0.0375],  0.52 sec/batch.
2022-10-19 00:47:58,778 step [1101], lr [0.0000375], embedding loss [ 0.7255], quantization loss [ 0.0386],  0.52 sec/batch.
2022-10-19 00:48:00,887 step [1102], lr [0.0000375], embedding loss [ 0.7250], quantization loss [ 0.0344],  0.53 sec/batch.
2022-10-19 00:48:02,996 step [1103], lr [0.0000375], embedding loss [ 0.7417], quantization loss [ 0.0341],  0.52 sec/batch.
2022-10-19 00:48:05,166 step [1104], lr [0.0000375], embedding loss [ 0.7249], quantization loss [ 0.0395],  0.53 sec/batch.
2022-10-19 00:48:07,260 step [1105], lr [0.0000375], embedding loss [ 0.7197], quantization loss [ 0.0326],  0.52 sec/batch.
2022-10-19 00:48:09,314 step [1106], lr [0.0000375], embedding loss [ 0.7258], quantization loss [ 0.0351],  0.51 sec/batch.
2022-10-19 00:48:11,415 step [1107], lr [0.0000375], embedding loss [ 0.7223], quantization loss [ 0.0360],  0.53 sec/batch.
2022-10-19 00:48:13,609 step [1108], lr [0.0000375], embedding loss [ 0.7278], quantization loss [ 0.0352],  0.53 sec/batch.
2022-10-19 00:48:15,757 step [1109], lr [0.0000375], embedding loss [ 0.7295], quantization loss [ 0.0284],  0.53 sec/batch.
2022-10-19 00:48:17,917 step [1110], lr [0.0000375], embedding loss [ 0.7263], quantization loss [ 0.0367],  0.53 sec/batch.
2022-10-19 00:48:20,015 step [1111], lr [0.0000375], embedding loss [ 0.7216], quantization loss [ 0.0376],  0.53 sec/batch.
2022-10-19 00:48:22,110 step [1112], lr [0.0000375], embedding loss [ 0.7262], quantization loss [ 0.0329],  0.53 sec/batch.
2022-10-19 00:48:24,206 step [1113], lr [0.0000375], embedding loss [ 0.7252], quantization loss [ 0.0315],  0.53 sec/batch.
2022-10-19 00:48:26,313 step [1114], lr [0.0000375], embedding loss [ 0.7378], quantization loss [ 0.0371],  0.52 sec/batch.
2022-10-19 00:48:28,409 step [1115], lr [0.0000375], embedding loss [ 0.7280], quantization loss [ 0.0348],  0.53 sec/batch.
2022-10-19 00:48:30,503 step [1116], lr [0.0000375], embedding loss [ 0.7217], quantization loss [ 0.0352],  0.53 sec/batch.
2022-10-19 00:48:32,601 step [1117], lr [0.0000375], embedding loss [ 0.7268], quantization loss [ 0.0325],  0.53 sec/batch.
2022-10-19 00:48:34,708 step [1118], lr [0.0000375], embedding loss [ 0.7272], quantization loss [ 0.0338],  0.53 sec/batch.
2022-10-19 00:48:36,814 step [1119], lr [0.0000375], embedding loss [ 0.7247], quantization loss [ 0.0345],  0.53 sec/batch.
2022-10-19 00:48:38,947 step [1120], lr [0.0000375], embedding loss [ 0.7200], quantization loss [ 0.0335],  0.52 sec/batch.
2022-10-19 00:48:41,090 step [1121], lr [0.0000375], embedding loss [ 0.7203], quantization loss [ 0.0362],  0.51 sec/batch.
2022-10-19 00:48:41,091 update codes and centers iter(1/1).
2022-10-19 00:48:42,680 number of update_code wrong: 0.
2022-10-19 00:48:46,114 non zero codewords: 512.
2022-10-19 00:48:46,114 finish center update, duration: 5.02 sec.
2022-10-19 00:48:48,215 step [1122], lr [0.0000375], embedding loss [ 0.7276], quantization loss [ 0.0394],  0.54 sec/batch.
2022-10-19 00:48:50,338 step [1123], lr [0.0000375], embedding loss [ 0.7292], quantization loss [ 0.0332],  0.53 sec/batch.
2022-10-19 00:48:52,514 step [1124], lr [0.0000375], embedding loss [ 0.7244], quantization loss [ 0.0338],  0.53 sec/batch.
2022-10-19 00:48:54,676 step [1125], lr [0.0000375], embedding loss [ 0.7347], quantization loss [ 0.0323],  0.53 sec/batch.
2022-10-19 00:48:56,924 step [1126], lr [0.0000375], embedding loss [ 0.7215], quantization loss [ 0.0364],  0.53 sec/batch.
2022-10-19 00:48:59,030 step [1127], lr [0.0000375], embedding loss [ 0.7188], quantization loss [ 0.0322],  0.53 sec/batch.
2022-10-19 00:49:01,139 step [1128], lr [0.0000375], embedding loss [ 0.7176], quantization loss [ 0.0338],  0.53 sec/batch.
2022-10-19 00:49:03,246 step [1129], lr [0.0000375], embedding loss [ 0.7194], quantization loss [ 0.0367],  0.53 sec/batch.
2022-10-19 00:49:05,419 step [1130], lr [0.0000375], embedding loss [ 0.7255], quantization loss [ 0.0349],  0.53 sec/batch.
2022-10-19 00:49:07,549 step [1131], lr [0.0000375], embedding loss [ 0.7268], quantization loss [ 0.0370],  0.53 sec/batch.
2022-10-19 00:49:09,778 step [1132], lr [0.0000375], embedding loss [ 0.7307], quantization loss [ 0.0347],  0.53 sec/batch.
2022-10-19 00:49:12,000 step [1133], lr [0.0000375], embedding loss [ 0.7279], quantization loss [ 0.0358],  0.53 sec/batch.
2022-10-19 00:49:14,151 step [1134], lr [0.0000375], embedding loss [ 0.7187], quantization loss [ 0.0338],  0.52 sec/batch.
2022-10-19 00:49:16,333 step [1135], lr [0.0000375], embedding loss [ 0.7147], quantization loss [ 0.0362],  0.53 sec/batch.
2022-10-19 00:49:18,483 step [1136], lr [0.0000375], embedding loss [ 0.7259], quantization loss [ 0.0320],  0.52 sec/batch.
2022-10-19 00:49:20,555 step [1137], lr [0.0000375], embedding loss [ 0.7253], quantization loss [ 0.0354],  0.51 sec/batch.
2022-10-19 00:49:22,637 step [1138], lr [0.0000375], embedding loss [ 0.7208], quantization loss [ 0.0327],  0.51 sec/batch.
2022-10-19 00:49:24,773 step [1139], lr [0.0000375], embedding loss [ 0.7178], quantization loss [ 0.0354],  0.53 sec/batch.
2022-10-19 00:49:26,915 step [1140], lr [0.0000375], embedding loss [ 0.7236], quantization loss [ 0.0365],  0.53 sec/batch.
2022-10-19 00:49:29,082 step [1141], lr [0.0000375], embedding loss [ 0.7318], quantization loss [ 0.0348],  0.52 sec/batch.
2022-10-19 00:49:31,222 step [1142], lr [0.0000375], embedding loss [ 0.7365], quantization loss [ 0.0346],  0.52 sec/batch.
2022-10-19 00:49:33,498 step [1143], lr [0.0000375], embedding loss [ 0.7281], quantization loss [ 0.0358],  0.54 sec/batch.
2022-10-19 00:49:35,615 step [1144], lr [0.0000375], embedding loss [ 0.7283], quantization loss [ 0.0328],  0.53 sec/batch.
2022-10-19 00:49:37,814 step [1145], lr [0.0000375], embedding loss [ 0.7229], quantization loss [ 0.0379],  0.53 sec/batch.
2022-10-19 00:49:39,990 step [1146], lr [0.0000375], embedding loss [ 0.7220], quantization loss [ 0.0337],  0.53 sec/batch.
2022-10-19 00:49:42,152 step [1147], lr [0.0000375], embedding loss [ 0.7269], quantization loss [ 0.0383],  0.53 sec/batch.
2022-10-19 00:49:44,319 step [1148], lr [0.0000375], embedding loss [ 0.7236], quantization loss [ 0.0326],  0.53 sec/batch.
2022-10-19 00:49:46,430 step [1149], lr [0.0000375], embedding loss [ 0.7213], quantization loss [ 0.0315],  0.53 sec/batch.
2022-10-19 00:49:48,605 step [1150], lr [0.0000375], embedding loss [ 0.7311], quantization loss [ 0.0385],  0.54 sec/batch.
2022-10-19 00:49:50,820 step [1151], lr [0.0000375], embedding loss [ 0.7221], quantization loss [ 0.0317],  0.53 sec/batch.
2022-10-19 00:49:53,004 step [1152], lr [0.0000375], embedding loss [ 0.7290], quantization loss [ 0.0377],  0.53 sec/batch.
2022-10-19 00:49:55,179 step [1153], lr [0.0000375], embedding loss [ 0.7229], quantization loss [ 0.0295],  0.53 sec/batch.
2022-10-19 00:49:57,392 step [1154], lr [0.0000375], embedding loss [ 0.7321], quantization loss [ 0.0382],  0.53 sec/batch.
2022-10-19 00:49:59,524 step [1155], lr [0.0000375], embedding loss [ 0.7321], quantization loss [ 0.0347],  0.51 sec/batch.
2022-10-19 00:50:01,605 step [1156], lr [0.0000375], embedding loss [ 0.7339], quantization loss [ 0.0326],  0.52 sec/batch.
2022-10-19 00:50:03,713 step [1157], lr [0.0000375], embedding loss [ 0.7280], quantization loss [ 0.0363],  0.52 sec/batch.
2022-10-19 00:50:05,804 step [1158], lr [0.0000375], embedding loss [ 0.7246], quantization loss [ 0.0351],  0.52 sec/batch.
2022-10-19 00:50:07,871 step [1159], lr [0.0000375], embedding loss [ 0.7293], quantization loss [ 0.0365],  0.51 sec/batch.
2022-10-19 00:50:09,999 step [1160], lr [0.0000375], embedding loss [ 0.7256], quantization loss [ 0.0358],  0.53 sec/batch.
2022-10-19 00:50:12,122 step [1161], lr [0.0000375], embedding loss [ 0.7232], quantization loss [ 0.0347],  0.52 sec/batch.
2022-10-19 00:50:14,218 step [1162], lr [0.0000375], embedding loss [ 0.7215], quantization loss [ 0.0335],  0.50 sec/batch.
2022-10-19 00:50:16,296 step [1163], lr [0.0000375], embedding loss [ 0.7317], quantization loss [ 0.0350],  0.51 sec/batch.
2022-10-19 00:50:18,369 step [1164], lr [0.0000375], embedding loss [ 0.7234], quantization loss [ 0.0312],  0.50 sec/batch.
2022-10-19 00:50:20,434 step [1165], lr [0.0000375], embedding loss [ 0.7352], quantization loss [ 0.0327],  0.51 sec/batch.
2022-10-19 00:50:22,526 step [1166], lr [0.0000375], embedding loss [ 0.7249], quantization loss [ 0.0334],  0.52 sec/batch.
2022-10-19 00:50:24,694 step [1167], lr [0.0000375], embedding loss [ 0.7243], quantization loss [ 0.0356],  0.53 sec/batch.
2022-10-19 00:50:26,802 step [1168], lr [0.0000375], embedding loss [ 0.7263], quantization loss [ 0.0324],  0.53 sec/batch.
2022-10-19 00:50:28,910 step [1169], lr [0.0000375], embedding loss [ 0.7180], quantization loss [ 0.0326],  0.51 sec/batch.
2022-10-19 00:50:31,041 step [1170], lr [0.0000375], embedding loss [ 0.7289], quantization loss [ 0.0353],  0.52 sec/batch.
2022-10-19 00:50:33,215 step [1171], lr [0.0000375], embedding loss [ 0.7274], quantization loss [ 0.0337],  0.53 sec/batch.
2022-10-19 00:50:35,339 step [1172], lr [0.0000375], embedding loss [ 0.7284], quantization loss [ 0.0298],  0.53 sec/batch.
2022-10-19 00:50:37,452 step [1173], lr [0.0000375], embedding loss [ 0.7251], quantization loss [ 0.0313],  0.52 sec/batch.
2022-10-19 00:50:39,560 step [1174], lr [0.0000375], embedding loss [ 0.7248], quantization loss [ 0.0350],  0.52 sec/batch.
2022-10-19 00:50:41,741 step [1175], lr [0.0000375], embedding loss [ 0.7286], quantization loss [ 0.0397],  0.53 sec/batch.
2022-10-19 00:50:43,908 step [1176], lr [0.0000375], embedding loss [ 0.7108], quantization loss [ 0.0325],  0.51 sec/batch.
2022-10-19 00:50:46,085 step [1177], lr [0.0000375], embedding loss [ 0.7377], quantization loss [ 0.0288],  0.52 sec/batch.
2022-10-19 00:50:48,270 step [1178], lr [0.0000375], embedding loss [ 0.7166], quantization loss [ 0.0348],  0.53 sec/batch.
2022-10-19 00:50:50,383 step [1179], lr [0.0000375], embedding loss [ 0.7370], quantization loss [ 0.0330],  0.53 sec/batch.
2022-10-19 00:50:52,494 step [1180], lr [0.0000375], embedding loss [ 0.7424], quantization loss [ 0.0334],  0.53 sec/batch.
2022-10-19 00:50:54,577 step [1181], lr [0.0000375], embedding loss [ 0.7260], quantization loss [ 0.0319],  0.52 sec/batch.
2022-10-19 00:50:56,746 step [1182], lr [0.0000375], embedding loss [ 0.7131], quantization loss [ 0.0362],  0.53 sec/batch.
2022-10-19 00:50:58,864 step [1183], lr [0.0000375], embedding loss [ 0.7351], quantization loss [ 0.0405],  0.52 sec/batch.
2022-10-19 00:51:01,027 step [1184], lr [0.0000375], embedding loss [ 0.7277], quantization loss [ 0.0357],  0.53 sec/batch.
2022-10-19 00:51:03,149 step [1185], lr [0.0000375], embedding loss [ 0.7226], quantization loss [ 0.0337],  0.52 sec/batch.
2022-10-19 00:51:05,263 step [1186], lr [0.0000375], embedding loss [ 0.7154], quantization loss [ 0.0335],  0.53 sec/batch.
2022-10-19 00:51:07,376 step [1187], lr [0.0000375], embedding loss [ 0.7235], quantization loss [ 0.0322],  0.52 sec/batch.
2022-10-19 00:51:09,484 step [1188], lr [0.0000375], embedding loss [ 0.7248], quantization loss [ 0.0329],  0.53 sec/batch.
2022-10-19 00:51:11,590 step [1189], lr [0.0000375], embedding loss [ 0.7210], quantization loss [ 0.0366],  0.53 sec/batch.
2022-10-19 00:51:13,699 step [1190], lr [0.0000375], embedding loss [ 0.7227], quantization loss [ 0.0331],  0.52 sec/batch.
2022-10-19 00:51:15,967 step [1191], lr [0.0000375], embedding loss [ 0.7266], quantization loss [ 0.0312],  0.53 sec/batch.
2022-10-19 00:51:18,059 step [1192], lr [0.0000375], embedding loss [ 0.7268], quantization loss [ 0.0333],  0.53 sec/batch.
2022-10-19 00:51:20,246 step [1193], lr [0.0000375], embedding loss [ 0.7216], quantization loss [ 0.0377],  0.53 sec/batch.
2022-10-19 00:51:22,346 step [1194], lr [0.0000375], embedding loss [ 0.7142], quantization loss [ 0.0349],  0.52 sec/batch.
2022-10-19 00:51:24,502 step [1195], lr [0.0000375], embedding loss [ 0.7287], quantization loss [ 0.0340],  0.52 sec/batch.
2022-10-19 00:51:26,627 step [1196], lr [0.0000375], embedding loss [ 0.7398], quantization loss [ 0.0363],  0.52 sec/batch.
2022-10-19 00:51:28,725 step [1197], lr [0.0000375], embedding loss [ 0.7109], quantization loss [ 0.0333],  0.53 sec/batch.
2022-10-19 00:51:30,812 step [1198], lr [0.0000375], embedding loss [ 0.7163], quantization loss [ 0.0373],  0.52 sec/batch.
2022-10-19 00:51:32,992 step [1199], lr [0.0000375], embedding loss [ 0.7233], quantization loss [ 0.0379],  0.53 sec/batch.
2022-10-19 00:51:35,130 step [1200], lr [0.0000375], embedding loss [ 0.7293], quantization loss [ 0.0313],  0.53 sec/batch.
2022-10-19 00:51:37,283 step [1201], lr [0.0000188], embedding loss [ 0.7266], quantization loss [ 0.0336],  0.51 sec/batch.
2022-10-19 00:51:37,284 update codes and centers iter(1/1).
2022-10-19 00:51:38,905 number of update_code wrong: 0.
2022-10-19 00:51:42,400 non zero codewords: 512.
2022-10-19 00:51:42,400 finish center update, duration: 5.12 sec.
2022-10-19 00:51:44,599 step [1202], lr [0.0000188], embedding loss [ 0.7184], quantization loss [ 0.0323],  0.54 sec/batch.
2022-10-19 00:51:46,753 step [1203], lr [0.0000188], embedding loss [ 0.7066], quantization loss [ 0.0304],  0.53 sec/batch.
2022-10-19 00:51:49,003 step [1204], lr [0.0000188], embedding loss [ 0.7176], quantization loss [ 0.0342],  0.57 sec/batch.
2022-10-19 00:51:51,143 step [1205], lr [0.0000188], embedding loss [ 0.7163], quantization loss [ 0.0324],  0.52 sec/batch.
2022-10-19 00:51:53,359 step [1206], lr [0.0000188], embedding loss [ 0.7155], quantization loss [ 0.0330],  0.53 sec/batch.
2022-10-19 00:51:55,477 step [1207], lr [0.0000188], embedding loss [ 0.7427], quantization loss [ 0.0378],  0.52 sec/batch.
2022-10-19 00:51:57,592 step [1208], lr [0.0000188], embedding loss [ 0.7364], quantization loss [ 0.0333],  0.52 sec/batch.
2022-10-19 00:51:59,792 step [1209], lr [0.0000188], embedding loss [ 0.7404], quantization loss [ 0.0356],  0.53 sec/batch.
2022-10-19 00:52:01,954 step [1210], lr [0.0000188], embedding loss [ 0.7268], quantization loss [ 0.0326],  0.53 sec/batch.
2022-10-19 00:52:04,156 step [1211], lr [0.0000188], embedding loss [ 0.7316], quantization loss [ 0.0362],  0.53 sec/batch.
2022-10-19 00:52:06,321 step [1212], lr [0.0000188], embedding loss [ 0.7222], quantization loss [ 0.0389],  0.53 sec/batch.
2022-10-19 00:52:08,516 step [1213], lr [0.0000188], embedding loss [ 0.7136], quantization loss [ 0.0367],  0.53 sec/batch.
2022-10-19 00:52:10,663 step [1214], lr [0.0000188], embedding loss [ 0.7284], quantization loss [ 0.0351],  0.52 sec/batch.
2022-10-19 00:52:12,846 step [1215], lr [0.0000188], embedding loss [ 0.7289], quantization loss [ 0.0358],  0.53 sec/batch.
2022-10-19 00:52:15,019 step [1216], lr [0.0000188], embedding loss [ 0.7265], quantization loss [ 0.0359],  0.53 sec/batch.
2022-10-19 00:52:17,204 step [1217], lr [0.0000188], embedding loss [ 0.7235], quantization loss [ 0.0334],  0.51 sec/batch.
2022-10-19 00:52:19,331 step [1218], lr [0.0000188], embedding loss [ 0.7304], quantization loss [ 0.0370],  0.51 sec/batch.
2022-10-19 00:52:21,449 step [1219], lr [0.0000188], embedding loss [ 0.7097], quantization loss [ 0.0315],  0.53 sec/batch.
2022-10-19 00:52:23,526 step [1220], lr [0.0000188], embedding loss [ 0.7271], quantization loss [ 0.0316],  0.52 sec/batch.
2022-10-19 00:52:25,641 step [1221], lr [0.0000188], embedding loss [ 0.7274], quantization loss [ 0.0336],  0.53 sec/batch.
2022-10-19 00:52:27,790 step [1222], lr [0.0000188], embedding loss [ 0.7279], quantization loss [ 0.0344],  0.52 sec/batch.
2022-10-19 00:52:29,906 step [1223], lr [0.0000188], embedding loss [ 0.7279], quantization loss [ 0.0327],  0.52 sec/batch.
2022-10-19 00:52:32,020 step [1224], lr [0.0000188], embedding loss [ 0.7305], quantization loss [ 0.0381],  0.52 sec/batch.
2022-10-19 00:52:34,118 step [1225], lr [0.0000188], embedding loss [ 0.7088], quantization loss [ 0.0323],  0.52 sec/batch.
2022-10-19 00:52:36,298 step [1226], lr [0.0000188], embedding loss [ 0.7268], quantization loss [ 0.0339],  0.53 sec/batch.
2022-10-19 00:52:38,455 step [1227], lr [0.0000188], embedding loss [ 0.7302], quantization loss [ 0.0377],  0.52 sec/batch.
2022-10-19 00:52:40,615 step [1228], lr [0.0000188], embedding loss [ 0.7297], quantization loss [ 0.0391],  0.51 sec/batch.
2022-10-19 00:52:42,706 step [1229], lr [0.0000188], embedding loss [ 0.7331], quantization loss [ 0.0336],  0.51 sec/batch.
2022-10-19 00:52:44,801 step [1230], lr [0.0000188], embedding loss [ 0.7303], quantization loss [ 0.0342],  0.52 sec/batch.
2022-10-19 00:52:46,879 step [1231], lr [0.0000188], embedding loss [ 0.7296], quantization loss [ 0.0333],  0.52 sec/batch.
2022-10-19 00:52:48,983 step [1232], lr [0.0000188], embedding loss [ 0.7255], quantization loss [ 0.0349],  0.52 sec/batch.
2022-10-19 00:52:51,119 step [1233], lr [0.0000188], embedding loss [ 0.7136], quantization loss [ 0.0336],  0.53 sec/batch.
2022-10-19 00:52:53,244 step [1234], lr [0.0000188], embedding loss [ 0.7277], quantization loss [ 0.0356],  0.51 sec/batch.
2022-10-19 00:52:55,377 step [1235], lr [0.0000188], embedding loss [ 0.7195], quantization loss [ 0.0366],  0.52 sec/batch.
2022-10-19 00:52:57,553 step [1236], lr [0.0000188], embedding loss [ 0.7198], quantization loss [ 0.0333],  0.53 sec/batch.
2022-10-19 00:52:59,675 step [1237], lr [0.0000188], embedding loss [ 0.7233], quantization loss [ 0.0329],  0.53 sec/batch.
2022-10-19 00:53:01,861 step [1238], lr [0.0000188], embedding loss [ 0.7132], quantization loss [ 0.0360],  0.53 sec/batch.
2022-10-19 00:53:04,083 step [1239], lr [0.0000188], embedding loss [ 0.7190], quantization loss [ 0.0329],  0.53 sec/batch.
2022-10-19 00:53:06,239 step [1240], lr [0.0000188], embedding loss [ 0.7261], quantization loss [ 0.0341],  0.53 sec/batch.
2022-10-19 00:53:08,360 step [1241], lr [0.0000188], embedding loss [ 0.7223], quantization loss [ 0.0335],  0.53 sec/batch.
2022-10-19 00:53:10,496 step [1242], lr [0.0000188], embedding loss [ 0.7185], quantization loss [ 0.0321],  0.52 sec/batch.
2022-10-19 00:53:12,597 step [1243], lr [0.0000188], embedding loss [ 0.7236], quantization loss [ 0.0280],  0.51 sec/batch.
2022-10-19 00:53:14,707 step [1244], lr [0.0000188], embedding loss [ 0.7336], quantization loss [ 0.0340],  0.54 sec/batch.
2022-10-19 00:53:16,797 step [1245], lr [0.0000188], embedding loss [ 0.7409], quantization loss [ 0.0342],  0.56 sec/batch.
2022-10-19 00:53:18,829 step [1246], lr [0.0000188], embedding loss [ 0.7132], quantization loss [ 0.0315],  0.54 sec/batch.
2022-10-19 00:53:20,964 step [1247], lr [0.0000188], embedding loss [ 0.7367], quantization loss [ 0.0308],  0.52 sec/batch.
2022-10-19 00:53:23,095 step [1248], lr [0.0000188], embedding loss [ 0.7257], quantization loss [ 0.0333],  0.53 sec/batch.
2022-10-19 00:53:25,240 step [1249], lr [0.0000188], embedding loss [ 0.7124], quantization loss [ 0.0344],  0.53 sec/batch.
2022-10-19 00:53:27,512 step [1250], lr [0.0000188], embedding loss [ 0.7216], quantization loss [ 0.0332],  0.55 sec/batch.
2022-10-19 00:53:29,712 step [1251], lr [0.0000188], embedding loss [ 0.7270], quantization loss [ 0.0351],  0.53 sec/batch.
2022-10-19 00:53:31,886 step [1252], lr [0.0000188], embedding loss [ 0.7143], quantization loss [ 0.0335],  0.52 sec/batch.
2022-10-19 00:53:34,016 step [1253], lr [0.0000188], embedding loss [ 0.7141], quantization loss [ 0.0287],  0.53 sec/batch.
2022-10-19 00:53:36,124 step [1254], lr [0.0000188], embedding loss [ 0.7292], quantization loss [ 0.0343],  0.51 sec/batch.
2022-10-19 00:53:38,288 step [1255], lr [0.0000188], embedding loss [ 0.7330], quantization loss [ 0.0400],  0.51 sec/batch.
2022-10-19 00:53:40,482 step [1256], lr [0.0000188], embedding loss [ 0.7232], quantization loss [ 0.0312],  0.56 sec/batch.
2022-10-19 00:53:42,645 step [1257], lr [0.0000188], embedding loss [ 0.7265], quantization loss [ 0.0353],  0.52 sec/batch.
2022-10-19 00:53:44,763 step [1258], lr [0.0000188], embedding loss [ 0.7234], quantization loss [ 0.0324],  0.53 sec/batch.
2022-10-19 00:53:46,918 step [1259], lr [0.0000188], embedding loss [ 0.7407], quantization loss [ 0.0324],  0.53 sec/batch.
2022-10-19 00:53:49,034 step [1260], lr [0.0000188], embedding loss [ 0.7250], quantization loss [ 0.0375],  0.52 sec/batch.
2022-10-19 00:53:51,124 step [1261], lr [0.0000188], embedding loss [ 0.7248], quantization loss [ 0.0295],  0.50 sec/batch.
2022-10-19 00:53:53,226 step [1262], lr [0.0000188], embedding loss [ 0.7111], quantization loss [ 0.0287],  0.52 sec/batch.
2022-10-19 00:53:55,391 step [1263], lr [0.0000188], embedding loss [ 0.7274], quantization loss [ 0.0284],  0.52 sec/batch.
2022-10-19 00:53:57,561 step [1264], lr [0.0000188], embedding loss [ 0.7244], quantization loss [ 0.0339],  0.53 sec/batch.
2022-10-19 00:53:59,702 step [1265], lr [0.0000188], embedding loss [ 0.7297], quantization loss [ 0.0345],  0.51 sec/batch.
2022-10-19 00:54:01,828 step [1266], lr [0.0000188], embedding loss [ 0.7154], quantization loss [ 0.0339],  0.52 sec/batch.
2022-10-19 00:54:03,982 step [1267], lr [0.0000188], embedding loss [ 0.7185], quantization loss [ 0.0343],  0.53 sec/batch.
2022-10-19 00:54:06,139 step [1268], lr [0.0000188], embedding loss [ 0.7283], quantization loss [ 0.0293],  0.51 sec/batch.
2022-10-19 00:54:08,246 step [1269], lr [0.0000188], embedding loss [ 0.7373], quantization loss [ 0.0313],  0.53 sec/batch.
2022-10-19 00:54:10,401 step [1270], lr [0.0000188], embedding loss [ 0.7165], quantization loss [ 0.0313],  0.51 sec/batch.
2022-10-19 00:54:12,530 step [1271], lr [0.0000188], embedding loss [ 0.7280], quantization loss [ 0.0322],  0.53 sec/batch.
2022-10-19 00:54:14,682 step [1272], lr [0.0000188], embedding loss [ 0.7324], quantization loss [ 0.0332],  0.52 sec/batch.
2022-10-19 00:54:16,791 step [1273], lr [0.0000188], embedding loss [ 0.7223], quantization loss [ 0.0325],  0.52 sec/batch.
2022-10-19 00:54:18,911 step [1274], lr [0.0000188], embedding loss [ 0.7240], quantization loss [ 0.0319],  0.52 sec/batch.
2022-10-19 00:54:21,086 step [1275], lr [0.0000188], embedding loss [ 0.7149], quantization loss [ 0.0344],  0.52 sec/batch.
2022-10-19 00:54:23,203 step [1276], lr [0.0000188], embedding loss [ 0.7394], quantization loss [ 0.0335],  0.52 sec/batch.
2022-10-19 00:54:25,358 step [1277], lr [0.0000188], embedding loss [ 0.7118], quantization loss [ 0.0341],  0.50 sec/batch.
2022-10-19 00:54:27,462 step [1278], lr [0.0000188], embedding loss [ 0.7090], quantization loss [ 0.0357],  0.50 sec/batch.
2022-10-19 00:54:29,563 step [1279], lr [0.0000188], embedding loss [ 0.7181], quantization loss [ 0.0348],  0.52 sec/batch.
2022-10-19 00:54:31,708 step [1280], lr [0.0000188], embedding loss [ 0.7055], quantization loss [ 0.0334],  0.53 sec/batch.
2022-10-19 00:54:33,914 step [1281], lr [0.0000188], embedding loss [ 0.7389], quantization loss [ 0.0311],  0.53 sec/batch.
2022-10-19 00:54:33,914 update codes and centers iter(1/1).
2022-10-19 00:54:35,510 number of update_code wrong: 0.
2022-10-19 00:54:38,659 non zero codewords: 512.
2022-10-19 00:54:38,659 finish center update, duration: 4.75 sec.
2022-10-19 00:54:40,714 step [1282], lr [0.0000188], embedding loss [ 0.7244], quantization loss [ 0.0314],  0.53 sec/batch.
2022-10-19 00:54:42,826 step [1283], lr [0.0000188], embedding loss [ 0.7262], quantization loss [ 0.0319],  0.51 sec/batch.
2022-10-19 00:54:44,926 step [1284], lr [0.0000188], embedding loss [ 0.7159], quantization loss [ 0.0320],  0.51 sec/batch.
2022-10-19 00:54:47,036 step [1285], lr [0.0000188], embedding loss [ 0.7110], quantization loss [ 0.0313],  0.50 sec/batch.
2022-10-19 00:54:49,159 step [1286], lr [0.0000188], embedding loss [ 0.7316], quantization loss [ 0.0332],  0.53 sec/batch.
2022-10-19 00:54:51,296 step [1287], lr [0.0000188], embedding loss [ 0.7192], quantization loss [ 0.0316],  0.52 sec/batch.
2022-10-19 00:54:53,412 step [1288], lr [0.0000188], embedding loss [ 0.7070], quantization loss [ 0.0337],  0.52 sec/batch.
2022-10-19 00:54:55,475 step [1289], lr [0.0000188], embedding loss [ 0.7265], quantization loss [ 0.0325],  0.51 sec/batch.
2022-10-19 00:54:57,577 step [1290], lr [0.0000188], embedding loss [ 0.7232], quantization loss [ 0.0323],  0.52 sec/batch.
2022-10-19 00:54:59,598 step [1291], lr [0.0000188], embedding loss [ 0.7138], quantization loss [ 0.0336],  0.50 sec/batch.
2022-10-19 00:55:01,713 step [1292], lr [0.0000188], embedding loss [ 0.7317], quantization loss [ 0.0315],  0.53 sec/batch.
2022-10-19 00:55:03,900 step [1293], lr [0.0000188], embedding loss [ 0.7362], quantization loss [ 0.0310],  0.53 sec/batch.
2022-10-19 00:55:06,026 step [1294], lr [0.0000188], embedding loss [ 0.7214], quantization loss [ 0.0336],  0.53 sec/batch.
2022-10-19 00:55:08,184 step [1295], lr [0.0000188], embedding loss [ 0.7149], quantization loss [ 0.0310],  0.52 sec/batch.
2022-10-19 00:55:10,293 step [1296], lr [0.0000188], embedding loss [ 0.7171], quantization loss [ 0.0323],  0.52 sec/batch.
2022-10-19 00:55:12,412 step [1297], lr [0.0000188], embedding loss [ 0.7200], quantization loss [ 0.0345],  0.53 sec/batch.
2022-10-19 00:55:14,551 step [1298], lr [0.0000188], embedding loss [ 0.7334], quantization loss [ 0.0319],  0.53 sec/batch.
2022-10-19 00:55:16,681 step [1299], lr [0.0000188], embedding loss [ 0.7254], quantization loss [ 0.0308],  0.53 sec/batch.
2022-10-19 00:55:18,791 step [1300], lr [0.0000188], embedding loss [ 0.7334], quantization loss [ 0.0345],  0.52 sec/batch.
2022-10-19 00:55:20,980 step [1301], lr [0.0000188], embedding loss [ 0.7142], quantization loss [ 0.0325],  0.53 sec/batch.
2022-10-19 00:55:23,110 step [1302], lr [0.0000188], embedding loss [ 0.7261], quantization loss [ 0.0333],  0.52 sec/batch.
2022-10-19 00:55:25,241 step [1303], lr [0.0000188], embedding loss [ 0.7149], quantization loss [ 0.0340],  0.53 sec/batch.
2022-10-19 00:55:27,397 step [1304], lr [0.0000188], embedding loss [ 0.7341], quantization loss [ 0.0318],  0.53 sec/batch.
2022-10-19 00:55:29,560 step [1305], lr [0.0000188], embedding loss [ 0.7212], quantization loss [ 0.0287],  0.53 sec/batch.
2022-10-19 00:55:31,706 step [1306], lr [0.0000188], embedding loss [ 0.7182], quantization loss [ 0.0318],  0.52 sec/batch.
2022-10-19 00:55:33,816 step [1307], lr [0.0000188], embedding loss [ 0.7354], quantization loss [ 0.0321],  0.51 sec/batch.
2022-10-19 00:55:35,931 step [1308], lr [0.0000188], embedding loss [ 0.7172], quantization loss [ 0.0359],  0.51 sec/batch.
2022-10-19 00:55:38,186 step [1309], lr [0.0000188], embedding loss [ 0.7308], quantization loss [ 0.0339],  0.52 sec/batch.
2022-10-19 00:55:40,292 step [1310], lr [0.0000188], embedding loss [ 0.7173], quantization loss [ 0.0355],  0.51 sec/batch.
2022-10-19 00:55:42,433 step [1311], lr [0.0000188], embedding loss [ 0.7151], quantization loss [ 0.0304],  0.51 sec/batch.
2022-10-19 00:55:44,558 step [1312], lr [0.0000188], embedding loss [ 0.7253], quantization loss [ 0.0304],  0.52 sec/batch.
2022-10-19 00:55:46,657 step [1313], lr [0.0000188], embedding loss [ 0.7198], quantization loss [ 0.0295],  0.52 sec/batch.
2022-10-19 00:55:48,829 step [1314], lr [0.0000188], embedding loss [ 0.7290], quantization loss [ 0.0313],  0.52 sec/batch.
2022-10-19 00:55:50,944 step [1315], lr [0.0000188], embedding loss [ 0.7152], quantization loss [ 0.0322],  0.52 sec/batch.
2022-10-19 00:55:53,094 step [1316], lr [0.0000188], embedding loss [ 0.7370], quantization loss [ 0.0326],  0.55 sec/batch.
2022-10-19 00:55:55,215 step [1317], lr [0.0000188], embedding loss [ 0.7235], quantization loss [ 0.0302],  0.51 sec/batch.
2022-10-19 00:55:57,406 step [1318], lr [0.0000188], embedding loss [ 0.7372], quantization loss [ 0.0334],  0.53 sec/batch.
2022-10-19 00:55:59,538 step [1319], lr [0.0000188], embedding loss [ 0.7291], quantization loss [ 0.0333],  0.52 sec/batch.
2022-10-19 00:56:01,715 step [1320], lr [0.0000188], embedding loss [ 0.7274], quantization loss [ 0.0315],  0.52 sec/batch.
2022-10-19 00:56:03,850 step [1321], lr [0.0000188], embedding loss [ 0.7168], quantization loss [ 0.0290],  0.51 sec/batch.
2022-10-19 00:56:06,089 step [1322], lr [0.0000188], embedding loss [ 0.7220], quantization loss [ 0.0324],  0.53 sec/batch.
2022-10-19 00:56:08,249 step [1323], lr [0.0000188], embedding loss [ 0.7324], quantization loss [ 0.0341],  0.53 sec/batch.
2022-10-19 00:56:10,432 step [1324], lr [0.0000188], embedding loss [ 0.7267], quantization loss [ 0.0341],  0.53 sec/batch.
2022-10-19 00:56:12,630 step [1325], lr [0.0000188], embedding loss [ 0.7251], quantization loss [ 0.0306],  0.53 sec/batch.
2022-10-19 00:56:14,758 step [1326], lr [0.0000188], embedding loss [ 0.7157], quantization loss [ 0.0305],  0.52 sec/batch.
2022-10-19 00:56:16,948 step [1327], lr [0.0000188], embedding loss [ 0.7230], quantization loss [ 0.0360],  0.53 sec/batch.
2022-10-19 00:56:19,075 step [1328], lr [0.0000188], embedding loss [ 0.7329], quantization loss [ 0.0345],  0.52 sec/batch.
2022-10-19 00:56:21,254 step [1329], lr [0.0000188], embedding loss [ 0.7182], quantization loss [ 0.0300],  0.52 sec/batch.
2022-10-19 00:56:23,360 step [1330], lr [0.0000188], embedding loss [ 0.7316], quantization loss [ 0.0307],  0.51 sec/batch.
2022-10-19 00:56:25,493 step [1331], lr [0.0000188], embedding loss [ 0.7226], quantization loss [ 0.0327],  0.54 sec/batch.
2022-10-19 00:56:27,703 step [1332], lr [0.0000188], embedding loss [ 0.7251], quantization loss [ 0.0332],  0.54 sec/batch.
2022-10-19 00:56:29,909 step [1333], lr [0.0000188], embedding loss [ 0.7249], quantization loss [ 0.0322],  0.53 sec/batch.
2022-10-19 00:56:32,118 step [1334], lr [0.0000188], embedding loss [ 0.7163], quantization loss [ 0.0330],  0.52 sec/batch.
2022-10-19 00:56:34,155 step [1335], lr [0.0000188], embedding loss [ 0.7140], quantization loss [ 0.0305],  0.49 sec/batch.
2022-10-19 00:56:36,285 step [1336], lr [0.0000188], embedding loss [ 0.7201], quantization loss [ 0.0295],  0.53 sec/batch.
2022-10-19 00:56:38,429 step [1337], lr [0.0000188], embedding loss [ 0.7237], quantization loss [ 0.0302],  0.53 sec/batch.
2022-10-19 00:56:40,608 step [1338], lr [0.0000188], embedding loss [ 0.7249], quantization loss [ 0.0351],  0.51 sec/batch.
2022-10-19 00:56:42,770 step [1339], lr [0.0000188], embedding loss [ 0.7249], quantization loss [ 0.0362],  0.52 sec/batch.
2022-10-19 00:56:44,922 step [1340], lr [0.0000188], embedding loss [ 0.7221], quantization loss [ 0.0333],  0.52 sec/batch.
2022-10-19 00:56:47,007 step [1341], lr [0.0000188], embedding loss [ 0.7176], quantization loss [ 0.0333],  0.52 sec/batch.
2022-10-19 00:56:49,079 step [1342], lr [0.0000188], embedding loss [ 0.7177], quantization loss [ 0.0318],  0.52 sec/batch.
2022-10-19 00:56:51,154 step [1343], lr [0.0000188], embedding loss [ 0.7201], quantization loss [ 0.0337],  0.51 sec/batch.
2022-10-19 00:56:53,323 step [1344], lr [0.0000188], embedding loss [ 0.7271], quantization loss [ 0.0306],  0.52 sec/batch.
2022-10-19 00:56:55,395 step [1345], lr [0.0000188], embedding loss [ 0.7260], quantization loss [ 0.0315],  0.49 sec/batch.
2022-10-19 00:56:57,515 step [1346], lr [0.0000188], embedding loss [ 0.7183], quantization loss [ 0.0261],  0.53 sec/batch.
2022-10-19 00:56:59,641 step [1347], lr [0.0000188], embedding loss [ 0.7221], quantization loss [ 0.0320],  0.51 sec/batch.
2022-10-19 00:57:01,708 step [1348], lr [0.0000188], embedding loss [ 0.7238], quantization loss [ 0.0320],  0.51 sec/batch.
2022-10-19 00:57:03,829 step [1349], lr [0.0000188], embedding loss [ 0.7120], quantization loss [ 0.0307],  0.53 sec/batch.
2022-10-19 00:57:05,965 step [1350], lr [0.0000188], embedding loss [ 0.7259], quantization loss [ 0.0344],  0.53 sec/batch.
2022-10-19 00:57:08,199 step [1351], lr [0.0000188], embedding loss [ 0.7238], quantization loss [ 0.0298],  0.53 sec/batch.
2022-10-19 00:57:10,377 step [1352], lr [0.0000188], embedding loss [ 0.7185], quantization loss [ 0.0293],  0.53 sec/batch.
2022-10-19 00:57:12,462 step [1353], lr [0.0000188], embedding loss [ 0.7208], quantization loss [ 0.0357],  0.52 sec/batch.
2022-10-19 00:57:14,548 step [1354], lr [0.0000188], embedding loss [ 0.7222], quantization loss [ 0.0295],  0.52 sec/batch.
2022-10-19 00:57:16,685 step [1355], lr [0.0000188], embedding loss [ 0.7226], quantization loss [ 0.0314],  0.53 sec/batch.
2022-10-19 00:57:18,827 step [1356], lr [0.0000188], embedding loss [ 0.7195], quantization loss [ 0.0330],  0.53 sec/batch.
2022-10-19 00:57:20,969 step [1357], lr [0.0000188], embedding loss [ 0.7306], quantization loss [ 0.0281],  0.53 sec/batch.
2022-10-19 00:57:23,122 step [1358], lr [0.0000188], embedding loss [ 0.7250], quantization loss [ 0.0296],  0.52 sec/batch.
2022-10-19 00:57:25,253 step [1359], lr [0.0000188], embedding loss [ 0.7314], quantization loss [ 0.0301],  0.53 sec/batch.
2022-10-19 00:57:27,393 step [1360], lr [0.0000188], embedding loss [ 0.7246], quantization loss [ 0.0338],  0.53 sec/batch.
2022-10-19 00:57:29,618 step [1361], lr [0.0000188], embedding loss [ 0.7149], quantization loss [ 0.0311],  0.53 sec/batch.
2022-10-19 00:57:29,618 update codes and centers iter(1/1).
2022-10-19 00:57:31,230 number of update_code wrong: 0.
2022-10-19 00:57:34,675 non zero codewords: 512.
2022-10-19 00:57:34,675 finish center update, duration: 5.06 sec.
2022-10-19 00:57:36,806 step [1362], lr [0.0000188], embedding loss [ 0.7259], quantization loss [ 0.0332],  0.53 sec/batch.
2022-10-19 00:57:38,937 step [1363], lr [0.0000188], embedding loss [ 0.7258], quantization loss [ 0.0300],  0.52 sec/batch.
2022-10-19 00:57:41,172 step [1364], lr [0.0000188], embedding loss [ 0.7334], quantization loss [ 0.0331],  0.54 sec/batch.
2022-10-19 00:57:43,436 step [1365], lr [0.0000188], embedding loss [ 0.7451], quantization loss [ 0.0315],  0.51 sec/batch.
2022-10-19 00:57:45,702 step [1366], lr [0.0000188], embedding loss [ 0.7316], quantization loss [ 0.0346],  0.53 sec/batch.
2022-10-19 00:57:47,940 step [1367], lr [0.0000188], embedding loss [ 0.7111], quantization loss [ 0.0287],  0.53 sec/batch.
2022-10-19 00:57:50,136 step [1368], lr [0.0000188], embedding loss [ 0.7209], quantization loss [ 0.0316],  0.53 sec/batch.
2022-10-19 00:57:52,303 step [1369], lr [0.0000188], embedding loss [ 0.7162], quantization loss [ 0.0300],  0.54 sec/batch.
2022-10-19 00:57:54,528 step [1370], lr [0.0000188], embedding loss [ 0.7152], quantization loss [ 0.0342],  0.53 sec/batch.
2022-10-19 00:57:56,673 step [1371], lr [0.0000188], embedding loss [ 0.7211], quantization loss [ 0.0298],  0.55 sec/batch.
2022-10-19 00:57:58,846 step [1372], lr [0.0000188], embedding loss [ 0.7230], quantization loss [ 0.0317],  0.55 sec/batch.
2022-10-19 00:58:01,032 step [1373], lr [0.0000188], embedding loss [ 0.7256], quantization loss [ 0.0343],  0.53 sec/batch.
2022-10-19 00:58:03,207 step [1374], lr [0.0000188], embedding loss [ 0.7245], quantization loss [ 0.0323],  0.52 sec/batch.
2022-10-19 00:58:05,466 step [1375], lr [0.0000188], embedding loss [ 0.7152], quantization loss [ 0.0307],  0.53 sec/batch.
2022-10-19 00:58:07,608 step [1376], lr [0.0000188], embedding loss [ 0.7284], quantization loss [ 0.0291],  0.52 sec/batch.
2022-10-19 00:58:09,817 step [1377], lr [0.0000188], embedding loss [ 0.7274], quantization loss [ 0.0286],  0.53 sec/batch.
2022-10-19 00:58:11,993 step [1378], lr [0.0000188], embedding loss [ 0.7305], quantization loss [ 0.0296],  0.55 sec/batch.
2022-10-19 00:58:14,253 step [1379], lr [0.0000188], embedding loss [ 0.7320], quantization loss [ 0.0313],  0.53 sec/batch.
2022-10-19 00:58:16,454 step [1380], lr [0.0000188], embedding loss [ 0.7106], quantization loss [ 0.0322],  0.53 sec/batch.
2022-10-19 00:58:18,619 step [1381], lr [0.0000188], embedding loss [ 0.7135], quantization loss [ 0.0320],  0.51 sec/batch.
2022-10-19 00:58:20,773 step [1382], lr [0.0000188], embedding loss [ 0.7212], quantization loss [ 0.0304],  0.53 sec/batch.
2022-10-19 00:58:22,952 step [1383], lr [0.0000188], embedding loss [ 0.7234], quantization loss [ 0.0292],  0.53 sec/batch.
2022-10-19 00:58:25,181 step [1384], lr [0.0000188], embedding loss [ 0.7247], quantization loss [ 0.0297],  0.53 sec/batch.
2022-10-19 00:58:27,298 step [1385], lr [0.0000188], embedding loss [ 0.7159], quantization loss [ 0.0319],  0.52 sec/batch.
2022-10-19 00:58:29,468 step [1386], lr [0.0000188], embedding loss [ 0.7275], quantization loss [ 0.0309],  0.53 sec/batch.
2022-10-19 00:58:31,587 step [1387], lr [0.0000188], embedding loss [ 0.7280], quantization loss [ 0.0309],  0.52 sec/batch.
2022-10-19 00:58:33,756 step [1388], lr [0.0000188], embedding loss [ 0.7260], quantization loss [ 0.0351],  0.51 sec/batch.
2022-10-19 00:58:35,960 step [1389], lr [0.0000188], embedding loss [ 0.7262], quantization loss [ 0.0313],  0.53 sec/batch.
2022-10-19 00:58:38,111 step [1390], lr [0.0000188], embedding loss [ 0.7191], quantization loss [ 0.0330],  0.53 sec/batch.
2022-10-19 00:58:40,261 step [1391], lr [0.0000188], embedding loss [ 0.7239], quantization loss [ 0.0327],  0.53 sec/batch.
2022-10-19 00:58:42,417 step [1392], lr [0.0000188], embedding loss [ 0.7179], quantization loss [ 0.0333],  0.53 sec/batch.
2022-10-19 00:58:44,574 step [1393], lr [0.0000188], embedding loss [ 0.7214], quantization loss [ 0.0296],  0.53 sec/batch.
2022-10-19 00:58:46,747 step [1394], lr [0.0000188], embedding loss [ 0.7121], quantization loss [ 0.0306],  0.54 sec/batch.
2022-10-19 00:58:48,936 step [1395], lr [0.0000188], embedding loss [ 0.7348], quantization loss [ 0.0337],  0.53 sec/batch.
2022-10-19 00:58:51,087 step [1396], lr [0.0000188], embedding loss [ 0.7198], quantization loss [ 0.0291],  0.52 sec/batch.
2022-10-19 00:58:53,253 step [1397], lr [0.0000188], embedding loss [ 0.7047], quantization loss [ 0.0332],  0.53 sec/batch.
2022-10-19 00:58:55,394 step [1398], lr [0.0000188], embedding loss [ 0.7188], quantization loss [ 0.0304],  0.52 sec/batch.
2022-10-19 00:58:57,549 step [1399], lr [0.0000188], embedding loss [ 0.7195], quantization loss [ 0.0312],  0.52 sec/batch.
2022-10-19 00:58:59,695 step [1400], lr [0.0000188], embedding loss [ 0.7141], quantization loss [ 0.0307],  0.53 sec/batch.
2022-10-19 00:59:01,885 step [1401], lr [0.0000188], embedding loss [ 0.7137], quantization loss [ 0.0302],  0.51 sec/batch.
2022-10-19 00:59:04,029 step [1402], lr [0.0000188], embedding loss [ 0.7251], quantization loss [ 0.0311],  0.53 sec/batch.
2022-10-19 00:59:06,224 step [1403], lr [0.0000188], embedding loss [ 0.7208], quantization loss [ 0.0322],  0.53 sec/batch.
2022-10-19 00:59:08,361 step [1404], lr [0.0000188], embedding loss [ 0.7197], quantization loss [ 0.0296],  0.51 sec/batch.
2022-10-19 00:59:10,509 step [1405], lr [0.0000188], embedding loss [ 0.7235], quantization loss [ 0.0321],  0.52 sec/batch.
2022-10-19 00:59:12,672 step [1406], lr [0.0000188], embedding loss [ 0.7310], quantization loss [ 0.0365],  0.53 sec/batch.
2022-10-19 00:59:14,813 step [1407], lr [0.0000188], embedding loss [ 0.7247], quantization loss [ 0.0317],  0.52 sec/batch.
2022-10-19 00:59:16,986 step [1408], lr [0.0000188], embedding loss [ 0.7256], quantization loss [ 0.0337],  0.52 sec/batch.
2022-10-19 00:59:19,108 step [1409], lr [0.0000188], embedding loss [ 0.7225], quantization loss [ 0.0308],  0.53 sec/batch.
2022-10-19 00:59:21,242 step [1410], lr [0.0000188], embedding loss [ 0.7224], quantization loss [ 0.0327],  0.53 sec/batch.
2022-10-19 00:59:23,344 step [1411], lr [0.0000188], embedding loss [ 0.7121], quantization loss [ 0.0347],  0.52 sec/batch.
2022-10-19 00:59:25,475 step [1412], lr [0.0000188], embedding loss [ 0.7226], quantization loss [ 0.0299],  0.52 sec/batch.
2022-10-19 00:59:27,600 step [1413], lr [0.0000188], embedding loss [ 0.7287], quantization loss [ 0.0301],  0.53 sec/batch.
2022-10-19 00:59:29,751 step [1414], lr [0.0000188], embedding loss [ 0.7077], quantization loss [ 0.0322],  0.53 sec/batch.
2022-10-19 00:59:31,884 step [1415], lr [0.0000188], embedding loss [ 0.7199], quantization loss [ 0.0305],  0.52 sec/batch.
2022-10-19 00:59:34,104 step [1416], lr [0.0000188], embedding loss [ 0.7235], quantization loss [ 0.0306],  0.53 sec/batch.
2022-10-19 00:59:36,250 step [1417], lr [0.0000188], embedding loss [ 0.7211], quantization loss [ 0.0326],  0.53 sec/batch.
2022-10-19 00:59:38,364 step [1418], lr [0.0000188], embedding loss [ 0.7263], quantization loss [ 0.0314],  0.51 sec/batch.
2022-10-19 00:59:40,549 step [1419], lr [0.0000188], embedding loss [ 0.7178], quantization loss [ 0.0293],  0.53 sec/batch.
2022-10-19 00:59:42,747 step [1420], lr [0.0000188], embedding loss [ 0.7264], quantization loss [ 0.0329],  0.53 sec/batch.
2022-10-19 00:59:44,928 step [1421], lr [0.0000188], embedding loss [ 0.7196], quantization loss [ 0.0319],  0.53 sec/batch.
2022-10-19 00:59:47,119 step [1422], lr [0.0000188], embedding loss [ 0.7256], quantization loss [ 0.0320],  0.54 sec/batch.
2022-10-19 00:59:49,378 step [1423], lr [0.0000188], embedding loss [ 0.7109], quantization loss [ 0.0337],  0.53 sec/batch.
2022-10-19 00:59:51,531 step [1424], lr [0.0000188], embedding loss [ 0.7280], quantization loss [ 0.0313],  0.53 sec/batch.
2022-10-19 00:59:53,695 step [1425], lr [0.0000188], embedding loss [ 0.7181], quantization loss [ 0.0310],  0.53 sec/batch.
2022-10-19 00:59:55,857 step [1426], lr [0.0000188], embedding loss [ 0.7231], quantization loss [ 0.0330],  0.53 sec/batch.
2022-10-19 00:59:58,018 step [1427], lr [0.0000188], embedding loss [ 0.7264], quantization loss [ 0.0307],  0.53 sec/batch.
2022-10-19 01:00:00,244 step [1428], lr [0.0000188], embedding loss [ 0.7176], quantization loss [ 0.0295],  0.53 sec/batch.
2022-10-19 01:00:02,396 step [1429], lr [0.0000188], embedding loss [ 0.7163], quantization loss [ 0.0310],  0.52 sec/batch.
2022-10-19 01:00:04,553 step [1430], lr [0.0000188], embedding loss [ 0.7188], quantization loss [ 0.0330],  0.53 sec/batch.
2022-10-19 01:00:06,705 step [1431], lr [0.0000188], embedding loss [ 0.7181], quantization loss [ 0.0302],  0.52 sec/batch.
2022-10-19 01:00:08,832 step [1432], lr [0.0000188], embedding loss [ 0.7249], quantization loss [ 0.0305],  0.51 sec/batch.
2022-10-19 01:00:10,991 step [1433], lr [0.0000188], embedding loss [ 0.7280], quantization loss [ 0.0286],  0.53 sec/batch.
2022-10-19 01:00:13,148 step [1434], lr [0.0000188], embedding loss [ 0.7309], quantization loss [ 0.0301],  0.53 sec/batch.
2022-10-19 01:00:15,303 step [1435], lr [0.0000188], embedding loss [ 0.7269], quantization loss [ 0.0318],  0.53 sec/batch.
2022-10-19 01:00:17,466 step [1436], lr [0.0000188], embedding loss [ 0.7075], quantization loss [ 0.0361],  0.52 sec/batch.
2022-10-19 01:00:19,661 step [1437], lr [0.0000188], embedding loss [ 0.7182], quantization loss [ 0.0346],  0.53 sec/batch.
2022-10-19 01:00:21,827 step [1438], lr [0.0000188], embedding loss [ 0.7364], quantization loss [ 0.0312],  0.53 sec/batch.
2022-10-19 01:00:24,125 step [1439], lr [0.0000188], embedding loss [ 0.7222], quantization loss [ 0.0327],  0.55 sec/batch.
2022-10-19 01:00:26,284 step [1440], lr [0.0000188], embedding loss [ 0.7196], quantization loss [ 0.0307],  0.52 sec/batch.
2022-10-19 01:00:28,435 step [1441], lr [0.0000188], embedding loss [ 0.7307], quantization loss [ 0.0328],  0.53 sec/batch.
2022-10-19 01:00:28,436 update codes and centers iter(1/1).
2022-10-19 01:00:30,038 number of update_code wrong: 0.
2022-10-19 01:00:33,545 non zero codewords: 512.
2022-10-19 01:00:33,546 finish center update, duration: 5.11 sec.
2022-10-19 01:00:35,750 step [1442], lr [0.0000188], embedding loss [ 0.7272], quantization loss [ 0.0321],  0.54 sec/batch.
2022-10-19 01:00:37,980 step [1443], lr [0.0000188], embedding loss [ 0.7344], quantization loss [ 0.0331],  0.53 sec/batch.
2022-10-19 01:00:40,222 step [1444], lr [0.0000188], embedding loss [ 0.7221], quantization loss [ 0.0350],  0.52 sec/batch.
2022-10-19 01:00:42,392 step [1445], lr [0.0000188], embedding loss [ 0.7225], quantization loss [ 0.0336],  0.53 sec/batch.
2022-10-19 01:00:44,554 step [1446], lr [0.0000188], embedding loss [ 0.7194], quantization loss [ 0.0311],  0.52 sec/batch.
2022-10-19 01:00:46,702 step [1447], lr [0.0000188], embedding loss [ 0.7006], quantization loss [ 0.0319],  0.52 sec/batch.
2022-10-19 01:00:48,985 step [1448], lr [0.0000188], embedding loss [ 0.7184], quantization loss [ 0.0332],  0.53 sec/batch.
2022-10-19 01:00:51,177 step [1449], lr [0.0000188], embedding loss [ 0.7177], quantization loss [ 0.0306],  0.53 sec/batch.
2022-10-19 01:00:53,457 step [1450], lr [0.0000188], embedding loss [ 0.7130], quantization loss [ 0.0300],  0.53 sec/batch.
2022-10-19 01:00:55,619 step [1451], lr [0.0000188], embedding loss [ 0.7110], quantization loss [ 0.0282],  0.53 sec/batch.
2022-10-19 01:00:57,838 step [1452], lr [0.0000188], embedding loss [ 0.7223], quantization loss [ 0.0349],  0.53 sec/batch.
2022-10-19 01:01:00,050 step [1453], lr [0.0000188], embedding loss [ 0.7276], quantization loss [ 0.0320],  0.52 sec/batch.
2022-10-19 01:01:02,244 step [1454], lr [0.0000188], embedding loss [ 0.7242], quantization loss [ 0.0271],  0.52 sec/batch.
2022-10-19 01:01:04,377 step [1455], lr [0.0000188], embedding loss [ 0.7290], quantization loss [ 0.0319],  0.53 sec/batch.
2022-10-19 01:01:06,544 step [1456], lr [0.0000188], embedding loss [ 0.7157], quantization loss [ 0.0270],  0.53 sec/batch.
2022-10-19 01:01:08,752 step [1457], lr [0.0000188], embedding loss [ 0.7204], quantization loss [ 0.0336],  0.53 sec/batch.
2022-10-19 01:01:10,934 step [1458], lr [0.0000188], embedding loss [ 0.7256], quantization loss [ 0.0334],  0.51 sec/batch.
2022-10-19 01:01:13,089 step [1459], lr [0.0000188], embedding loss [ 0.7211], quantization loss [ 0.0306],  0.51 sec/batch.
2022-10-19 01:01:15,267 step [1460], lr [0.0000188], embedding loss [ 0.7190], quantization loss [ 0.0312],  0.52 sec/batch.
2022-10-19 01:01:17,476 step [1461], lr [0.0000188], embedding loss [ 0.7219], quantization loss [ 0.0342],  0.53 sec/batch.
2022-10-19 01:01:19,663 step [1462], lr [0.0000188], embedding loss [ 0.7195], quantization loss [ 0.0319],  0.52 sec/batch.
2022-10-19 01:01:21,879 step [1463], lr [0.0000188], embedding loss [ 0.7397], quantization loss [ 0.0333],  0.53 sec/batch.
2022-10-19 01:01:24,070 step [1464], lr [0.0000188], embedding loss [ 0.7242], quantization loss [ 0.0304],  0.52 sec/batch.
2022-10-19 01:01:26,278 step [1465], lr [0.0000188], embedding loss [ 0.7218], quantization loss [ 0.0333],  0.52 sec/batch.
2022-10-19 01:01:28,535 step [1466], lr [0.0000188], embedding loss [ 0.7289], quantization loss [ 0.0279],  0.53 sec/batch.
2022-10-19 01:01:30,736 step [1467], lr [0.0000188], embedding loss [ 0.7289], quantization loss [ 0.0324],  0.51 sec/batch.
2022-10-19 01:01:32,869 step [1468], lr [0.0000188], embedding loss [ 0.7175], quantization loss [ 0.0292],  0.52 sec/batch.
2022-10-19 01:01:34,968 step [1469], lr [0.0000188], embedding loss [ 0.7147], quantization loss [ 0.0305],  0.50 sec/batch.
2022-10-19 01:01:37,105 step [1470], lr [0.0000188], embedding loss [ 0.7280], quantization loss [ 0.0295],  0.53 sec/batch.
2022-10-19 01:01:39,165 step [1471], lr [0.0000188], embedding loss [ 0.7198], quantization loss [ 0.0317],  0.52 sec/batch.
2022-10-19 01:01:41,383 step [1472], lr [0.0000188], embedding loss [ 0.7162], quantization loss [ 0.0292],  0.53 sec/batch.
2022-10-19 01:01:43,564 step [1473], lr [0.0000188], embedding loss [ 0.7096], quantization loss [ 0.0328],  0.53 sec/batch.
2022-10-19 01:01:45,746 step [1474], lr [0.0000188], embedding loss [ 0.7098], quantization loss [ 0.0348],  0.53 sec/batch.
2022-10-19 01:01:47,947 step [1475], lr [0.0000188], embedding loss [ 0.7310], quantization loss [ 0.0298],  0.52 sec/batch.
2022-10-19 01:01:50,176 step [1476], lr [0.0000188], embedding loss [ 0.7349], quantization loss [ 0.0299],  0.52 sec/batch.
2022-10-19 01:01:52,347 step [1477], lr [0.0000188], embedding loss [ 0.7281], quantization loss [ 0.0357],  0.53 sec/batch.
2022-10-19 01:01:54,503 step [1478], lr [0.0000188], embedding loss [ 0.7204], quantization loss [ 0.0361],  0.52 sec/batch.
2022-10-19 01:01:56,718 step [1479], lr [0.0000188], embedding loss [ 0.7235], quantization loss [ 0.0303],  0.52 sec/batch.
2022-10-19 01:01:58,807 step [1480], lr [0.0000188], embedding loss [ 0.7168], quantization loss [ 0.0318],  0.51 sec/batch.
2022-10-19 01:02:00,996 step [1481], lr [0.0000188], embedding loss [ 0.7244], quantization loss [ 0.0319],  0.51 sec/batch.
2022-10-19 01:02:03,125 step [1482], lr [0.0000188], embedding loss [ 0.7026], quantization loss [ 0.0275],  0.52 sec/batch.
2022-10-19 01:02:05,274 step [1483], lr [0.0000188], embedding loss [ 0.7157], quantization loss [ 0.0288],  0.53 sec/batch.
2022-10-19 01:02:07,440 step [1484], lr [0.0000188], embedding loss [ 0.7152], quantization loss [ 0.0321],  0.52 sec/batch.
2022-10-19 01:02:09,657 step [1485], lr [0.0000188], embedding loss [ 0.7189], quantization loss [ 0.0333],  0.53 sec/batch.
2022-10-19 01:02:11,795 step [1486], lr [0.0000188], embedding loss [ 0.7264], quantization loss [ 0.0297],  0.52 sec/batch.
2022-10-19 01:02:14,005 step [1487], lr [0.0000188], embedding loss [ 0.7127], quantization loss [ 0.0299],  0.52 sec/batch.
2022-10-19 01:02:16,218 step [1488], lr [0.0000188], embedding loss [ 0.7242], quantization loss [ 0.0322],  0.53 sec/batch.
2022-10-19 01:02:18,402 step [1489], lr [0.0000188], embedding loss [ 0.7178], quantization loss [ 0.0350],  0.52 sec/batch.
2022-10-19 01:02:20,532 step [1490], lr [0.0000188], embedding loss [ 0.7117], quantization loss [ 0.0309],  0.50 sec/batch.
2022-10-19 01:02:22,691 step [1491], lr [0.0000188], embedding loss [ 0.7281], quantization loss [ 0.0318],  0.52 sec/batch.
2022-10-19 01:02:24,949 step [1492], lr [0.0000188], embedding loss [ 0.7301], quantization loss [ 0.0291],  0.53 sec/batch.
2022-10-19 01:02:27,148 step [1493], lr [0.0000188], embedding loss [ 0.7319], quantization loss [ 0.0312],  0.53 sec/batch.
2022-10-19 01:02:29,380 step [1494], lr [0.0000188], embedding loss [ 0.7177], quantization loss [ 0.0293],  0.51 sec/batch.
2022-10-19 01:02:31,579 step [1495], lr [0.0000188], embedding loss [ 0.7383], quantization loss [ 0.0304],  0.53 sec/batch.
2022-10-19 01:02:33,829 step [1496], lr [0.0000188], embedding loss [ 0.7211], quantization loss [ 0.0305],  0.53 sec/batch.
2022-10-19 01:02:35,991 step [1497], lr [0.0000188], embedding loss [ 0.7232], quantization loss [ 0.0345],  0.53 sec/batch.
2022-10-19 01:02:38,174 step [1498], lr [0.0000188], embedding loss [ 0.7230], quantization loss [ 0.0335],  0.52 sec/batch.
2022-10-19 01:02:40,386 step [1499], lr [0.0000188], embedding loss [ 0.7224], quantization loss [ 0.0314],  0.53 sec/batch.
2022-10-19 01:02:42,585 step [1500], lr [0.0000188], embedding loss [ 0.7298], quantization loss [ 0.0329],  0.53 sec/batch.
2022-10-19 01:02:42,585 finish training iterations and begin saving model.
2022-10-19 01:02:50,136 finish model saving.
2022-10-19 01:02:50,137 finish training, model saved under ./checkpoints/nuswide_WSDQH_nbits=16_adaMargin_gamma=1_lambda=0.0001_221018.npy.
2022-10-19 03:28:58,461 prepare dataset.
2022-10-19 03:29:06,185 prepare data loader.
2022-10-19 03:29:06,185 Initializing DataLoader.
2022-10-19 03:29:06,189 DataLoader already.
2022-10-19 03:29:06,189 Initializing DataLoader.
2022-10-19 03:29:06,192 DataLoader already.
2022-10-19 03:29:06,192 prepare model.
2022-10-19 03:29:06,385 Number of semantic embeddings: 928.
2022-10-19 03:29:23,256 begin validation.
2022-10-19 03:29:55,034 finish query feature extraction, duration: 31.78 sec.
2022-10-19 03:45:09,697 finish database feature extraction, duration: 914.66 sec.
2022-10-19 03:45:09,697 compute quantization codes for query.
2022-10-19 03:45:11,379 number of update_code wrong: 0.
2022-10-19 03:45:11,379 finish query encoding, duration: 1.68 sec.
2022-10-19 03:45:11,379 compute quantization codes for database.
2022-10-19 03:45:38,814 number of update_code wrong: 0.
2022-10-19 03:45:38,814 finish database encoding, duration: 27.43 sec.
2022-10-19 03:45:38,814 save retrieval information: codes, features, reconstructions of queries and database.
2022-10-19 03:45:40,168 begin to calculate MAP@5000.
2022-10-19 03:45:40,169 begin to calculate AQD mAP@5000.
2022-10-19 03:46:10,120 AQD mAP@5000 = [0.7275], duration: 29.95 sec.
2022-10-19 03:46:10,120 begin to calculate SQD mAP@5000.
2022-10-19 03:46:38,167 SQD mAP@5000 = [0.7261], duration: 28.05 sec.
2022-10-19 03:46:38,167 begin to calculate feats mAP@5000.
2022-10-19 03:47:02,622 feats mAP@5000 = [0.7301], duration: 24.46 sec.
2022-10-19 03:47:02,624 finish validation.
2022-10-20 11:01:15,323 prepare dataset.
2022-10-20 11:01:22,770 prepare data loader.
2022-10-20 11:01:22,770 Initializing DataLoader.
2022-10-20 11:01:22,774 DataLoader already.
2022-10-20 11:01:22,774 Initializing DataLoader.
2022-10-20 11:01:22,777 DataLoader already.
2022-10-20 11:01:22,777 prepare model.
2022-10-20 11:01:22,976 Number of semantic embeddings: 928.
2022-10-20 11:01:42,564 begin validation.
2022-10-20 11:02:13,768 finish query feature extraction, duration: 31.20 sec.
2022-10-20 11:17:28,136 finish database feature extraction, duration: 914.37 sec.
2022-10-20 11:17:28,137 compute quantization codes for query.
2022-10-20 11:17:29,870 number of update_code wrong: 0.
2022-10-20 11:17:29,870 finish query encoding, duration: 1.73 sec.
2022-10-20 11:17:29,870 compute quantization codes for database.
2022-10-20 11:17:57,442 number of update_code wrong: 0.
2022-10-20 11:17:57,442 finish database encoding, duration: 27.57 sec.
2022-10-20 11:17:57,442 save retrieval information: codes, features, reconstructions of queries and database.
2022-10-20 11:17:58,781 begin to calculate MAP@5000.
2022-10-20 11:17:58,781 begin to calculate AQD mAP@5000.
2022-10-20 11:18:21,108 AQD mAP@5000 = [0.7269], duration: 22.33 sec.
2022-10-20 11:18:21,109 begin to calculate SQD mAP@5000.
2022-10-20 11:18:44,831 SQD mAP@5000 = [0.7246], duration: 23.72 sec.
2022-10-20 11:18:44,831 begin to calculate feats mAP@5000.
2022-10-20 11:19:07,641 feats mAP@5000 = [0.7303], duration: 22.81 sec.
2022-10-20 11:19:07,642 finish validation.
